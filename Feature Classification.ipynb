{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce8c3a0",
   "metadata": {},
   "source": [
    "# Classification, Prediction & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15a52152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ba88a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>lemma_count</th>\n",
       "      <th>spell_err_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neu_score</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20716</td>\n",
       "      <td>A long time ago when I was in third grade I h...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>708</td>\n",
       "      <td>39</td>\n",
       "      <td>3.669492</td>\n",
       "      <td>233</td>\n",
       "      <td>0.105932</td>\n",
       "      <td>0.231638</td>\n",
       "      <td>0.069209</td>\n",
       "      <td>0.225989</td>\n",
       "      <td>0.036723</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.067189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20717</td>\n",
       "      <td>Softball has to be one of the single most gre...</td>\n",
       "      <td>46.0</td>\n",
       "      <td>785</td>\n",
       "      <td>30</td>\n",
       "      <td>3.945223</td>\n",
       "      <td>280</td>\n",
       "      <td>0.014013</td>\n",
       "      <td>0.188535</td>\n",
       "      <td>0.066242</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.090446</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.105492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20718</td>\n",
       "      <td>Some people like making people laugh, I love ...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>861</td>\n",
       "      <td>39</td>\n",
       "      <td>3.979094</td>\n",
       "      <td>291</td>\n",
       "      <td>0.011614</td>\n",
       "      <td>0.186992</td>\n",
       "      <td>0.066202</td>\n",
       "      <td>0.202091</td>\n",
       "      <td>0.072009</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.084715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20719</td>\n",
       "      <td>\"LAUGHTER\"  @CAPS1 I hang out with my friends...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>713</td>\n",
       "      <td>30</td>\n",
       "      <td>3.688640</td>\n",
       "      <td>221</td>\n",
       "      <td>0.021038</td>\n",
       "      <td>0.192146</td>\n",
       "      <td>0.044881</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.088359</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.088823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20721</td>\n",
       "      <td>Well ima tell a story about the time i got @CA...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>647</td>\n",
       "      <td>26</td>\n",
       "      <td>3.452859</td>\n",
       "      <td>182</td>\n",
       "      <td>0.108192</td>\n",
       "      <td>0.224111</td>\n",
       "      <td>0.066461</td>\n",
       "      <td>0.276662</td>\n",
       "      <td>0.111283</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.136575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>21626</td>\n",
       "      <td>In most stories mothers and daughters are eit...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>863</td>\n",
       "      <td>27</td>\n",
       "      <td>3.997683</td>\n",
       "      <td>309</td>\n",
       "      <td>0.064890</td>\n",
       "      <td>0.208575</td>\n",
       "      <td>0.055620</td>\n",
       "      <td>0.198146</td>\n",
       "      <td>0.082271</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.067341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>21628</td>\n",
       "      <td>I never understood the meaning laughter is th...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>559</td>\n",
       "      <td>35</td>\n",
       "      <td>3.844365</td>\n",
       "      <td>197</td>\n",
       "      <td>0.066190</td>\n",
       "      <td>0.241503</td>\n",
       "      <td>0.071556</td>\n",
       "      <td>0.184258</td>\n",
       "      <td>0.098390</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.125316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>21629</td>\n",
       "      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>835</td>\n",
       "      <td>41</td>\n",
       "      <td>4.294611</td>\n",
       "      <td>334</td>\n",
       "      <td>0.076647</td>\n",
       "      <td>0.241916</td>\n",
       "      <td>0.049102</td>\n",
       "      <td>0.204790</td>\n",
       "      <td>0.071856</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.097131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>21630</td>\n",
       "      <td>Trippin' on fen...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>576</td>\n",
       "      <td>39</td>\n",
       "      <td>4.109375</td>\n",
       "      <td>232</td>\n",
       "      <td>0.039931</td>\n",
       "      <td>0.199653</td>\n",
       "      <td>0.069444</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.092014</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.072300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>21633</td>\n",
       "      <td>Many people believe that laughter can improve...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>475</td>\n",
       "      <td>29</td>\n",
       "      <td>4.181053</td>\n",
       "      <td>213</td>\n",
       "      <td>0.025263</td>\n",
       "      <td>0.202105</td>\n",
       "      <td>0.090526</td>\n",
       "      <td>0.218947</td>\n",
       "      <td>0.073684</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.089798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>723 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     essay_id                                              essay  \\\n",
       "0       20716   A long time ago when I was in third grade I h...   \n",
       "1       20717   Softball has to be one of the single most gre...   \n",
       "2       20718   Some people like making people laugh, I love ...   \n",
       "3       20719   \"LAUGHTER\"  @CAPS1 I hang out with my friends...   \n",
       "4       20721  Well ima tell a story about the time i got @CA...   \n",
       "..        ...                                                ...   \n",
       "718     21626   In most stories mothers and daughters are eit...   \n",
       "719     21628   I never understood the meaning laughter is th...   \n",
       "720     21629  When you laugh, is @CAPS5 out of habit, or is ...   \n",
       "721     21630                                 Trippin' on fen...   \n",
       "722     21633   Many people believe that laughter can improve...   \n",
       "\n",
       "     domain1_score  word_count  sent_count  avg_word_len  lemma_count  \\\n",
       "0             34.0         708          39      3.669492          233   \n",
       "1             46.0         785          30      3.945223          280   \n",
       "2             40.0         861          39      3.979094          291   \n",
       "3             30.0         713          30      3.688640          221   \n",
       "4             26.0         647          26      3.452859          182   \n",
       "..             ...         ...         ...           ...          ...   \n",
       "718           35.0         863          27      3.997683          309   \n",
       "719           32.0         559          35      3.844365          197   \n",
       "720           40.0         835          41      4.294611          334   \n",
       "721           40.0         576          39      4.109375          232   \n",
       "722           40.0         475          29      4.181053          213   \n",
       "\n",
       "     spell_err_count  noun_count  adj_count  verb_count  adv_count  neg_score  \\\n",
       "0           0.105932    0.231638   0.069209    0.225989   0.036723      0.057   \n",
       "1           0.014013    0.188535   0.066242    0.207643   0.090446      0.057   \n",
       "2           0.011614    0.186992   0.066202    0.202091   0.072009      0.086   \n",
       "3           0.021038    0.192146   0.044881    0.217391   0.088359      0.051   \n",
       "4           0.108192    0.224111   0.066461    0.276662   0.111283      0.051   \n",
       "..               ...         ...        ...         ...        ...        ...   \n",
       "718         0.064890    0.208575   0.055620    0.198146   0.082271      0.076   \n",
       "719         0.066190    0.241503   0.071556    0.184258   0.098390      0.034   \n",
       "720         0.076647    0.241916   0.049102    0.204790   0.071856      0.060   \n",
       "721         0.039931    0.199653   0.069444    0.187500   0.092014      0.103   \n",
       "722         0.025263    0.202105   0.090526    0.218947   0.073684      0.057   \n",
       "\n",
       "     pos_score  neu_score  cosine_similarity  \n",
       "0        0.125      0.818           0.067189  \n",
       "1        0.224      0.719           0.105492  \n",
       "2        0.184      0.730           0.084715  \n",
       "3        0.222      0.727           0.088823  \n",
       "4        0.200      0.748           0.136575  \n",
       "..         ...        ...                ...  \n",
       "718      0.185      0.739           0.067341  \n",
       "719      0.211      0.755           0.125316  \n",
       "720      0.173      0.767           0.097131  \n",
       "721      0.167      0.731           0.072300  \n",
       "722      0.253      0.691           0.089798  \n",
       "\n",
       "[723 rows x 16 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('features/features_set_8.csv')\n",
    "data.dropna(inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98898f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.iloc[:,3:]\n",
    "y=data.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52792405",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(X ,y, test_size=0.2 , random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c566d0",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "### What is Random Forest?\n",
    "Random Forest is a powerful and versatile supervised machine learning algorithm that grows and combines multiple decision trees to create a “forest”.  A decision tree is another type of algorithm used to classify data. In very simple terms, it is like a flowchart that draws a clear pathway to a decision or outcome; it starts at a single point and then branches off into two or more directions, with each branch of the decision tree offering different possible outcomes.\n",
    "\n",
    "### How does it work?\n",
    "Random Forest grows multiple decision trees which are merged together for a more accurate prediction.\n",
    "\n",
    "The logic behind the Random Forest model is that multiple uncorrelated models (the individual decision trees) perform much better as a group than they do alone. When using Random Forest for classification, each tree gives a classification or a “vote.” The forest chooses the classification with the majority of the “votes.” When using Random Forest for regression, the forest picks the average of the outputs of all trees.\n",
    "\n",
    "### Advantages\n",
    "1. Very easy to use.\n",
    "2. Random Forest is much more efficient than a single Decision Tree while performing analysis on a large database.\n",
    "3. Random forests have a very good accuracy.\n",
    "4. They are versatile (can be used for regression or classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fcab3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(class_weight='balanced_subsample')\n",
    "rf_params = {'n_estimators':list(range(20,200,10)),\n",
    "                'max_depth':list(range(2,14,1))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd6b6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = sklearn.metrics.make_scorer(sklearn.metrics.f1_score, average = 'macro')\n",
    "rf_random=GridSearchCV(estimator = rf, param_grid  = rf_params, cv = 5, verbose=2,  n_jobs = 2, scoring=scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af108920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=RandomForestClassifier(class_weight='balanced_subsample'),\n",
       "             n_jobs=2,\n",
       "             param_grid={'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
       "                         'n_estimators': [20, 30, 40, 50, 60, 70, 80, 90, 100,\n",
       "                                          110, 120, 130, 140, 150, 160, 170,\n",
       "                                          180, 190]},\n",
       "             scoring=make_scorer(f1_score, average=macro), verbose=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1529f797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced_subsample', max_depth=10,\n",
       "                       n_estimators=120, random_state=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_final = RandomForestClassifier(random_state=0, n_estimators=rf_random.best_params_['n_estimators'], max_depth=rf_random.best_params_['max_depth'],class_weight='balanced_subsample')\n",
    "rf_final.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76399166",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = rf_final.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe9a7087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.000     0.000     0.000         0\n",
      "         1.0      0.821     0.716     0.765       141\n",
      "         2.0      0.642     0.705     0.672       122\n",
      "         3.0      0.741     0.723     0.732        83\n",
      "\n",
      "    accuracy                          0.714       346\n",
      "   macro avg      0.551     0.536     0.542       346\n",
      "weighted avg      0.739     0.714     0.724       346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(X_pred,y_test,digits=3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bf0da66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, X_pred)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41e0a872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7138728323699421\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, X_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa7626",
   "metadata": {},
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb67926",
   "metadata": {},
   "source": [
    "### What is XGBoost?\n",
    "\n",
    "XGBoost is one of the most popular machine learning algorithm these days. Regardless of the type of prediction task at hand; regression or classification. XGBoost is well known to provide better solutions than other machine learning algorithms. In fact, since its inception, it has become the \"state-of-the-art” machine learning algorithm to deal with structured data.\n",
    "\n",
    "XGBoost belongs to a family of boosting algorithms and uses the gradient boosting (GBM) framework at its core. It is an optimized distributed gradient boosting library.\n",
    "\n",
    "### Boosting\n",
    "Boosting is a sequential technique which works on the principle of an ensemble. It combines a set of weak learners and delivers improved prediction accuracy. At any instant t, the model outcomes are weighed based on the outcomes of previous instant t-1. The outcomes predicted correctly are given a lower weight and the ones miss-classified are weighted higher. A weak learner is one which is slightly better than random guessing. \n",
    "\n",
    "\n",
    "### Advantages of XGBoost:\n",
    "\n",
    "1. It is comparatively faster than other ensemble classifiers\n",
    "2. The code XGBoost algorithm is parallelizable and hence can harness the power of multi-core GPUs.\n",
    "3. It has a wide range of tuning parameters (cross-validation, regularization, user-defined objective functions, missing values, tree parameters, scikit-learn compatible API).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e6e1601",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(tree_method = \"exact\", predictor = \"cpu_predictor\",\n",
    "                            objective = \"multi:softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae4fdc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"learning_rate\": [0.1, 0.01, 0.001],\n",
    "               \"gamma\" : [0.01, 0.1, 0.3, 0.5, 1, 1.5, 2],\n",
    "               \"max_depth\": [2, 4, 7, 10],\n",
    "               \"colsample_bytree\": [0.3, 0.6, 0.8, 1.0],\n",
    "               \"subsample\": [0.2, 0.4, 0.5, 0.6, 0.7],\n",
    "               \"reg_alpha\": [0, 0.5, 1],\n",
    "               \"reg_lambda\": [1, 1.5, 2, 3, 4.5],\n",
    "               \"min_child_weight\": [1, 3, 5, 7],\n",
    "               \"n_estimators\": [100, 250, 500, 1000]}\n",
    "# subsample: Denotes the subsample ratio of columns for each split, in each level.\n",
    "# colsample_bytree: Denotes the fraction of columns to be randomly samples for each tree.\n",
    "# gamma: Gamma specifies the minimum loss reduction required to make a split.\n",
    "# reg_alpha: Lasso L1 regularization\n",
    "# reg_lambda: Ridge L2 reguralarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd17da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RandomizedSearchCV Object\n",
    "xgb_rscv = RandomizedSearchCV(xgb_clf, param_distributions = parameters, scoring = \"f1_micro\",\n",
    "                             cv = 10, verbose = 3, random_state = 40 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97c24c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[08:41:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.2s\n",
      "[08:41:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.1s\n",
      "[08:41:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.2s\n",
      "[08:41:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.2s\n",
      "[08:41:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.2s\n",
      "[08:42:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.2s\n",
      "[08:42:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 7/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.4s\n",
      "[08:42:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 8/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.2s\n",
      "[08:42:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 9/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.2s\n",
      "[08:42:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 10/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.2s\n",
      "[08:42:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.2s\n",
      "[08:42:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.1s\n",
      "[08:42:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.2s\n",
      "[08:42:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.1s\n",
      "[08:42:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.2s\n",
      "[08:42:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.1s\n",
      "[08:42:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 7/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.2s\n",
      "[08:42:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 8/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.1s\n",
      "[08:42:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 9/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.3s\n",
      "[08:42:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 10/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.1s\n",
      "[08:42:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.3s\n",
      "[08:42:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.6s\n",
      "[08:42:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.3s\n",
      "[08:42:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.3s\n",
      "[08:42:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.3s\n",
      "[08:42:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.3s\n",
      "[08:42:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 7/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.3s\n",
      "[08:42:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 8/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.3s\n",
      "[08:42:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 9/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.3s\n",
      "[08:42:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 10/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.3s\n",
      "[08:42:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.2s\n",
      "[08:42:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.2s\n",
      "[08:42:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.2s\n",
      "[08:42:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.2s\n",
      "[08:42:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.2s\n",
      "[08:42:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.2s\n",
      "[08:42:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 7/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.2s\n",
      "[08:42:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 8/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.2s\n",
      "[08:42:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 9/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.2s\n",
      "[08:42:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 10/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.2s\n",
      "[08:42:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   1.2s\n",
      "[08:42:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   1.2s\n",
      "[08:42:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   1.2s\n",
      "[08:42:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   1.2s\n",
      "[08:42:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   1.3s\n",
      "[08:42:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   1.3s\n",
      "[08:42:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 7/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   1.2s\n",
      "[08:42:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 8/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   1.2s\n",
      "[08:42:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 9/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   1.2s\n",
      "[08:42:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 10/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   1.3s\n",
      "[08:42:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.3s\n",
      "[08:42:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.3s\n",
      "[08:42:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.4s\n",
      "[08:42:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.6s\n",
      "[08:42:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.3s\n",
      "[08:42:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.3s\n",
      "[08:42:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 7/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.4s\n",
      "[08:42:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 8/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.4s\n",
      "[08:42:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 9/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.4s\n",
      "[08:42:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 10/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.3s\n",
      "[08:42:24] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   0.7s\n",
      "[08:42:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   0.8s\n",
      "[08:42:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   0.7s\n",
      "[08:42:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   0.9s\n",
      "[08:42:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   0.7s\n",
      "[08:42:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   0.7s\n",
      "[08:42:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 7/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   0.7s\n",
      "[08:42:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 8/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   0.7s\n",
      "[08:42:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 9/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   0.7s\n",
      "[08:42:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 10/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   0.8s\n",
      "[08:42:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   3.2s\n",
      "[08:42:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   3.5s\n",
      "[08:42:39] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   4.4s\n",
      "[08:42:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   3.2s\n",
      "[08:42:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   3.5s\n",
      "[08:42:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   3.1s\n",
      "[08:42:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 7/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   2.9s\n",
      "[08:42:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 8/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   2.8s\n",
      "[08:42:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 9/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   2.9s\n",
      "[08:43:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 10/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   2.9s\n",
      "[08:43:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   1.2s\n",
      "[08:43:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   1.1s\n",
      "[08:43:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   1.2s\n",
      "[08:43:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   0.9s\n",
      "[08:43:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   0.9s\n",
      "[08:43:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   1.0s\n",
      "[08:43:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 7/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   1.2s\n",
      "[08:43:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 8/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   1.8s\n",
      "[08:43:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 9/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   1.0s\n",
      "[08:43:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 10/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   1.1s\n",
      "[08:43:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   0.5s\n",
      "[08:43:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   0.7s\n",
      "[08:43:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   1.2s\n",
      "[08:43:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   0.6s\n",
      "[08:43:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   1.2s\n",
      "[08:43:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   0.5s\n",
      "[08:43:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 7/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   0.6s\n",
      "[08:43:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 8/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   0.5s\n",
      "[08:43:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 9/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   0.5s\n",
      "[08:43:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 10/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   0.6s\n",
      "[08:43:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "model_xgboost = xgb_rscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60e6cbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:43:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1.0,\n",
       "              enable_categorical=False, gamma=1.5, gpu_id=-1,\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=2,\n",
       "              min_child_weight=5, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=12, num_parallel_tree=1,\n",
       "              objective='multi:softprob', predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=None, subsample=0.6,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_final = XGBClassifier(random_state=0, \n",
    "                          n_estimators=xgb_rscv.best_params_['n_estimators'], \n",
    "                          max_depth=xgb_rscv.best_params_['max_depth'],\n",
    "                          learning_rate=xgb_rscv.best_params_['learning_rate'],\n",
    "                          gamma=xgb_rscv.best_params_['gamma'],\n",
    "                          colsample_bytree=xgb_rscv.best_params_['colsample_bytree'],\n",
    "                          subsample=xgb_rscv.best_params_['subsample'],\n",
    "                          reg_alpha=xgb_rscv.best_params_['reg_alpha'],\n",
    "                          reg_lambda=xgb_rscv.best_params_['reg_lambda'],\n",
    "                          min_child_weight=xgb_rscv.best_params_['min_child_weight'])\n",
    "xgb_final.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07011f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_final.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acf44109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.250     1.000     0.400         2\n",
      "         1.0      0.789     0.688     0.735       141\n",
      "         2.0      0.604     0.643     0.623       126\n",
      "         3.0      0.679     0.714     0.696        77\n",
      "\n",
      "    accuracy                          0.679       346\n",
      "   macro avg      0.581     0.761     0.614       346\n",
      "weighted avg      0.694     0.679     0.684       346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_pred,y_test,digits=3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5e798a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm1 = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm1, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0ffb871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6791907514450867"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016bd82f",
   "metadata": {},
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "### What is a Support Vector Machine (SVM)?\n",
    "A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they’re able to categorize new text.\n",
    "\n",
    "Compared to newer algorithms like neural networks, they have two main advantages: higher speed and better performance with a limited number of samples (in the thousands). This makes the algorithm very suitable for text classification problems, where it’s common to have access to a dataset of at most a couple of thousands of tagged samples.\n",
    "\n",
    "### How do SVMs work?\n",
    "A support vector machine takes data points and outputs the hyperplane (which in two dimensions it’s simply a line) that best separates the tags. This line is the decision boundary: anything that falls to one side of it we will classify as blue, and anything that falls to the other as red.\n",
    "\n",
    "<img src=\"hyperplane.png\" />\n",
    "\n",
    "### Advantages\n",
    "1. SVMs can give results at a higher speed and better accuracy with lesser number of samples.\n",
    "2. The SVM provides a very useful technique within it known as kernel and by the application of associated kernel function we can solve any complex problem.\n",
    "3. SVM generally do not suffer condition of overfitting and performs well when there is a clear indication of separation between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c28a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X = StandardScaler()\n",
    "X_Train = sc_X.fit_transform(X_train)\n",
    "X_Test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3de852fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_Train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d192ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Pred = classifier.predict(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a51a888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD8CAYAAABJsn7AAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAdxElEQVR4nO3deXxU5dnG8d89k7AvCgiEhApW6q5QBK34KhYEC1JobVHrQi02al1AWxUr1WrFulCq1hVbNHVBEamsojRKBRcEEcsSBAGBhLDLJhaSyfP+kTEEDWSYTObJnFxfP+eTOWcm51w5jHee3GcZc84hIiLJF/IdQESktlIBFhHxRAVYRMQTFWAREU9UgEVEPFEBFhHxRAVYROQAzGyMmW00s0XlljUzsxlmtjz69fByz91mZp+Z2adm1ruy9asAi4gc2LPAed9YNgzIdc51AHKj85jZ8cBFwAnR73nczMIHW7kKsIjIATjn3gG2fmNxfyAn+jgHGFBu+UvOuT3OuVXAZ0DXg60/LXFRD7CBOpm61C4qs3Fz3xFqjIKdW3xHkBqoeG+BVXUdRZtXxlxz6hzx3auA7HKLRjvnRlfyba2cc4UAzrlCM2sZXZ4JfFDudfnRZQdU7QVYRKSmihbbygpurCr65XHQXwYqwCISLCWR6t7CBjPLiI5+M4CN0eX5QNtyr8sC1h1sReoBi0iwRIpjn+IzCRgUfTwImFhu+UVmVtfM2gMdgA8PtiKNgEUkUJwrSdi6zGws0B1oYWb5wJ3AfcA4MxsMrAF+Xrpdt9jMxgFLgGLgWufcQYfjVt23o9RBuH10EG4fHYSTiiTiINze/IWxH4TLOqnK26sKjYBFJFgSOAKubirAIhIs1X8QLmFUgEUkWDQCFhHxw8V/dkPSqQCLSLCUaAQsIuKHWhAiIp7oIJyIiCcaAYuIeKKDcCIinuggnIiIH5XcfqFGUQEWkWBRD1hExBO1IEREPNEIWETEk0iR7wQxUwEWkWBRC0JExJMUakEE/jPhevfqzuJF77B0yWxuufla33G8Gnz1pcx4dwJvzp7AI6Pvp27dOr4jeaP3xT6B2xclJbFPngW6AIdCIR55eATn97uUk045hwsvHMBxx3XwHcuLVhktuSL7Es7vcTG9zvwp4XCIfj89z3csL/S+2CeQ+0IFuGbo2qUTK1Z8zqpVaygqKmLcuIn8uF9v37G8CaeFqVevLuFwmPr167GhcJPvSF7ofbFPEPeFixTFPPlWaQ/YzI4F+gOZgKP0c+4nOefyqjlblbXJbM3a/HVl8/kFhXTt0sljIn82FG5k9KM5vP/Jm/zvf/9j1tvvM2vm+75jeaH3xT6B3BdB6QGb2a3AS4BR+vn2c6OPx5rZsOqPVzVm3/7A0+r+FOiaqknTxvTqcw5nfv9HdD2hJ/Ub1ucnP+/rO5YXel/sE8h9kUItiMpGwIOBE5xz+43VzWwUsBi4r6JvMrNsIBvAwk0JhRomIOqhK8gvpG1Wm7L5rMwMCgs3eMni25lnn87a1fls3fIFANOn5NK5a0f+9cpUz8mST++LfQK5L4IyAgZKgDYVLM+IPlch59xo59ypzrlTfRVfgLnzFnD00e1p164t6enpDBzYn8lT3vSWx6d1BevpdOrJ1KtfD4BuZ53GZ8tWek7lh94X+wRyXwRoBDwUyDWz5cDa6LLvAEcD11VjroSIRCIMGTqcaVNfJBwK8WzOyyxZssx3LC8WfLSQaZP+zdS3XyZSHGHxwjxezBnvO5YXel/sE8h9kUIjYKus32NmIaArpQfhDMgH5roY7/mWViczxRtKiZPZuLnvCDVGwc4tviNIDVS8t+DbTelD9NXUh2KuOfX7Dq3y9qqi0rMgnHMlwAdJyCIiUnUpNALWpcgiEiw1oLcbKxVgEQkWjYBFRDzRCFhExBONgEVEPCnWx9KLiPiRQpdSqwCLSLCoBywi4kkKFeBA3w9YRGohVxL7VAkzu9HMFpvZIjMba2b1zKyZmc0ws+XRr4fHG1UFWESCJRKJfToIM8sEbgBOdc6dCISBi4BhQK5zrgOQG52PiwqwiARLYu+GlgbUN7M0oAGlH0jRH8iJPp8DDIg3qgqwiATLIRRgM8s2s3nlpuyvV+OcKwBGAmuAQmC7c+5NoJVzrjD6mkKgZbxRdRBORILlEC7EcM6NBkZX9Fy0t9sfaA9sA14xs0sTkLCMCrCIBIorSdh5wD2BVc65TQBmNgE4A9hgZhnOuUIzywA2xrsBtSBEJFgS1wNeA5xuZg2s9MPzegB5wCRgUPQ1g4CJ8UbVCFhEgqWSsxti5ZybY2bjgflAMfAxpe2KRsA4MxtMaZH+ebzbUAEWkWBJ4IUYzrk7gTu/sXgPpaPhKlMBFpFgSaEr4VSARSRYdDMeERFPNAIWEfEkcaehVTsV4CT67NPXfEeoMfp2+o3vCDXG/O0rfUcIlgSdBZEMKsAiEihOLQgREU/UghAR8UQfyiki4olGwCIinhTrIJyIiB9qQYiIeKIWhIiIHzoNTUTEF42ARUQ8UQEWEfFElyKLiPiRwM+Eq3YqwCISLCrAIiKe6CwIERFPNAIWEfFEBVhExA8XUQtCRMQPjYBFRPzQaWgiIr6oAIuIeJI6LWAVYBEJFlecOhVYBVhEgiV16m/wC3DvXt0ZNepuwqEQY54ZywMPPuY7UpUNv3cU77z7Ic0OP4zXnn+yyuubOG0GT+W8BMBVgy6if59zAbj1j/ezeOly0tLSOPH473HnLTeQnpaab5mbRt7I6T1OY9uWbWT3vBqAo44/iiF/vp46desQiUT42+2P8umCZZ6TJt/8hW+xa9eXRCIlRIqL6dn9At+RqiSVDsKFfAeoTqFQiEceHsH5/S7lpFPO4cILB3DccR18x6qyAX3O5clR9xzy9/3yulsoKNyw37LtO3byxDMvMvbphxj79EM88cyLbN+xE4C+vc5h8tin+ddzT7Bnz15enTw9Ifl9mPHKDH5/2fD9lv369sE8/9cXuOa8a8kZ+RxX/v5KT+n8G9D3cs45s3/KF1+gdAQc6+RZoAtw1y6dWLHic1atWkNRURHjxk3kx/16+45VZad2PImmTRrvt2xN/jquumk4A391PZdf8ztWrl4b07renfMRP+jSiaZNGtO0SWN+0KUT7875CICzzuiKmWFmnHTcMWzYuDnhP0uyLJyziJ3bdu63zDlo0LgBAA2bNGTLhi0+okmCuRIX8+Rb3AXYzK5IZJDq0CazNWvz15XN5xcU0qZNa4+Jqs9dDzzC72+8hnFj/sbvrruSe0bG1mrZsGkzrVseUTbf6ogWbNi0f6EtKi5m8hu5nHnaqQnN7NsTf3ySX99+JS/MeY7s4Vcy5r5nfEfywjnH+NfGkPufCVz+ywt9x6m6FBoBV6WhdxdQ4TvWzLKBbAALNyUUaliFzcTPzL61zDn/v/USbffur1iwMI+bht9btmxvUREA/5r6Js+PmwjAmoJ1XPO7P5Celk5mm1Y88uc7qGh3fHO/3TPyMTqfciKdO55YfT+EB/0uO58n73qK2a+/y1nn/x83PXgjw35xm+9YSde318WsX7+RFi2aMX7isyxftoL335vnO1bcXLHvBLE7aAE2s/8e6Cmg1YG+zzk3GhgNkFYn01vFK8gvpG1Wm7L5rMwMCr/RAw2CEldC48YNeTXn26Pen/TtxU/69gJKe8Ajbv8tmRn7/ulat2zB3I/3/TNv2LSZLp1OLpt/fMwLfLFtO3feu3//NAjO/VlPHr/zCQDemTKLGx8Y6jeQJ+vXbwRg8+atTJsyg+93Pjm1C3ANGNnGqrIWRCvgcqBfBVONb5jNnbeAo49uT7t2bUlPT2fgwP5MnvKm71gJ16hhQzIzWvPGW7OA0lH+0uUrY/rebqd15r0P57N9x06279jJex/Op9tpnQEYP2k67875iAfuupVQKHiHC7Zs2MLJp5f+sunYrSPrVq2r5DuCp0GD+jRq1LDscfcfdiMvb7nnVFWUwBaEmR1mZuPNbKmZ5ZnZD8ysmZnNMLPl0a+Hxxu1shbEFKCRc25BBcFmxrvRZIlEIgwZOpxpU18kHArxbM7LLFmS+qcZ3Xznfcz9+L9s27aDHgMu5TeDL+P+O2/hTyMf5amcsRQXF/OjHmdzbIejKl1X0yaNueqXF3PRlUMAuPqKX5Qd4PvTyL+R0aoll2TfBEDPs8/gml9dUn0/WDW67dFhnHz6yTRt1oQXPnyO5/7yPH+99WF+88erCaWFKdqzl4eGPew7ZtId0bIFOS+U/uWUlhbm1Vcm89a/Z3lOVTUJHgE/DEx3zv3MzOoADYDfA7nOufvMbBgwDLg1npVbdfdEfbYgapqv1qX2GzuR+nb6je8INcb87bH9tVIbbN6x7NsHbg7Rxh5nx1xzWub+54DbM7MmwCfAUa5coTSzT4HuzrlCM8sAZjrnjokna/D+rhSRWs1FLObJzLLNbF65Kbvcqo4CNgHPmNnHZvZ3M2sItHLOFQJEv7aMN2tqXtYkInIAh9KCKH/CQAXSgO8D1zvn5pjZw5S2GxJGI2ARCRRXYjFPlcgH8p1zc6Lz4yktyBuirQeiXzfGm1UFWEQCxZXEPh10Pc6tB9aa2df93R7AEmASMCi6bBAwMd6sakGISKA4V+XjeOVdD7wQPQNiJXAFpQPXcWY2GFgD/DzelasAi0igJPI0tOgpuBVdg98jEetXARaRQCmJJHQEXK1UgEUkUGI4uFZjqACLSKCoAIuIeJJKNzxUARaRQNEIWETEkwSfhlatVIBFJFAiOgtCRMQPjYBFRDxRD1hExBOdBSEi4olGwCIinkRKUucmjyrAIhIoakGIiHhSorMgRET80GloIiKeqAUhFfpnxzt8R6gxXrvjON8RaozedxX5jhAoakGIiHiisyBERDxJoQ6ECrCIBItaECIinugsCBERTxL4ocjVTgVYRALFoRGwiIgXxWpBiIj4oRGwiIgn6gGLiHiiEbCIiCcaAYuIeBLRCFhExI8U+kQiFWARCZYSjYBFRPzQzXhERDzRQTgREU9KTC0IEREvIr4DHILUuXW8iEgMSiz2KRZmFjazj81sSnS+mZnNMLPl0a+Hx5tVBVhEAqUEi3mK0RAgr9z8MCDXOdcByI3Ox0UFWEQCxR3CVBkzywL6An8vt7g/kBN9nAMMiDerCrCIBMqhtCDMLNvM5pWbsr+xuoeAW9j/5IpWzrlCgOjXlvFmDfxBuN69ujNq1N2EQyHGPDOWBx58zHekpAnXTafvq8MJ1UkjFA6zatqHfPyXCTQ77jt0u+8K0hrWY9faTcy8/gmKdn3lO261+nzrLm6d8nHZfMH2r7jmjA6c2rY5I/69iK+KimnTpAEj+pxCo7rpHpNWv2F/+R1n9DydLzZvY1CPK8uWX3DFAH56xQAixRHez53DEyNGe0wZv0M5Dc05Nxqo8Ac1s/OBjc65j8ysewKifUugC3AoFOKRh0dwXp+Lyc8v5IP3pzF5ypvk5S33HS0pInuKmDbwXop378HSwpz/rz+Q//Yn/ODuQXx4z4us/2ApHS48i5Ou7sv8keN9x61W7Zo14uXL/w+ASImj91O5nNOhNTdPms+NZx/LqW2b89rCteTMW8W13b7nOW31en3cG0x4ZiK3P3xr2bJOZ3TkzN5n8Muev6ZobxGHNT/MX8AqiiTuLLRuwI/NrA9QD2hiZs8DG8wswzlXaGYZwMZ4N1BpC8LMjjWzHmbW6BvLz4t3o8nStUsnVqz4nFWr1lBUVMS4cRP5cb/evmMlVfHuPQCE0sKE0tLAQdPvZrD+g6UArHtnEe36dPEZMek+XLOZrMMa0qZJfVZ/8SWds5oBcPqRLchdtt5zuur3yZyF7Ni2Y79lAy7vx/OPvUTR3iIAtm3Z5iFZYpQcwnQwzrnbnHNZzrl2wEXAW865S4FJwKDoywYBE+PNetACbGY3RFd+PbDIzPqXe/reeDeaLG0yW7M2f13ZfH5BIW3atPaYKPksZAx4YwSXfPI462YtZNPHK/ji07V8p9f3AWh//mk0bNPMc8rkemNpIecdmwHAd5s3YuaK0gHMjGWFbNgZ7FbMgbQ9KotTup7EU5Mf5W/jR3HsKcf4jhS3RBXgg7gPONfMlgPnRufjUtkI+NdAZ+fcAKA78AczGxJ97oAD/fKN7ZKSL+PNVmVWwRUxzqXSleJV50ocr/W+nZe63ECLjt/l8GOymPXbpzl+0Ln0n/Yn0hvVo6So2HfMpCmKlPCfFRs493ulBfiPvU9m3ILV/OK52ezeGyE9XDuPS4fDYRo3bcRV/a7j8Xue4q4n/+A7UtycxT7FvE7nZjrnzo8+3uKc6+Gc6xD9ujXerJX1gMPOuV3RjX4ebUSPN7MjOUgBLt/YTquT6a3iFeQX0jarTdl8VmYGhYUbfMXxau+O3ax/P4/M7iez6KlpTL/kfgCatG9N2x4d/YZLotmrNnFsq6Y0b1gXgPbNG/HEz7oCsHrrLmatirudl9I2FW7iP6/PBiBvwae4EsdhzZqybet2z8kOXSrdC6KyX/frzazj1zPRYnw+0AI4qRpzJcTceQs4+uj2tGvXlvT0dAYO7M/kKW/6jpU09Zo1pk6TBgCE66XT5swT2f7ZOuo1b1L6AjM6DulP3nO5HlMm1/Sl68raDwBboz3yEud4es4Kfnbyd3xF82rWG+/SuVsnoLQdkVYnLSWLL5Reihzr5FtlI+DLgf3+PnXOFQOXm9lT1ZYqQSKRCEOGDmfa1BcJh0I8m/MyS5Ys8x0raeq3Ooyz/3oVFg5hZqycMoe1uQs4YXBvjhvUE4DPX5/H8pff8Zw0Ob4qijBn9WaGn3ti2bLpS9fx8oLVAPzw6Nb0PzHLV7ykufOx2+n0g1No2qwpr857iTEjc5j60nRu+8vN5OT+neKiYu4der/vmHFLpRuyW3X3RH22IGqap1qe4ztCjXHxHUf4jlBj9L7rv74j1BizCnKrXD7/+p1LY645N6553mu5DvR5wCJS+6RSD1gFWEQCJZX+5FYBFpFASaUesAqwiARKTTi7IVYqwCISKCUp1IRQARaRQNFBOBERT1Jn/KsCLCIBoxGwiIgnxZY6Y2AVYBEJlNQpvyrAIhIwakGIiHii09BERDxJnfKrAiwiAaMWhIiIJ5EUGgOrAItIoGgELCLiidMIWETED42ARUQ80WloIiKepE75VQEWkYApTqESrAIsIoGig3BSoSFb3/UdocaYOaKT7wg1xt8b1/EdIVB0EE5ExBONgEVEPNEIWETEk4jTCFhExAudBywi4ol6wCIinqgHLCLiSSq1IEK+A4iIJJI7hP8OxszamtnbZpZnZovNbEh0eTMzm2Fmy6NfD483qwqwiARKxLmYp0oUA791zh0HnA5ca2bHA8OAXOdcByA3Oh8XFWARCZQSXMzTwTjnCp1z86OPdwJ5QCbQH8iJviwHGBBvVhVgEQmUkkOYzCzbzOaVm7IrWqeZtQM6AXOAVs65Qigt0kDLeLPqIJyIBMqhnIbmnBsNjD7Ya8ysEfAqMNQ5t8PMqhawHBVgEQmURJ4FYWbplBbfF5xzE6KLN5hZhnOu0MwygI3xrl8tCBEJFOdczNPBWOlQ9x9AnnNuVLmnJgGDoo8HARPjzaoRsIgESgI/lr4bcBmw0MwWRJf9HrgPGGdmg4E1wM/j3YAKsIgESqJaEM652cCBGr49ErENFWARCZTKWgs1iQqwiARKKl2KrAIsIoGiu6GJiHiiG7KLiHiiFoSIiCepVIADfyFG717dWbzoHZYumc0tN1/rO443devWYeY7r/H+B9OYO+8Nbh8+1HekpGqW0ZzbXrqL+3If4c8zHqLXFX33e75Pdn+eWz2BRoc39pQwuY7KfZZ2kx7nyH89ypHjH97vucN/dQHHLH2d8GFNPKWrmkRdiJEMgR4Bh0IhHnl4BOf1uZj8/EI+eH8ak6e8SV7ect/Rkm7Pnr30/dEv+PLL3aSlpTEj9xXefGMmc+cu8B0tKSKREl68J4fVi1ZSr2E97p4ykkWzP2Hd8nyaZTTnhDNPZnP+Jt8xk2rt5cOIbNux37K01i1oeEYnigo2eEpVdRoB1xBdu3RixYrPWbVqDUVFRYwbN5Ef9+vtO5Y3X365G4D09DTS09NS6G1adds3fsHqRSsB+N+X/2PdZ/k0a9UcgEvu+BUv//m5GjEi8q3lbVex6cF/+I5RJYm6IXsyVFqAzayrmXWJPj7ezG4ysz7VH63q2mS2Zm3+urL5/IJC2rRp7TGRX6FQiPc+mMqq1fN4K3c282rJ6PebWmQdwZEntOezBcvo1LMLX6zfwpq8z33HSirnHFn/GMGRrz5C04E/AqDhOadRvGEzez5d5Tld1URcScyTbwdtQZjZncCPgDQzmwGcBswEhplZJ+fciOqPGL+KbhtXm0c5JSUlnHF6X5o2bczYl57i+OO/x5Ily3zHSqq6Depxw5O38MLdYygpjtD/ugu4/7K7fcdKujW/+C2RjVsJN2tK1ph72btyLc2vvoj8wbf7jlZlqfT/eGUj4J9RekOKs4BrgQHOubuB3sCFB/qm8jc5Lin5MmFhD1VBfiFts9qUzWdlZlBYmLq9rUTZvn0ns2Z9QM9zz/YdJanCaWFuePJm3nvtHeZNn0PLI1tzRNtWjHh9FKNmP0mzjOb8aepImh5xmO+o1S6ycWvp163b2fXv92jQ5STSs1rTbuLjHJX7LGmtWnDkhL8RbhH3x515k6hPxEiGyg7CFTvnIsBuM1vhnNsB4Jz7yswOOH4vf5PjtDqZ3n7KufMWcPTR7WnXri0FBesZOLA/l11eO8+EaNGiGUVFRWzfvpN69epyzjlnMmrUk75jJdWVD1zLus8KmP73yQDkf7qGaztfUfb8qNlPcke/m9n1xU5fEZPC6teFUAj35VdY/bo07PZ9Nj/2Ilu6XVz2mqNyn2X1BTd86yBdKqgJvd1YVVaA95pZA+fcbqDz1wvNrCmln+hRo0UiEYYMHc60qS8SDoV4NuflWvcn99datW7J6KdHEg6FCYWMCROmMv31t3zHSprvnXosZ17QnTV5n3PPtL8A8MqDL/DJ2/M9J0u+tOaH0+bRPwBg4TA7psxk9+yPPKdKnJIUakHYwfolZlbXObenguUtgAzn3MLKNuBzBFzT1Eur4ztCjfGTIzr5jlBjDK//le8INcYxS1+v8uf9nNDqtJhrzuINcxL3+UJxOOgIuKLiG12+GdhcLYlERKqgJpzdEKtAX4ghIrVPKrUgVIBFJFCCdBBORCSlaAQsIuKJRsAiIp5EXMR3hJipAItIoKTSpcgqwCISKDXhEuNYqQCLSKBoBCwi4onOghAR8URnQYiIeKJLkUVEPFEPWETEE/WARUQ80QhYRMQTnQcsIuKJRsAiIp7oLAgREU90EE5ExJNUakGEfAcQEUkkdwj/VcbMzjOzT83sMzMbluisGgGLSKAkagRsZmHgMeBcIB+Ya2aTnHNLErIBVIBFJGAS2APuCnzmnFsJYGYvAf2B1CnAxXsLrLq3EQszy3bOjfadoybQvthH+2KfoOyLQ6k5ZpYNZJdbNLrcPsgE1pZ7Lh84reoJ96lNPeDsyl9Sa2hf7KN9sU+t2xfOudHOuVPLTeV/AVVUyBN6hK82FWARkUORD7QtN58FrEvkBlSARUQqNhfoYGbtzawOcBEwKZEbqE0H4VK+t5VA2hf7aF/so31RjnOu2MyuA94AwsAY59ziRG7DUumkZRGRIFELQkTEExVgERFPAl+Aq/tSwlRiZmPMbKOZLfKdxScza2tmb5tZnpktNrMhvjP5Ymb1zOxDM/skui/u8p2pNgl0Dzh6KeEyyl1KCFycyEsJU4mZnQXsAv7pnDvRdx5fzCwDyHDOzTezxsBHwIDa+L4wMwMaOud2mVk6MBsY4pz7wHO0WiHoI+CySwmdc3uBry8lrJWcc+8AW33n8M05V+icmx99vBPIo/Sqp1rHldoVnU2PTsEdldUwQS/AFV1KWCv/R5OKmVk7oBMwx3MUb8wsbGYLgI3ADOdcrd0XyRb0AlztlxJK6jKzRsCrwFDn3A7feXxxzkWccx0pvdKrq5nV2vZUsgW9AFf7pYSSmqL9zleBF5xzE3znqQmcc9uAmcB5fpPUHkEvwNV+KaGknuiBp38Aec65Ub7z+GRmR5jZYdHH9YGewFKvoWqRQBdg51wx8PWlhHnAuERfSphKzGws8D5wjJnlm9lg35k86QZcBvzQzBZEpz6+Q3mSAbxtZv+ldMAywzk3xXOmWiPQp6GJiNRkgR4Bi4jUZCrAIiKeqACLiHiiAiwi4okKsIiIJyrAIiKeqACLiHjy/4LSnnXEcGaIAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm2 = confusion_matrix(y_test, Y_Pred)\n",
    "sns.heatmap(cm2, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2da8a219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6734104046242775"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, Y_Pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff62dae5",
   "metadata": {},
   "source": [
    "## Extreme Learning Machine (ELM)\n",
    "\n",
    "### What is ELM?\n",
    "Extreme learning machines are feed-forward neural networks for classification, regression, clustering, sparse approximation, compression and feature learning with a single layer or multiple layers of hidden nodes, where the parameters of hidden nodes (not just the weights connecting inputs to hidden nodes) need not be tuned. These hidden nodes can be randomly assigned and never updated (i.e. they are random projection but with nonlinear transforms), or can be inherited from their ancestors without being changed. In most cases, the output weights of hidden nodes are usually learned in a single step, which essentially amounts to learning a linear model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59c25630",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[0]\n",
    "hidden_size = 13\n",
    "input_weights = np.random.normal(size=[hidden_size, input_size])\n",
    "biases = np.random.normal(size=[input_size])\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0, x)\n",
    "\n",
    "def lrelu(x):\n",
    "    return np.where(x > 0, x, x * 0.01)\n",
    "\n",
    "def hidden_nodes_relu(X):\n",
    "    G = np.dot(X, input_weights)\n",
    "    G = G + biases\n",
    "    H = relu(G)\n",
    "\n",
    "    return H\n",
    "\n",
    "def hidden_nodes_lrelu(X):\n",
    "    G = np.dot(X, input_weights)\n",
    "    G = G + biases\n",
    "    H = relu(G)\n",
    "\n",
    "    return H\n",
    "\n",
    "output_weights_relu = np.dot(np.linalg.pinv(hidden_nodes_relu(X_train)), pd.DataFrame(y_train))\n",
    "output_weights_lrelu = np.dot(np.linalg.pinv(hidden_nodes_lrelu(X_train)), pd.DataFrame(y_train))\n",
    "\n",
    "def predict_relu(X):\n",
    "    out = hidden_nodes_relu(X)\n",
    "    out = np.dot(out, output_weights_relu)\n",
    "    return out\n",
    "\n",
    "def predict_lrelu(X):\n",
    "    out = hidden_nodes_lrelu(X)\n",
    "    out = np.dot(out, output_weights_lrelu)\n",
    "    return out\n",
    "\n",
    "prediction_relu = predict_relu(X_test)\n",
    "prediction_lrelu = predict_lrelu(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b4c5fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 36.11222522],\n",
      "       [ 29.23991274],\n",
      "       [ 39.18094103],\n",
      "       [ 35.36220648],\n",
      "       [ 41.89856483],\n",
      "       [ 35.3795304 ],\n",
      "       [ 37.39923431],\n",
      "       [ 37.82708773],\n",
      "       [ 34.47155069],\n",
      "       [ 45.12160449],\n",
      "       [ 37.45763847],\n",
      "       [ 37.12241038],\n",
      "       [ 14.07012923],\n",
      "       [ 37.05241815],\n",
      "       [ 39.4098679 ],\n",
      "       [ 37.6005833 ],\n",
      "       [ 39.37502333],\n",
      "       [ 33.85295609],\n",
      "       [ 35.72970189],\n",
      "       [ 31.55526501],\n",
      "       [ 38.88054906],\n",
      "       [ 39.20205361],\n",
      "       [ 29.37614824],\n",
      "       [ 38.75114026],\n",
      "       [ 33.09303374],\n",
      "       [ 41.82425834],\n",
      "       [ 33.666269  ],\n",
      "       [ 39.87312529],\n",
      "       [ 41.58719769],\n",
      "       [ 39.23812188],\n",
      "       [ 41.79806232],\n",
      "       [ 31.43944494],\n",
      "       [ 33.91635254],\n",
      "       [ 35.2143499 ],\n",
      "       [ 34.48351591],\n",
      "       [ 38.59661582],\n",
      "       [ 37.58903885],\n",
      "       [ 34.35130745],\n",
      "       [ 33.60505254],\n",
      "       [ 36.21469342],\n",
      "       [ 35.17539522],\n",
      "       [ 41.72952339],\n",
      "       [ 39.12486374],\n",
      "       [ 35.69096564],\n",
      "       [ 38.17261509],\n",
      "       [ 27.0314391 ],\n",
      "       [ 36.64464895],\n",
      "       [ 37.7017785 ],\n",
      "       [ 47.59855222],\n",
      "       [ 26.84853878],\n",
      "       [ 40.97339205],\n",
      "       [ 40.98712114],\n",
      "       [ 29.97172852],\n",
      "       [ 40.23956625],\n",
      "       [ 35.0399266 ],\n",
      "       [ 35.98687035],\n",
      "       [ 38.86799926],\n",
      "       [ 38.79908257],\n",
      "       [ 30.24962931],\n",
      "       [ 37.68722619],\n",
      "       [  9.9131838 ],\n",
      "       [ 38.00842051],\n",
      "       [ 38.74306363],\n",
      "       [ 38.1289692 ],\n",
      "       [ 39.23242029],\n",
      "       [ 36.68256652],\n",
      "       [ 43.67631838],\n",
      "       [ 37.21850839],\n",
      "       [ 79.55521186],\n",
      "       [ 40.86264368],\n",
      "       [ 38.14549866],\n",
      "       [ 31.07925752],\n",
      "       [ 38.51381593],\n",
      "       [ 29.48539267],\n",
      "       [ 40.33448976],\n",
      "       [ 38.15498169],\n",
      "       [ 41.35401079],\n",
      "       [ 36.77359967],\n",
      "       [ 39.81319058],\n",
      "       [ 31.06098581],\n",
      "       [ 40.73886551],\n",
      "       [ 35.70125035],\n",
      "       [-11.80918104],\n",
      "       [ 41.38006714],\n",
      "       [ 31.05045997],\n",
      "       [ 34.29238453],\n",
      "       [ 37.00767002],\n",
      "       [ 33.98117507],\n",
      "       [ 33.72828007],\n",
      "       [ 37.93792717],\n",
      "       [ 35.83536755],\n",
      "       [ 37.83479378],\n",
      "       [ 36.65686531],\n",
      "       [ 41.01942334],\n",
      "       [ 32.59039149],\n",
      "       [ 40.55963904],\n",
      "       [ 36.05943739],\n",
      "       [ 65.94766333],\n",
      "       [ 38.47695773],\n",
      "       [ 35.10986508],\n",
      "       [ 41.34693078],\n",
      "       [ 36.63390028],\n",
      "       [ 43.47771925],\n",
      "       [ 37.11135792],\n",
      "       [ 38.22171339],\n",
      "       [ 39.73365032],\n",
      "       [ 37.9999975 ],\n",
      "       [ 36.34189771],\n",
      "       [ 34.95052607],\n",
      "       [ 34.01792699],\n",
      "       [ 36.17215263],\n",
      "       [ 33.69632515],\n",
      "       [ 34.60913091],\n",
      "       [ 27.85514417],\n",
      "       [ 39.46306507],\n",
      "       [ 34.48722537],\n",
      "       [ 39.78229131],\n",
      "       [ 30.57397924],\n",
      "       [ 39.57499401],\n",
      "       [ 31.88687468],\n",
      "       [ 36.88015133],\n",
      "       [ 37.54216336],\n",
      "       [ 39.70771651],\n",
      "       [ 36.59195084],\n",
      "       [ 41.17498658],\n",
      "       [ 42.82851918],\n",
      "       [ 36.38585109],\n",
      "       [ 41.54049507],\n",
      "       [ 38.74890214],\n",
      "       [ 36.65520713],\n",
      "       [ 41.50604872],\n",
      "       [ 40.89776308],\n",
      "       [ 31.03129511],\n",
      "       [ 35.40561851],\n",
      "       [ 31.30983355],\n",
      "       [ 36.14832887],\n",
      "       [ 36.07348523],\n",
      "       [ 31.65407007],\n",
      "       [ 36.61677239],\n",
      "       [ 39.77557376],\n",
      "       [ 33.80052407],\n",
      "       [ 42.3911103 ],\n",
      "       [ 36.47410942],\n",
      "       [ 32.82023456],\n",
      "       [ 32.99667389]]), array([[ 36.11222522],\n",
      "       [ 29.23991274],\n",
      "       [ 39.18094103],\n",
      "       [ 35.36220648],\n",
      "       [ 41.89856483],\n",
      "       [ 35.3795304 ],\n",
      "       [ 37.39923431],\n",
      "       [ 37.82708773],\n",
      "       [ 34.47155069],\n",
      "       [ 45.12160449],\n",
      "       [ 37.45763847],\n",
      "       [ 37.12241038],\n",
      "       [ 14.07012923],\n",
      "       [ 37.05241815],\n",
      "       [ 39.4098679 ],\n",
      "       [ 37.6005833 ],\n",
      "       [ 39.37502333],\n",
      "       [ 33.85295609],\n",
      "       [ 35.72970189],\n",
      "       [ 31.55526501],\n",
      "       [ 38.88054906],\n",
      "       [ 39.20205361],\n",
      "       [ 29.37614824],\n",
      "       [ 38.75114026],\n",
      "       [ 33.09303374],\n",
      "       [ 41.82425834],\n",
      "       [ 33.666269  ],\n",
      "       [ 39.87312529],\n",
      "       [ 41.58719769],\n",
      "       [ 39.23812188],\n",
      "       [ 41.79806232],\n",
      "       [ 31.43944494],\n",
      "       [ 33.91635254],\n",
      "       [ 35.2143499 ],\n",
      "       [ 34.48351591],\n",
      "       [ 38.59661582],\n",
      "       [ 37.58903885],\n",
      "       [ 34.35130745],\n",
      "       [ 33.60505254],\n",
      "       [ 36.21469342],\n",
      "       [ 35.17539522],\n",
      "       [ 41.72952339],\n",
      "       [ 39.12486374],\n",
      "       [ 35.69096564],\n",
      "       [ 38.17261509],\n",
      "       [ 27.0314391 ],\n",
      "       [ 36.64464895],\n",
      "       [ 37.7017785 ],\n",
      "       [ 47.59855222],\n",
      "       [ 26.84853878],\n",
      "       [ 40.97339205],\n",
      "       [ 40.98712114],\n",
      "       [ 29.97172852],\n",
      "       [ 40.23956625],\n",
      "       [ 35.0399266 ],\n",
      "       [ 35.98687035],\n",
      "       [ 38.86799926],\n",
      "       [ 38.79908257],\n",
      "       [ 30.24962931],\n",
      "       [ 37.68722619],\n",
      "       [  9.9131838 ],\n",
      "       [ 38.00842051],\n",
      "       [ 38.74306363],\n",
      "       [ 38.1289692 ],\n",
      "       [ 39.23242029],\n",
      "       [ 36.68256652],\n",
      "       [ 43.67631838],\n",
      "       [ 37.21850839],\n",
      "       [ 79.55521186],\n",
      "       [ 40.86264368],\n",
      "       [ 38.14549866],\n",
      "       [ 31.07925752],\n",
      "       [ 38.51381593],\n",
      "       [ 29.48539267],\n",
      "       [ 40.33448976],\n",
      "       [ 38.15498169],\n",
      "       [ 41.35401079],\n",
      "       [ 36.77359967],\n",
      "       [ 39.81319058],\n",
      "       [ 31.06098581],\n",
      "       [ 40.73886551],\n",
      "       [ 35.70125035],\n",
      "       [-11.80918104],\n",
      "       [ 41.38006714],\n",
      "       [ 31.05045997],\n",
      "       [ 34.29238453],\n",
      "       [ 37.00767002],\n",
      "       [ 33.98117507],\n",
      "       [ 33.72828007],\n",
      "       [ 37.93792717],\n",
      "       [ 35.83536755],\n",
      "       [ 37.83479378],\n",
      "       [ 36.65686531],\n",
      "       [ 41.01942334],\n",
      "       [ 32.59039149],\n",
      "       [ 40.55963904],\n",
      "       [ 36.05943739],\n",
      "       [ 65.94766333],\n",
      "       [ 38.47695773],\n",
      "       [ 35.10986508],\n",
      "       [ 41.34693078],\n",
      "       [ 36.63390028],\n",
      "       [ 43.47771925],\n",
      "       [ 37.11135792],\n",
      "       [ 38.22171339],\n",
      "       [ 39.73365032],\n",
      "       [ 37.9999975 ],\n",
      "       [ 36.34189771],\n",
      "       [ 34.95052607],\n",
      "       [ 34.01792699],\n",
      "       [ 36.17215263],\n",
      "       [ 33.69632515],\n",
      "       [ 34.60913091],\n",
      "       [ 27.85514417],\n",
      "       [ 39.46306507],\n",
      "       [ 34.48722537],\n",
      "       [ 39.78229131],\n",
      "       [ 30.57397924],\n",
      "       [ 39.57499401],\n",
      "       [ 31.88687468],\n",
      "       [ 36.88015133],\n",
      "       [ 37.54216336],\n",
      "       [ 39.70771651],\n",
      "       [ 36.59195084],\n",
      "       [ 41.17498658],\n",
      "       [ 42.82851918],\n",
      "       [ 36.38585109],\n",
      "       [ 41.54049507],\n",
      "       [ 38.74890214],\n",
      "       [ 36.65520713],\n",
      "       [ 41.50604872],\n",
      "       [ 40.89776308],\n",
      "       [ 31.03129511],\n",
      "       [ 35.40561851],\n",
      "       [ 31.30983355],\n",
      "       [ 36.14832887],\n",
      "       [ 36.07348523],\n",
      "       [ 31.65407007],\n",
      "       [ 36.61677239],\n",
      "       [ 39.77557376],\n",
      "       [ 33.80052407],\n",
      "       [ 42.3911103 ],\n",
      "       [ 36.47410942],\n",
      "       [ 32.82023456],\n",
      "       [ 32.99667389]])]\n"
     ]
    }
   ],
   "source": [
    "print([prediction_relu, prediction_lrelu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31a72fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_graph(prediction):\n",
    "    prediction = np.array(list(map(int, np.array(prediction).flatten())))\n",
    "    correct = 0\n",
    "    total = X_test.shape[0]\n",
    "    actual_vals = np.array(list(map(int, np.array(y_test))))\n",
    "\n",
    "    for i in range(total - 1):\n",
    "        predicted = prediction[i]\n",
    "        actual = actual_vals[i]\n",
    "        if predicted == actual:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct/total\n",
    "\n",
    "    print(\"Accuracy for \", hidden_size, \" hidden nodes: \", accuracy)\n",
    "    print(\"Total samples: \", X_test.shape[0], \"\\nCorrect Predictions: \", correct)\n",
    "\n",
    "    ind = np.arange(total)\n",
    "    actual_vs_pred = pd.DataFrame({\n",
    "        'Index': ind,\n",
    "        'Actual': np.array(y_test),\n",
    "        'Predicted': prediction\n",
    "    }, index=ind)\n",
    "\n",
    "    diff = []\n",
    "\n",
    "    # print(prediction)\n",
    "    # print(y_test)\n",
    "    for i in range(prediction.size):\n",
    "        diff.append(abs(prediction[i] - np.array(y_test)[i]))\n",
    "\n",
    "    fig= plt.figure(figsize=(16,4))\n",
    "    plt.plot(ind, diff)\n",
    "\n",
    "    plt.title('Error margins for Extreme Learning Model')\n",
    "    plt.ylabel('Error Margin')\n",
    "    plt.xlabel('Test sample Number')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79386972",
   "metadata": {},
   "source": [
    "Dataset Accuracy Values for ELM\n",
    "\n",
    "1. 0.40336134453781514\n",
    "2. 0.42777777777777776\n",
    "3. 0.43641618497109824\n",
    "4. 0.4084507042253521\n",
    "5. 0.3684210526315789\n",
    "6. 0.39166666666666666\n",
    "7. 0.1305732484076433\n",
    "8. 0.1103448275862069"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53818935",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "### What is Linear Regression?\n",
    "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. For example, a modeler might want to relate the weights of individuals to their heights using a linear regression model.\n",
    "\n",
    "### How does it work?\n",
    "A linear regression line has an equation of the form Y = a + bX, where X is the explanatory variable and Y is the dependent variable. The slope of the line is b, and a is the intercept (the value of y when x = 0). \n",
    "### Advantages\n",
    "1. Linear Regression is a very simple algorithm that can be implemented very easily to give satisfactory results.\n",
    "2. Linear regression fits linearly seperable datasets almost perfectly and is often used to find the nature of the relationship between variables.\n",
    "3. Overfitting can be reduced by regularization. <strong>Overfitting</strong> is a situation that arises when a machine learning model fits a dataset very closely and hence captures the noisy data as well.This negatively impacts the performance of model and reduces its accuracy on the test set. <strong>Regularization</strong> is a technique that can be easily implemented and is capable of effectively reducing the complexity of a function so as to reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d59e4a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73fd94bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination is:  0.5317174721103257\n",
      "Model intercept:  -6.054430506751153\n",
      "Slope:  [ 2.96547935e-03  8.89948015e-03  2.84538504e-01  1.12440048e-02\n",
      "  3.72476033e-01  2.17551740e-01 -1.90805075e-01  4.97894323e-01\n",
      "  1.41310959e-01  5.90243090e+00  5.62982668e+00  4.90937084e+00\n",
      "  4.70038035e+00]\n"
     ]
    }
   ],
   "source": [
    "    r_sq = model.score(X_train, y_train)\n",
    "    print('Coefficient of determination is: ', r_sq)\n",
    "    print('Model intercept: ', model.intercept_)\n",
    "    print(\"Slope: \", model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "394484f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:  [1, 2, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 0, 2, 0, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 2, 1, 1, 3, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 0, 2, 1, 2, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 2, 1, 2, 0, 2, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 0, 0, 1, 1, 1, 2, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 0, 1, 1, 1, 2, 1, 3, 2, 3, 3, 1, 0, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 0, 1, 1, 2, 2, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 3, 1, 1, 3, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 0, 1, 1, 1, 2, 1, 1, 2, 3, 0, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 3, 1, 1, 0, 0, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 0, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "y_pred = list(map(int, model.predict(X_test)))\n",
    "print(\"Predicted values: \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c44b367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.3204\n",
      "Variance score: 0.5100\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean squared error: %.4f\" % np.mean((model.predict(X_test) - y_test) ** 2))\n",
    "print('Variance score: %.4f' % model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "024f4b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45375722543352603"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
