{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce8c3a0",
   "metadata": {},
   "source": [
    "## Feature Classification\n",
    "\n",
    "In this notebook we...\n",
    "\n",
    "### Novelty\n",
    "...\n",
    "\n",
    "\n",
    "#### REFRENCES:\n",
    "1. Mahana, M., Johns, M., & Apte, A. (2012). Automated essay grading using machine learning. Mach. Learn. Session, Stanford University.\n",
    "\n",
    "2. Suresh, A., & Jha, M. (2018). Automated essay grading using natural language processing and support vector machine. International Journal of Computing and Technology, 5(2), 18-21.\n",
    "\n",
    "3. Rokade, A., Patil, B., Rajani, S., Revandkar, S., & Shedge, R. (2018, April). Automated Grading System Using Natural Language Processing. In 2018 Second International Conference on Inventive Communication and Computational Technologies (ICICCT) (pp. 1123-1127). IEEE.\n",
    "\n",
    "4. Song, S., & Zhao, J. (2013). Automated essay scoring using machine learning. Stanford University.\n",
    "\n",
    "5. Kakkonen, T., Myller, N., & Sutinen, E. (2006). Applying Part-of-Seech Enhanced LSA to Automatic Essay Grading. arXiv preprint cs/0610118."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a52152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ba88a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>lemma_count</th>\n",
       "      <th>spell_err_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neu_score</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>350</td>\n",
       "      <td>16</td>\n",
       "      <td>4.237143</td>\n",
       "      <td>162</td>\n",
       "      <td>0.045714</td>\n",
       "      <td>0.237143</td>\n",
       "      <td>0.051429</td>\n",
       "      <td>0.211429</td>\n",
       "      <td>0.068571</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.123651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>423</td>\n",
       "      <td>20</td>\n",
       "      <td>4.312057</td>\n",
       "      <td>185</td>\n",
       "      <td>0.061466</td>\n",
       "      <td>0.252955</td>\n",
       "      <td>0.044917</td>\n",
       "      <td>0.200946</td>\n",
       "      <td>0.044917</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.121286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>283</td>\n",
       "      <td>14</td>\n",
       "      <td>4.342756</td>\n",
       "      <td>145</td>\n",
       "      <td>0.031802</td>\n",
       "      <td>0.289753</td>\n",
       "      <td>0.070671</td>\n",
       "      <td>0.183746</td>\n",
       "      <td>0.056537</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.151787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>530</td>\n",
       "      <td>27</td>\n",
       "      <td>4.813208</td>\n",
       "      <td>236</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>0.335849</td>\n",
       "      <td>0.079245</td>\n",
       "      <td>0.183019</td>\n",
       "      <td>0.054717</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.126890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>473</td>\n",
       "      <td>30</td>\n",
       "      <td>4.334038</td>\n",
       "      <td>190</td>\n",
       "      <td>0.035941</td>\n",
       "      <td>0.241015</td>\n",
       "      <td>0.067653</td>\n",
       "      <td>0.190275</td>\n",
       "      <td>0.076110</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.130996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>1783</td>\n",
       "      <td>Dear @CAPS1, @CAPS2 several reasons on way I t...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>509</td>\n",
       "      <td>21</td>\n",
       "      <td>4.015717</td>\n",
       "      <td>206</td>\n",
       "      <td>0.062868</td>\n",
       "      <td>0.223969</td>\n",
       "      <td>0.068762</td>\n",
       "      <td>0.212181</td>\n",
       "      <td>0.076621</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.076209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>1784</td>\n",
       "      <td>Do a adults and kids spend to much time on the...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>214</td>\n",
       "      <td>18</td>\n",
       "      <td>4.004673</td>\n",
       "      <td>109</td>\n",
       "      <td>0.032710</td>\n",
       "      <td>0.247664</td>\n",
       "      <td>0.060748</td>\n",
       "      <td>0.214953</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.100699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>1785</td>\n",
       "      <td>My opinion is that people should have computer...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>296</td>\n",
       "      <td>18</td>\n",
       "      <td>4.489865</td>\n",
       "      <td>100</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.277027</td>\n",
       "      <td>0.050676</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.143533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>1786</td>\n",
       "      <td>Dear readers, I think that its good and bad to...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>3.733333</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.051164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>1787</td>\n",
       "      <td>Dear - Local Newspaper I agree thats computers...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>217</td>\n",
       "      <td>18</td>\n",
       "      <td>4.069124</td>\n",
       "      <td>120</td>\n",
       "      <td>0.059908</td>\n",
       "      <td>0.239631</td>\n",
       "      <td>0.082949</td>\n",
       "      <td>0.188940</td>\n",
       "      <td>0.087558</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.091269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1783 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id                                              essay  \\\n",
       "0            1  Dear local newspaper, I think effects computer...   \n",
       "1            2  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2            3  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3            4  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4            5  Dear @LOCATION1, I know having computers has a...   \n",
       "...        ...                                                ...   \n",
       "1778      1783  Dear @CAPS1, @CAPS2 several reasons on way I t...   \n",
       "1779      1784  Do a adults and kids spend to much time on the...   \n",
       "1780      1785  My opinion is that people should have computer...   \n",
       "1781      1786  Dear readers, I think that its good and bad to...   \n",
       "1782      1787  Dear - Local Newspaper I agree thats computers...   \n",
       "\n",
       "      domain1_score  word_count  sent_count  avg_word_len  lemma_count  \\\n",
       "0               8.0         350          16      4.237143          162   \n",
       "1               9.0         423          20      4.312057          185   \n",
       "2               7.0         283          14      4.342756          145   \n",
       "3              10.0         530          27      4.813208          236   \n",
       "4               8.0         473          30      4.334038          190   \n",
       "...             ...         ...         ...           ...          ...   \n",
       "1778            8.0         509          21      4.015717          206   \n",
       "1779            7.0         214          18      4.004673          109   \n",
       "1780            8.0         296          18      4.489865          100   \n",
       "1781            2.0          15           1      3.733333           14   \n",
       "1782            7.0         217          18      4.069124          120   \n",
       "\n",
       "      spell_err_count  noun_count  adj_count  verb_count  adv_count  \\\n",
       "0            0.045714    0.237143   0.051429    0.211429   0.068571   \n",
       "1            0.061466    0.252955   0.044917    0.200946   0.044917   \n",
       "2            0.031802    0.289753   0.070671    0.183746   0.056537   \n",
       "3            0.122642    0.335849   0.079245    0.183019   0.054717   \n",
       "4            0.035941    0.241015   0.067653    0.190275   0.076110   \n",
       "...               ...         ...        ...         ...        ...   \n",
       "1778         0.062868    0.223969   0.068762    0.212181   0.076621   \n",
       "1779         0.032710    0.247664   0.060748    0.214953   0.046729   \n",
       "1780         0.027027    0.277027   0.050676    0.189189   0.027027   \n",
       "1781         0.000000    0.133333   0.200000    0.133333   0.000000   \n",
       "1782         0.059908    0.239631   0.082949    0.188940   0.087558   \n",
       "\n",
       "      neg_score  pos_score  neu_score  cosine_similarity  \n",
       "0         0.000      0.170      0.830           0.123651  \n",
       "1         0.014      0.219      0.766           0.121286  \n",
       "2         0.045      0.197      0.759           0.151787  \n",
       "3         0.008      0.152      0.840           0.126890  \n",
       "4         0.026      0.096      0.879           0.130996  \n",
       "...         ...        ...        ...                ...  \n",
       "1778      0.019      0.097      0.883           0.076209  \n",
       "1779      0.007      0.173      0.820           0.100699  \n",
       "1780      0.022      0.118      0.861           0.143533  \n",
       "1781      0.175      0.275      0.550           0.051164  \n",
       "1782      0.072      0.159      0.769           0.091269  \n",
       "\n",
       "[1783 rows x 16 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('features/features_set_3.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98898f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.iloc[:,3:]\n",
    "y=data.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52792405",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(X ,y, test_size=0.2 , random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c566d0",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "### What is Random Forest?\n",
    "Random Forest is a powerful and versatile supervised machine learning algorithm that grows and combines multiple decision trees to create a “forest”.  A decision tree is another type of algorithm used to classify data. In very simple terms, it is like a flowchart that draws a clear pathway to a decision or outcome; it starts at a single point and then branches off into two or more directions, with each branch of the decision tree offering different possible outcomes.\n",
    "\n",
    "### How does it work?\n",
    "Random Forest grows multiple decision trees which are merged together for a more accurate prediction.\n",
    "\n",
    "The logic behind the Random Forest model is that multiple uncorrelated models (the individual decision trees) perform much better as a group than they do alone. When using Random Forest for classification, each tree gives a classification or a “vote.” The forest chooses the classification with the majority of the “votes.” When using Random Forest for regression, the forest picks the average of the outputs of all trees.\n",
    "\n",
    "### Advantages\n",
    "1. Very easy to use.\n",
    "2. Random Forest is much more efficient than a single Decision Tree while performing analysis on a large database.\n",
    "3. Random forests have a very good accuracy.\n",
    "4. They are versatile (can be used for regression or classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fcab3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(class_weight='balanced_subsample')\n",
    "rf_params = {'n_estimators':list(range(20,200,10)),\n",
    "                'max_depth':list(range(2,14,1))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd6b6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = sklearn.metrics.make_scorer(sklearn.metrics.f1_score, average = 'macro')\n",
    "rf_random=GridSearchCV(estimator = rf, param_grid  = rf_params, cv = 5, verbose=2,  n_jobs = 2, scoring=scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af108920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=RandomForestClassifier(class_weight='balanced_subsample'),\n",
       "             n_jobs=2,\n",
       "             param_grid={'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
       "                         'n_estimators': [20, 30, 40, 50, 60, 70, 80, 90, 100,\n",
       "                                          110, 120, 130, 140, 150, 160, 170,\n",
       "                                          180, 190]},\n",
       "             scoring=make_scorer(f1_score, average=macro), verbose=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1529f797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced_subsample', max_depth=8,\n",
       "                       n_estimators=180, random_state=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_final = RandomForestClassifier(random_state=0, n_estimators=rf_random.best_params_['n_estimators'], max_depth=rf_random.best_params_['max_depth'],class_weight='balanced_subsample')\n",
    "rf_final.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76399166",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = rf_final.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe9a7087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         2.0      0.750     1.000     0.857         3\n",
      "         4.0      0.500     0.333     0.400         3\n",
      "         5.0      0.000     0.000     0.000         1\n",
      "         6.0      0.700     0.500     0.583        28\n",
      "         7.0      0.458     0.239     0.314        46\n",
      "         8.0      0.458     0.767     0.574        86\n",
      "         9.0      0.509     0.326     0.397        86\n",
      "        10.0      0.385     0.566     0.458        53\n",
      "        11.0      0.350     0.194     0.250        36\n",
      "        12.0      0.375     0.200     0.261        15\n",
      "\n",
      "    accuracy                          0.457       357\n",
      "   macro avg      0.449     0.413     0.409       357\n",
      "weighted avg      0.466     0.457     0.435       357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(X_pred,y_test,digits=3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bf0da66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVoAAAD4CAYAAACt8i4nAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAuY0lEQVR4nO3dd3wUdf7H8ddn0wgQSqgBC/bDAiiIAoIgRUBFrChHOU/lPDnbWY/z5JSz9ztOBSsCUkROUbqRIojUA+lVxNBbJAQEknx+f+zCL2KS3SUzk925z/Me88hmk/2+Z0zuw+S735mPqCrGGGPcEyjrHTDGGL+zQmuMMS6zQmuMMS6zQmuMMS6zQmuMMS5LdDsgPe0sT5Y15B752YsYAPILCjzLMuZ/Sd7hzVLaMY7s2hBxzUmqfnqp8yJhZ7TGGOMy189ojTHGUwX5Zb0Hv2KF1hjjL/l5Zb0Hv2KF1hjjK6qx9x6KFVpjjL/E4JvVVmiNMf4Sg2e0MbXqICUlmanTxjDzm3F8M28Cj/W717WsQYNe4sdN/2XRwi9dyyjsyg6tWb5sJqtWzOKRh/vGfY5lxVeWH4+pWAX5kW8eEbfv3hXtOtoKFcqTm3uAxMREJk4ZyV8e/QcL5i8O+7po19Fedtkl7N+fy3vvvsZFjdtF9dpo19EGAgFWLv+ajp1vJStrK9/OmUCPnnezcuXaqMaJlRzLiq+seDomJ9bRHt64IOKak1yvSWysoxWR34jIoyLyTxF5PfS4vls7lJt7AICkpEQSkxJx6x+CWbPmsndvtitjH6/pxReyfv1Gvv9+E0eOHGH06M/ocs2VcZtjWfGV5cdjKonm50W8eaXEQisijwIjAQHmAfNDj0eIyGOu7FAgwIzZ41i94VumT5vNwgVL3IjxVJ26tfkxa8uxz7M2b6VOndpxm2NZ8ZXlx2MqUUFB5JtHwr0ZdjtwnqoeKfykiLwCLAeeK+pFItIH6ANQPqUGKUmVI96hgoICLm/RhUqV0xj60RvUr3+WK3/ieEnk13+duHGm7lWOZcVXlh+PqURx+GZYAVCniOczQl8rkqoOVtUmqtokmiJb2L6fcpj99Vzatm91Qq+PJZuztnLySf//n/Gkuhls3bo9bnMsK76y/HhMJYrBN8PCFdr7gUwRmSgig0PbJCATuM/pnalWPZ1KldMAKFcuhcvbNGfNmg1Ox3hu/oLFnHnmadSrdzJJSUncfPO1fP7FlLjNsaz4yvLjMZVICyLfPFLi1IGqThKRs4GmQF2C87NZwHxVdfyfg1q1avDGoBdISAgQCAT4dOxEpkya5nQMAB9+OJBWLS+levV01q+bx4B/vMwHH4xyJSs/P5/77n+cCeM/IiEQ4IMho1ixYk3c5lhWfGX58ZhK3onYuwQ35pZ3nSi7TaIx8c+J5V2Hvpsccc1JaXClJ8u77MowY4yvuPDHdqnF1JVhxhhTag7O0YpIFREZIyKrRGSliDQTkXQRmSoia0Mfq4YbxwqtMcZfnF1H+zowSVV/AzQEVgKPAZmqehbBhQFhrymwQmuM8ReHzmhFpBLQCngXQFUPq2o2cC0wJPRtQ4Cu4XbJCq0xxl/yj0S8iUgfEVlQaOtTaKTTgZ3A+yLyXxF5R0QqALVUdStA6GPNcLtkb4YZY/wlilVBqjoYGFzMlxOBi4B7VHWuiLxOBNMExQ3kqn2HDrgdAUBCwLuTcy+zbCmZMVFy7kKELCBLVeeGPh9DsNBuF5EMVd0qIhnAjnAD2dSBMcZfHHozTFW3AT+KyDmhp9oCK4BxQO/Qc72Bz8Ltkk0dGGP8xdm/Au8BhotIMrABuI3gCepoEbkd2ATcFG4QK7TGGF/R/CPhvynSsVQXA02K+FLbaMaxQmuM8ZcYvE2iFVpjjL/E4BvIVmiNMf4Sg2e0MbfqwKsOml52wfUyy6/dTi0rPnK8zipSDLayialCGwgE+OfrT3P1NT24oGEbunXrSv36Z7mSNXTox1zTpacrY5dVlpf//SwrfrL8eEwlisEbf8dUofWyg6aXXXC9yvJrt1PLio8cr7OKlZcX+eaREy60InKbkzsCMdJBM475tdupZcVHjtdZxfLZGe2TxX2h8I0aCgpyIx4wJjpoxjG/dju1rPjI8TqrWDE4R1viqgMR+a64LwG1intd4Rs1JCbXjfi/ckx00Ixjfu12alnxkeN1VrHicNVBLaAXcE0R226ndyYmOmjGMb92O7Ws+MjxOqtY8XZGC3wBVAxdhvYLIjLd6Z3xsoOml11wvcrya7dTy4qPHK+zihWDZ7Sud8GNZuqgNLy8daGX7DaJ5n+JE11wD45+KuKak3rzE9YF1xhjohaDb6BboTXG+EsM/hVohdYY4y9WaI0xxmUx+GaYFVpjjL/k55f1HvyKbwptfkGBZysPvFwJYI0gjYlSDP4e+6bQ+nV5lzEmSlZojTHGZTZHa4wx7tICW0drjDHusqkDY4xxma06MMYYl9kZrTHGuMzBQisiG4EcIB/IU9UmIpIOjALqARuBm1V1b0njxNyaKD92wQV/HpdfO6v6McuPx1Qs1ci3yLRR1Uaq2iT0+WNApqqeBWSGPi9RTBVav3bB9eNx+bWzqh+z/HhMJXL/xt/XAkNCj4cAXcO9IGyhFZHfiEhbEal43PMdT2QPS+LXLrh+PC6/dlb1Y5Yfj6lEBRrxVri/YWjrc9xoCkwRkYWFvlZLVbcChD7WDLdLJRZaEbkX+Ay4B1gmItcW+vIzkR53pGKig6YL/Hhcfu2s6scsPx5TifLzI95UdbCqNim0DT5utBaqehHQCegrIq1OZJfCvRl2J9BYVfeLSD1gjIjUU9XXCTZoLFKo8vcBkITKBAIVItqZmOig6QI/HpdfO6v6McuPx1QSdfDNMFXdEvq4Q0T+AzQFtotIhqpuFZEMYEe4ccJNHSSo6v5Q0EagNdBJRF6hhEJb+F+JSIssxEgHTRf48bj82lnVj1l+PKYSRTF1UBIRqSAiaUcfAx2AZcA4oHfo23oT/Ku/ROEK7TYRaXT0k1DRvRqoDlwQbvBoxUQHTRf48bj82lnVj1l+PKYSaUHkW8lqAbNEZAkwDxivqpOA54D2IrIWaB/6vEThpg56AXm/OAbVPKCXiAwKN3i0/NoF14/H5dfOqn7M8uMxlcihex2o6gagYRHP7wbaRjOWdcE9AXY/WmPc4UQX3Nwnbom45lR4aqR1wTXGmKjZbRKNMcZldptEY4xxl5PLu5xihdYY4y92RmuMMS6zQusev75jflWtCz3LWpDzvWdZ2w9ke5Lj198LUwK78bcxxrjLeoYZY4zbrNAaY4zLYnC6yAqtMcZf7IzWGGNcZoXWGGPcpfmxN3UQUz3DwL9N5NzM6vvivby/8ENem/KvX33t2j5dGfvDONKqpjmaCTB78SSmzBrLxBkf80XmSMfHP8qvjTS9zPLjMRXLofvROimmCq1fm8i5nTXt40wG9P77r56vllGdBpc1YmdW2BvAn7BuXX5Pp8tv4uq2t7iW4ddGmtac0R1aoBFvXompQuvXJnJuZ62Yt5yc7P2/ev73T9zO0Gc/iPu2OX5tpGnNGV0Sj2e0ItJURC4OPT5XRP4sIp3d2Bm/NpEri4Z1F7dryu5tu9m4cqNrGarKsE8GMf6rUXTvfaNrOV7y4++FH4+pRAVRbB4p8c0wEelPsPtjoohMBS4BpgOPiciFqvp0Ma+L+eaMfs0CSC6XzA1/uomnevZ3LQPghk692L5tJ9WqpzN87GDWrfmeeXMWuprpNj/+XvjxmEqiefH3ZtiNQAugFdAX6KqqTwFXAt2Ke1E8NGf0axZA7VMzqHVyLV6Z+DpvzXqbahnVeWn8a1SpUcXRnO3bdgKwe9ceJo/PpFHj8x0dvyz48ffCj8dUohg8ow1XaPNUNV9VDwDrVXUfgKoexIXd9GsTOa8b1m1a/QO3Ne7FXZfdyV2X3cnurbt46Kr7yd6Z7VhGavlUKlQsf+xxyzbNWb1ynWPjlxU//l748ZhKEotvhoVbR3tYRMqHCm3jo0+KSGVcKLR+bSLndtYD/3yI85udT1rVSrz97XuMfHUEmaOmOjZ+UWrUqMbgoa8BkJiYwKdjJjAjc7YrWX5tpGnNGV0SezMHJTdnFJEUVT1UxPPVgQxVXRouwKvmjH7VJaNx+G9yiN0m0ZQ1J5oz7rnu8ohrTvp/ZpR9c8aiimzo+V3ALlf2yBhjSiMG/221S3CNMb6ieWW9B78WUxcsGGNMaWlB5FskRCRBRP4rIl+EPk8Xkakisjb0sWq4MazQGmP8xfnlXfcBKwt9/hiQqapnAZmhz0tkhdYY4ytOntGKyEnAVcA7hZ6+FhgSejwE6BpuHCu0xhhfiabQikgfEVlQaOtz3HCvAY/wy/PfWqq6FSD0sWa4fbI3w2Lcktwsz7JOL1/Lsyyv7Pn51zfbccvPeYc9yzLF0/zIV2yp6mBgcFFfE5GrgR2qulBEWpdmn6zQGmN8JdI3uSLQAugSuolWOaCSiAwDtotIhqpuFZEMIOx9SG3qwBjjK1ogEW8ljqP6F1U9SVXrAbcAX6lqD2Ac0Dv0bb2Bz8Ltk53RGmN8xcEz2uI8B4wWkduBTcBN4V5ghdYY4yuqzl9Vq6rTCd4iFlXdDbSN5vVWaI0xvuLBGW3UrNAaY3ylIIpVB16JuTfD/Nqt08ustEoVGfjeC0yZ8wmTv/mEC5s0cGTcR156iP8s/pj3v3z72HOXX9WK9zPf4atNUzinwdmO5BTFq467detmMGHiRyxcNJX5CyZz992/cy0LrAuuG5x6M8xJJd4m0QnR3CYxEAiwcvnXdOx8K1lZW/l2zgR69LyblSvXOr5f8ZJ1aqXo17a+OPBJ5n/7X0YP+5SkpETKpZYjZ1/49aQnl0sv8esNLrmAg7kH6ffao9zW7k4ATjnzFLSggAeff4A3Bwxi9XeR3Xt0w4Ho7ro/e/Ekrr7iFvbuyY7qddGuo61Vuwa1a9dkyeLlVKxYga9nf86t3fqwalX4m5pHu47Wq9/BePldB2duk7ixUfuIa069xVM9qbZRn9GKyIdu7Aj4t1unl1kVK1bg4mYXMXrYpwAcOZIXUZGNxHdzl5KTnfOL5zat28SPG7y7qMJt27ftZMni5QDs35/L6tXryHCpuaB1wXWHauSbV0ostCIy7rjtc+D6o587vTN+7dbpZdbJ9eqyZ/deXvjX3xn31Uc889rfSC1fzpUsL5VFx91TTqlLw4bnsmD+YlfGty647ojFqYNwZ7QnAfuAV4CXQ1tOocdFKnz9cEFBbsQ749dunV5mJSYmcF6D3zD8/TF0uaI7B3MPcte9t7mS5aUbOvXiqjbd6HXzH+l1+y00beZu54kKFcozfMSbPPrIAHJy3LmM17rgukNVIt68Eq7QNgEWAn8FfgqtJTuoqjNUdUZxL7IuuGWXtXXLDrZt2cGSRcsAmPh5Juc1/I0rWV7ysuNuYmIiwz96k1EjP2PcZ5Ndy7EuuO7Iz5eIN6+UWGhVtUBVXwVuA/4qIgNxcUmYX7t1epm1a8dutm7ezmlnngpA81ZNWbfau15gbvC64+4bbz7P6tXrGPivd13LAOuC65ZYPKONqGiqahZwk4hcRXAqwRV+7dbpdWfQJ//yPK++9TRJSUn8+EMWj9zzd0fG/dvAfjRq1pDK6ZX5eP4I3n95CPuyc7hvwJ+onF6ZZ4c8zbrl63mkR9j7IEfFy467zZo1oftvr2fZ0lV88+14AP7e/0WmTJ7ueJZ1wXWHl3OvkYqp5V3m105kedeJCre8y0nRLu86UXabxPjixPKulWd1jrjm1F87oey74BpjTLyJxTNaK7TGGF/JL4i5C16t0Bpj/MXj1WQRsUJrjPGVAg9XE0TKCq0xxle8XLYVKSu0xhhfsakDE7XkgHc/ojsLvLsm/eY1RTYeddzaS+7xJAfg1ty9nmWtyv7Rs6z8ghi8k3YJbOrAGGNcZqsOjDHGZTE4c2CF1hjjLzZ1YIwxLrNVB8YY47JYfOvOCq0xxlcUO6MN68oOrXnlladICAR47/0RvPDivy0rCvXOOIVX3n7m2Ocnn1qHfz0/mA8Hl75zbPk66TR7/S5Sa1ZGC5R1w6ax+t3JXPbWn0g7IwOA5ErlObzvABPb/7XUefty9tP/uddYt+EHEGFAvwdodH59hn/8GSM++ZyEhARaNW/Kg31vL1WOJCdRb+TzSHISJCSQM2k2O18fTo17u1Ol25Xk7wneGXTHy0PYP31BqY+rsB59unH9b69BFdauXM8T9z/N4UPO3wVs0KCX6NypLTt37uaixu0cH78wL/9/VZQ8h6YORKQcMBNIIVgrx6hqfxFJB0YB9YCNwM2qWuLavpgqtIFAgH++/vQvOmh+/sUU17p1+jFr4/pNXH9Fj2O5078bz5cTpjsydkFeAYue+oi9SzeSWKEcnSYNYOvMpcy6a+Cx77noie4czjngSN5zr71Fi0ua8OrTj3PkyBEO/nyIeQuXMG3Wt4z98A2Sk5PZvTe71Dl6+Agbe/RDD/wMiQmcNupF9s8IFtQ973/G7nfGljqjKDVrV6f7HTdxXavuHPr5MC8MHkDHru0YN2qC41lDh37Mm29+wHvvvub42IV5+bteHAfPaA8BV6jqfhFJAmaJyETgeiBTVZ8TkceAx4BHSxooqgVnInKZiPxZRDqc6J6XxK/dOsuqM+ilrS7mx41ZbMna5sh4P+/IZu/SjQDk5f7MT+u2UD7jl/ewPaXLJfzw6ZxSZ+3PzWXhkmXcEPrvlJSURKW0ioz6dDy397iZ5ORkAKpVrVLqLCBYZAFJTITEBM/WCCUkJJBSLoWEhARSU8uxc9suV3JmzZrLXgf+UQonFrrgFkSxlUSDjt7QOCm0KXAtMCT0/BCga7h9CtcFd16hx3cCA4E0oH+okjvKr906y6ozaOeu7Rk/1p02IhVOqk76+aeya9H6Y8/VvOQcft75Eznfl/6m3lmbt1G1SmUef/oVbvxdX5549jUOHPyZjZs2s3DJMm69835+1/dhlq5cXeosAAIBTv/8X5wzbzi5sxdzcElw3Ko9r+b08QPJeO4+ApUqOpMVsmPbLoa8OYLJC//Dl9+NI2fffubMmBf+hTEsJrrgIhFvhRvJhrY+hccSkQQRWQzsAKaq6lyglqpuBQh9rBlun8Kd0SYVetwHaK+qTwIdgN8W9yLrglt2WUclJSVyxZWtmPx5puNjJ5ZPoeU797HwiWHk7T947PlTuzZjowNnswB5+fmsXLOObtddxZgP/k1qajneHTqa/Px89uXs56PBr/Jg3zt46G/POvPfsqCADdfcw5oWvUlteDYpZ5/KnuETWNfmDjZcfQ95O/dSq1/p5oKPl1Y5jTYdW9K56Y20b9iF1PKpXHWDt2d/TouFLrjRnNEWbiQb2n5xbbiq5qtqI4IdwZuKyAl1BQ1XaAMiUlVEqhFse7MzFJ4L5BX3IuuCW3ZZR7Vs25wVS1exe+ceR8eVxARavnMfG8d+w48T//+NIUkIcHLni/lh3FxHcmrXrE6tGtVpcF6wg2+H1pexYs06atWsTrvLWyAiXHDuOYgIe7N/ciQToCAnl9xvv6Niq8bk786GggJQJXvkJFIbnu1YDsClrZqwedMW9u7OJi8vn8wJ02l48QWOZngtJrrgIhFvkVLVbGA60BHYLiIZAKGPO8K9PlyhrUyw3fgCIF1EaocGrwjOr6Hwa7fOsugMetV1HVyZNrj05TvYt3YLqwZP/MXztVuez751Wzi41ZnCXr1aOrVr1uD7H7IA+HbhYs6odwpXtGzGvIWLAdi4KYsjeXlUrVK5VFkJ6ZUIpAVPCCQlmYotGnFo/Y8k1qh67HvSOjTn0JofSpVzvG1Z22nQ+DzKpaYAcEnLJny/dqOjGV6LhS64BRL5VhIRqSEiVUKPU4F2wCpgHNA79G29gc/C7VOJqw5UtV5xxwJcF27waPm1W6fXnUHLpabQ/PJL6P/Qs46OW6Pp2Zx+U0v2rthEp6lPA7Dk2dFs+WoJp157qSNvghXW74E/8uiTL3Ak7wgn18lgQL8HKJ9ajsefeZWuPe4iKSmRZx5/sMg/V6ORWCOdOi/+GUkIQEDYN34W+6fNp85LD1Lu3NNBlSNZO9j6+L8cOrKgpf9dwdQvpjFyygfk5+ezaukaxgwN+//ZE/LhhwNp1fJSqldPZ/26eQz4x8t88MEox3NioQtugXPngBnAEBFJIHhSOlpVvxCROcBoEbkd2ATcFG4g64Ib486qUtezrMcTnf3TuCQ3f/eUJzl2m8TS8/I2iU50wf20dveIa07XbR9ZF1xjjImWXYJrjDEuKyjlVJIbrNAaY3wlv6x3oAhWaI0xvhJuNUFZsEJrjPEVB1cdOMYKbYxbm73Zs6x7Urx713xgQ2evsirOgwV1wn+TY7z771chqZxnWfEmFpc5WaE1xviKTR0YY4zLbHmXMca4LN/OaI0xxl12RmuMMS6zQmuMMS6LwW7jVmiNMf4Si2e0UfUM88KVHVqzfNlMVq2YxSMP97WsGM1JSUlm6rQxzPxmHN/Mm8Bj/e51dPy/vvwI45eMZVjme8eeq1QljddHvMjoWUN5fcSLpFV2prVMap10Wo35Kx1mvkD76c9z5h3BLgeVzzuVNl88Sbupz3DFpAFUbXS6I3mF9ejTjbEzhvHJ9GE89+aTJKckO57h9s+qrLKKkx/F5pWYKrRHO2hefU0PLmjYhm7dulK//lmWFWM5AIcOHabr1b1o1bwLrZp3oW27VjS5uJFj448fPYkHfvvLxqI9+3ZnwaxF3HxZTxbMWkTPvt0dydK8Ar57cjhTWj3CtKv6c8bv2pN2dl0a/O1WVr4yli/b92PFC2No8LdbHck76mgX3Fuv/D03tO5BICFAx67OtwJ3+2dVVlnFcerG304K15zxEhGpFHqcKiJPisjnIvK8iJTutvZF8GtnWq+yvO5AmpsbbCuelJRIYlKio72hFs/9jn3Z+37xXMsrmzPh48kATPh4Mq06tnAk6+cd2WQX6u6bs3YLqbWroqokVkwFIKlSeQ5uy3YkrzCvuuC6+bMqy6yiONUF10nhzmjfAw6EHr9OsLXN86Hn3nd6Z/zamdarLK87kAYCAWbMHsfqDd8yfdpsFi5Y4loWQHr1dHbvCLbK2b1jD1WrVQ3ziuiVP6k6VS44lT2L1rPkiaE0eOJWOi/4Jw2e6M6yZ53tSOBlF1wvf1Ze/14cLx4LbUBVjzZhbKKq96vqrFAn3GInrKwLbtlked2BtKCggMtbdOH837TkosYNXJum8EpC+RSavXs/i58YSt7+g5zeqx1L+g9jQpN7WdJ/GI1fvtPRPC+74Hr5syrr3wuNYvNKuEK7TERuCz1eIiJNAETkbOBIcS+yLrhlk1VWHUj3/ZTD7K/n0rZ9K1dz9uzaQ7Wa6QBUq5nO3t3O3cRFEhNo9u79bBo7my0Tgt19693cks3j5wOQ9flc0i88w7E8KJsuuF79rLzOKizu5miBO4DLRWQ9cC4wR0Q2AG+HvuYov3am9SrLy2OqVj2dSpXTAChXLoXL2zRnzZoNrmQdNWvKN3S+KXjG1/mmK/l68jeOjd3klTvJWbuZtYP+v7vvwe17qdGsPgA1LzuP/d9vcywPvOuC6+XPqix+L44Xi6sOwnXB/Qn4nYikEZwqSASyVNWV0yS/dqb1KsvLY6pVqwZvDHqBhIQAgUCAT8dOZMqkaY6N/+S/H+eiZo2okl6ZzxaM5p2XPuDDf4/g6bf6c82tndm+eQd//cPfHcmq1vRsTr2pJdkrNtFu6jMALHt2FAsfeodGA3ohCQEKDh1h4cPvOJJ3lFddcN3+WZVVVnEKYvBGidYF1xxTKaW8Z1nnVDrJkxwv70c7QL/3LOvH3J2eZXlpT87aUv9BP+DU30Zcc/72w3DrgmuMMdGKxTM7K7TGGF+xS3CNMcZleaIRbyURkZNFZJqIrBSR5SJyX+j5dBGZKiJrQx/DLui2QmuM8RUH19HmAQ+qan3gUqCviJwLPAZkqupZQGbo8xJZoTXG+IpTV4ap6lZVXRR6nAOsBOoC1wJDQt82BOgabp9sjtYcs+/QgfDf5JDvc51dk1qc2VVqeZID8OLBUzzLyqx2qmdZI/Yt8yzLCdEs7xKRPkCfQk8NVtXBRXxfPeBCYC5QS1W3QrAYi0jNcDlWaI0xvhLNqoNQUf1VYS1MRCoCnwD3q+q+oi51D8emDowxvuLkTWVEJIlgkR2uqmNDT28XkYzQ1zOAHeHGsUJrjPGVfDTirSQSPHV9F1ipqq8U+tI4oHfocW8g7OV8NnVgjPEVB9fRtgB6AktFZHHouX7Ac8BoEbkd2ATcFG4gK7TGGF9Rh64NU9VZQHETsm2jGcsKrTHGV+zKsAj4sWGil1l+PKajAoEAU2d+wtCRbzo67i0v/IGnFgzikckvHnuuTv1TuG/sUzw86QXueOdhUkItbUojkJLEJZP+QbOvnqf5jBc54+EbAUisUoHGo/vRYs6rNB7dj8TKkd/DuTiVM9LpM+JxHvzyJf485UVa3NYRgNTKFbhjaD8envYKdwztR2ql0mcdb/biSUyZNZaJMz7mi8yRjo8fTgEa8eaVmCq0fmyY6GWWH4+psDv/2JO1q52/t+m8MTMY3PvZXzzX7bk/8MXzI3ix4yN8N3k+V/S5ptQ5BYeOsOD6Acy54lHmtH2M6lc0onLjMzntnmvZ/fUyZjd7gN1fL+O0e64tfVZeAV/8Yxgvt3uIgdf9jWY9O1DzzLq0/uO1rPtmGS+2+TPrvllG67u7lDqrKN26/J5Ol9/E1W1vcWX8ksRjhwVP+bFhopdZfjymozLq1KJdh8sZPnSM42NvmLeK3J9+2XKp5ukZrJ+7EoA1s5bSoFNTR7LyDxwCQJISkMQEUKjZsQlbRs0EYMuomdTs1KTUOTk7s9myfCMAh3N/Zsf6zVSunc557RuzcEwwa+GYmZzXvvRZsSYPjXjzSrguuPeKyMle7YwfGyZ6meXHYzpqwLN/YcATL6EF3szAbV2TxfntGwPQsPMlVMmo5szAAeHSzOdovXwwu2cs5adF60iuUZnDO7IBOLwjm+TqlZzJCql6UnXqnluPTYvXUbFGZXJ2BrNydmZTweEsCPapG/bJIMZ/NYruvW90fPyw+VH8zyvhzmgHAHNF5GsRuVtEakQyqDVnLJssPx4TQPsrW7Nr5x6+W7LClfGLMvKRt7is55X8+fNnKFcxlfwjeeFfFIkC5du2jzGz0d1UvugMKv7G3RugJ5dPocebDzDuqQ85tP+gq1lH3dCpF1e16Uavm/9Ir9tvoWmzxp7kHhWLXXDDrTrYADQG2gHdgCdFZCEwAhgbutHCrxS+rC2aDgt+bJjoZZYfjwng4ksupEOnNrTt0IqUlGQqplVk4KDn+dMfHnUlD2DH+i281SvY1qbGaRnUb3Oho+Pn7TvAntkrqNamEYd3/kRyzSrBs9maVTi8a58jGYHEBHq+9QCLP53N8snBJpP7d/5EWo0q5OzMJq1GFXIdyips+7Zg94fdu/YweXwmjRqfz7w5Cx3PKY6XZ6qRCndGq6paoKpTVPV2oA7wBtCRYBF2lB8bJnqZ5cdjAnjmqVe56Lw2XNygHXfd/iCzZ851tcgCVKwW/JNaRGj/p+v4ZviXpR4zqVoaiZWC7YIC5ZKo1uoCctdtYefkhdTpFuwUW6dbK3ZMWlDqLIAbn+/DjnVb+PrdCceeW/HlQhrfGMxqfGMrlk91tgCmlk+lQsXyxx63bNOc1SvXOZoRTjye0f7i70NVPULw8rNxIlL69S7H8WPDRC+z/HhMXuj5z3s489JzqVA1jf5z/s2kV8eQUqEcLXp2AGDp5HnM+3h6qXNSalXl/H/+EUkIIIEA2z6bw66pi/hpwRoavH0/dbu34efNu1lyx6ulzqrX5Bwa39CKrSs3cd+E4IqKSS+MYvqb4/jtv+/j4ptbk71lN8Pufq3UWYXVqFGNwUODYyYmJvDpmAnMyJztaEY4+S73QTwRJTZnFJGzVbVU/++x5oymKNXLO/8mTFFurdLQkxyATt5MgQKQmepJT0HA29skbtqztNQH1v3U6yKuOR/98J+yb85Y2iJrjDFei8U5WrsE1xjjK7F4Ca4VWmOMr3h5aW2krNAaY3zFpg6MMcZlsbjqwAqtMcZXbOrAmJBdB5y/Iqkobx+e60kOwKSKEV2h7gzvGhZzuMChy489Ym+GGWOMy2yO1hhjXGZTB8YY4zK37iJXGlZojTG+Eq6NeFmwQmuM8RWbOjDGGJfF4tRBTPUMA/92cbUuuPGRVbduBhMmfsTCRVOZv2Ayd9/9O9ey6p1xCmO/GnZsm7/+K3r1cb6ZoVc5hbnVsTgSsdgFt8TbJDohmtskBgIBVi7/mo6dbyUrayvfzplAj553s3LlWsf3y49Zfjym0maVS0yOKqtW7RrUrl2TJYuXU7FiBb6e/Tm3duvDqlXhb159cinW0QYCAaZ/N55bOt7GlqxtJzyOGzl7DxfZSKVEf+jbm4aNzictrSI9b/ljxK/blr2y1LctbH1Su4hrzvSsL0vME5H3gKuBHap6fui5dGAUUA/YCNysqntLGidcc8ZkEeklIu1Cn3cXkYEi0ldEkiI9mEj5tYurdcGNn6zt23ayZPFyAPbvz2X16nVkuNh08qhLW13MjxuzXC2yXuW42bE4EvmqEW8R+IBgR5nCHgMyVfUsIDP0eYnCTR28D1wF3CciQ4GbgLnAxcA7kexlNPzaxdW64MZPVmGnnFKXhg3PZcH8xa5nde7anvFj3WkF5HWO1x2Lj+fk1IGqzgT2HPf0tcCQ0OMhQNdw44QrtBeoajfgOqADcKOqDgVuA4rtVmddcMsmy4/H5HXWURUqlGf4iDd59JEB5OTsdzUrKSmRK65sxeTPM+M+pyw6Fh8vmkJbuFaFtj4RRNRS1a0AoY81w70g3KqDgIgkAxWA8kBlgtU9BSh26sC64JZNlh+PyessgMTERIZ/9CajRn7GuM8mu5ZzVMu2zVmxdBW7dx5/4hR/OWXRsfh40fwjXLhWuSncGe27wCpgMfBX4GMReRuYD4x0emf82sXVuuDGTxbAG28+z+rV6xj4r3ddyyjsqus6eDJt4EVOWXQsPp4Hqw62i0gGQOjjjnAvCNcz7FURGRV6vEVEPgTaAW+r6rwT3cvi+LWLq3XBjZ+sZs2a0P2317Ns6Sq++XY8AH/v/yJTJk93Ja9cagrNL7+E/g8968r4XufEAg9uKjMO6A08F/r4WbgXxNTyLmOcFu3yrtIozfKuWHYiy7tOlBPLuy7KuCzimrNo66xwy7tGAK2B6sB2oD/wKTAaOAXYBNykqiXOx9iVYcYYX3Hy5FFVby3mS22jGccKrTHGV+xeB8YY4zK78bcxxrisIAZvKmOF1hjjK3ZGa4wxLsvX2GvPaIXW+NrPeYc9y1qbvdmzrDoV0z3LOpwfZ11wberAGGPcZVMHxhjjMjujNcYYl9kZrTHGuCxf88t6F37FCq0xxldisTmjFVpjjK/E4iW41gXXZ1l+PCbLKr3ZiycxZdZYJs74mC8yHb+V9DEpKclMnTaGmd+M45t5E3is372uZRVHVSPevBJTt0mMl86qsZrlx2OyrKJFu4529uJJXH3FLezdkx31fu4/8nNU31+hQnlycw+QmJjIxCkj+cuj/4i479qenLWlvk1iRpVzI645W7NXlDovEmHPaEXkDBF5SEReF5GXReQuEansxs74tbOqdcG1rLLO8lJu7gEg2KMsMSnR8zlTjeJ/XgnXbvxe4C2gHMHOt6nAycAcEWnt9M74tbOqdcG1rLLOUlWGfTKI8V+NonvvG13JOCoQCDBj9jhWb/iW6dNms3DBElfzjpevBRFvXgn3ZtidQCNVzReRV4AJqtpaRAYRbN9QZCfcUCfJPgCSUJlAoEJEO+PXzqrWBdeyyjrrhk692L5tJ9WqpzN87GDWrfmeeXMWupJVUFDA5S26UKlyGkM/eoP69c9yZeqlOLG46iCSN8OOFuMUIA1AVTcRpguuqjZR1SaRFlnwb2dV64JrWWWdtX3bTgB279rD5PGZNGp8vis5he37KYfZX8+lbftWrmcVVqAa8eaVcIX2HWC+iAwG5gADAUSkBsG2447ya2dV64JrWWWZlVo+lQoVyx973LJNc1avXOd4DkC16ulUqpwGQLlyKVzepjlr1mxwJas4sbjqIFwX3NdF5EugPvCKqq4KPb8TcPyfKb92VrUuuJZVllk1alRj8NDXAEhMTODTMROYkTnb8RyAWrVq8MagF0hICBAIBPh07ESmTJrmSlZxYnEdbUwt7zLGRMbL2yRGu7yrNJxY3lWpwukR15x9uRs8Wd5lV4YZY3zFbvxtjDEus9skGmOMy+J1eZcxxsQNJ68ME5GOIrJaRNaJyGMnuk92RmuM8RWnzmhFJAH4N9AeyCK41HWcqq6IdiwrtMYYX3FwjrYpsE5VNwCIyEjgWiD2Cm3e4c0ntHxCRPqo6mCn96esciwrvrL8eEx+ziosmppT+HYBIYML7XNd4MdCX8sCLjmRfYrlOdo+4b8lrnIsK76y/HhMfs46IYVvFxDaCv/DUFTBPqHT5VgutMYYU5ayCN6t8KiTgC3FfG+JrNAaY0zR5gNnichpIpIM3AKMO5GBYvnNMK/mdrycQ7Ks+Mny4zH5OctxqponIn8CJgMJwHuquvxExnL9XgfGGPO/zqYOjDHGZVZojTHGZTFXaJ265C2CnPdEZIeILHMro1DWySIyTURWishyEbnPpZxyIjJPRJaEcp50I+e4zAQR+a+IfOFyzkYRWSoii0VkgctZVURkjIisCv3MmrmUc07oeI5u+0TkfpeyHgj9TiwTkREiUs6NnFDWfaGc5W4dT9yJ5m7kbm8EJ5zXA6cDycAS4FyXsloBFwHLPDiuDOCi0OM0YI0bx0Vw3V/F0OMkYC5wqcvH9mfgI+ALl3M2AtXd/lmFsoYAd4QeJwNVPMhMALYBp7owdl3geyA19Plo4HcuHcf5wDKgPME3278EzvLi5xbLW6yd0R675E1VDwNHL3lznKrOxIV2PMVkbVXVRaHHOcBKgr/8Tueoqu4PfZoU2lx7t1NETgKuItjyyBdEpBLBf4TfBVDVw6qa7UF0W2C9qv7g0viJQKqIJBIsgie0HjQC9YFvVfWAquYBM4DrXMqKG7FWaIu65M3xglSWRKQewe7Bc10aP0FEFgM7gKmq6kpOyGvAI4AXd1pWYIqILAxdNumW04GdwPuhKZF3RCTyDqMn7hZghBsDq+pm4CVgE7AV+ElV3WmEFjybbSUi1USkPNCZXy76/58Ua4XWsUveYpGIVAQ+Ae5X1X1uZKhqvqo2IngVS1MRcaXdqYhcDexQVXd6Vv9aC1W9COgE9BURt1qrJhKcUnpTVS8EcgHX3isACC2G7wJ87NL4VQn+ZXgaUAeoICI93MhS1ZXA88BUYBLB6b88N7LiSawVWscueYs1IpJEsMgOV9WxbueF/tydDnR0KaIF0EVENhKc4rlCRIa5lIWqbgl93AH8h+A0kxuygKxCfwmMIVh43dQJWKSq7vQah3bA96q6U1WPAGOB5i5loarvqupFqtqK4PTcWrey4kWsFVrHLnmLJSIiBOf8VqrqKy7m1BCRKqHHqQT/D7bKjSxV/YuqnqSq9Qj+nL5SVVfOkkSkgoikHX0MdCD4J6rjVHUb8KOInBN6qi0ncFu8KN2KS9MGIZuAS0WkfOh3sS3B9wlcISI1Qx9PAa7H3WOLCzF1Ca46eMlbOCIyAmgNVBeRLKC/qr7rRhbBs7+ewNLQ/ClAP1Wd4HBOBjAkdMPiADBaVV1dduWRWsB/gjWCROAjVZ3kYt49wPDQP/YbgNvcCgrNY7YH/uBWhqrOFZExwCKCf8b/F3cvj/1ERKoBR4C+qrrXxay4YJfgGmOMy2Jt6sAYY3zHCq0xxrjMCq0xxrjMCq0xxrjMCq0xxrjMCq0xxrjMCq0xxrjs/wA3jZ24/fpbmgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, X_pred)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41e0a872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4565826330532213\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, X_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa7626",
   "metadata": {},
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb67926",
   "metadata": {},
   "source": [
    "### What is XGBoost?\n",
    "\n",
    "XGBoost is one of the most popular machine learning algorithm these days. Regardless of the type of prediction task at hand; regression or classification. XGBoost is well known to provide better solutions than other machine learning algorithms. In fact, since its inception, it has become the \"state-of-the-art” machine learning algorithm to deal with structured data.\n",
    "\n",
    "XGBoost belongs to a family of boosting algorithms and uses the gradient boosting (GBM) framework at its core. It is an optimized distributed gradient boosting library.\n",
    "\n",
    "### Boosting\n",
    "Boosting is a sequential technique which works on the principle of an ensemble. It combines a set of weak learners and delivers improved prediction accuracy. At any instant t, the model outcomes are weighed based on the outcomes of previous instant t-1. The outcomes predicted correctly are given a lower weight and the ones miss-classified are weighted higher. A weak learner is one which is slightly better than random guessing. \n",
    "\n",
    "\n",
    "### Advantages of XGBoost:\n",
    "\n",
    "1. It is comparatively faster than other ensemble classifiers\n",
    "2. The code XGBoost algorithm is parallelizable and hence can harness the power of multi-core GPUs.\n",
    "3. It has a wide range of tuning parameters (cross-validation, regularization, user-defined objective functions, missing values, tree parameters, scikit-learn compatible API).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e6e1601",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(tree_method = \"exact\", predictor = \"cpu_predictor\",\n",
    "                            objective = \"multi:softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae4fdc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"learning_rate\": [0.1, 0.01, 0.001],\n",
    "               \"gamma\" : [0.01, 0.1, 0.3, 0.5, 1, 1.5, 2],\n",
    "               \"max_depth\": [2, 4, 7, 10],\n",
    "               \"colsample_bytree\": [0.3, 0.6, 0.8, 1.0],\n",
    "               \"subsample\": [0.2, 0.4, 0.5, 0.6, 0.7],\n",
    "               \"reg_alpha\": [0, 0.5, 1],\n",
    "               \"reg_lambda\": [1, 1.5, 2, 3, 4.5],\n",
    "               \"min_child_weight\": [1, 3, 5, 7],\n",
    "               \"n_estimators\": [100, 250, 500, 1000]}\n",
    "# subsample: Denotes the subsample ratio of columns for each split, in each level.\n",
    "# colsample_bytree: Denotes the fraction of columns to be randomly samples for each tree.\n",
    "# gamma: Gamma specifies the minimum loss reduction required to make a split.\n",
    "# reg_alpha: Lasso L1 regularization\n",
    "# reg_lambda: Ridge L2 reguralarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd17da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RandomizedSearchCV Object\n",
    "xgb_rscv = RandomizedSearchCV(xgb_clf, param_distributions = parameters, scoring = \"f1_micro\",\n",
    "                             cv = 10, verbose = 3, random_state = 40 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97c24c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[16:36:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.3s\n",
      "[16:36:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.3s\n",
      "[16:36:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.4s\n",
      "[16:36:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.4s\n",
      "[16:36:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.4s\n",
      "[16:36:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.4s\n",
      "[16:36:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 7/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.4s\n",
      "[16:36:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 8/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.4s\n",
      "[16:36:05] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 9/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.4s\n",
      "[16:36:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 10/10] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.1, max_depth=2, min_child_weight=5, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.6; total time=   0.4s\n",
      "[16:36:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.4s\n",
      "[16:36:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.4s\n",
      "[16:36:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.4s\n",
      "[16:36:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.4s\n",
      "[16:36:07] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.4s\n",
      "[16:36:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.3s\n",
      "[16:36:08] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 7/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.4s\n",
      "[16:36:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 8/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.4s\n",
      "[16:36:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 9/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.4s\n",
      "[16:36:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 10/10] END colsample_bytree=0.8, gamma=2, learning_rate=0.01, max_depth=2, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=4.5, subsample=0.4; total time=   0.4s\n",
      "[16:36:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.8s\n",
      "[16:36:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.8s\n",
      "[16:36:11] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.8s\n",
      "[16:36:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.8s\n",
      "[16:36:13] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.8s\n",
      "[16:36:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.7s\n",
      "[16:36:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 7/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.8s\n",
      "[16:36:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 8/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.8s\n",
      "[16:36:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 9/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.8s\n",
      "[16:36:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 10/10] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.001, max_depth=2, min_child_weight=1, n_estimators=250, reg_alpha=0.5, reg_lambda=4.5, subsample=0.2; total time=   0.8s\n",
      "[16:36:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.5s\n",
      "[16:36:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.5s\n",
      "[16:36:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.5s\n",
      "[16:36:19] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.5s\n",
      "[16:36:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.5s\n",
      "[16:36:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.5s\n",
      "[16:36:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 7/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.5s\n",
      "[16:36:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 8/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.5s\n",
      "[16:36:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 9/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.5s\n",
      "[16:36:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 10/10] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.1, max_depth=4, min_child_weight=7, n_estimators=100, reg_alpha=0, reg_lambda=1, subsample=0.7; total time=   0.5s\n",
      "[16:36:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   2.3s\n",
      "[16:36:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   2.2s\n",
      "[16:36:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   2.2s\n",
      "[16:36:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   2.2s\n",
      "[16:36:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   2.2s\n",
      "[16:36:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   2.2s\n",
      "[16:36:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 7/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   2.3s\n",
      "[16:36:38] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 8/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   2.3s\n",
      "[16:36:41] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 9/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   2.3s\n",
      "[16:36:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 10/10] END colsample_bytree=1.0, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=7, n_estimators=500, reg_alpha=1, reg_lambda=4.5, subsample=0.5; total time=   2.3s\n",
      "[16:36:45] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.6s\n",
      "[16:36:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.6s\n",
      "[16:36:46] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.6s\n",
      "[16:36:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.6s\n",
      "[16:36:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.6s\n",
      "[16:36:48] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.6s\n",
      "[16:36:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 7/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.6s\n",
      "[16:36:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 8/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.6s\n",
      "[16:36:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 9/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.6s\n",
      "[16:36:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 10/10] END colsample_bytree=0.6, gamma=0.01, learning_rate=0.001, max_depth=10, min_child_weight=3, n_estimators=100, reg_alpha=1, reg_lambda=3, subsample=0.7; total time=   0.6s\n",
      "[16:36:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   1.3s\n",
      "[16:36:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   1.2s\n",
      "[16:36:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   1.2s\n",
      "[16:36:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   1.2s\n",
      "[16:36:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   1.3s\n",
      "[16:36:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   1.2s\n",
      "[16:36:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 7/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   1.2s\n",
      "[16:37:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 8/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   1.3s\n",
      "[16:37:01] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 9/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   1.2s\n",
      "[16:37:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 10/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=1, subsample=0.7; total time=   1.2s\n",
      "[16:37:04] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   5.7s\n",
      "[16:37:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   5.6s\n",
      "[16:37:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   5.5s\n",
      "[16:37:21] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   5.6s\n",
      "[16:37:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   5.6s\n",
      "[16:37:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   5.4s\n",
      "[16:37:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 7/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   9.3s\n",
      "[16:37:47] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 8/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   7.8s\n",
      "[16:37:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 9/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   7.9s\n",
      "[16:38:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 10/10] END colsample_bytree=0.6, gamma=0.3, learning_rate=0.01, max_depth=7, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=1.5, subsample=0.4; total time=   6.4s\n",
      "[16:38:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   1.5s\n",
      "[16:38:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   1.7s\n",
      "[16:38:12] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   1.6s\n",
      "[16:38:14] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   1.6s\n",
      "[16:38:15] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   1.6s\n",
      "[16:38:17] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 6/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   1.6s\n",
      "[16:38:18] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 7/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   1.6s\n",
      "[16:38:20] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 8/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   1.6s\n",
      "[16:38:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 9/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   1.6s\n",
      "[16:38:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 10/10] END colsample_bytree=1.0, gamma=0.3, learning_rate=0.01, max_depth=10, min_child_weight=7, n_estimators=250, reg_alpha=0, reg_lambda=3, subsample=0.7; total time=   1.6s\n",
      "[16:38:25] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   0.9s\n",
      "[16:38:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   0.9s\n",
      "[16:38:27] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   0.9s\n",
      "[16:38:28] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   0.9s\n",
      "[16:38:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   0.9s\n",
      "[16:38:30] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 6/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   0.9s\n",
      "[16:38:31] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 7/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   0.9s\n",
      "[16:38:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 8/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   0.9s\n",
      "[16:38:32] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 9/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   0.9s\n",
      "[16:38:33] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 10/10] END colsample_bytree=0.6, gamma=2, learning_rate=0.001, max_depth=7, min_child_weight=7, n_estimators=250, reg_alpha=0.5, reg_lambda=2, subsample=0.4; total time=   0.9s\n",
      "[16:38:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "model_xgboost = xgb_rscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60e6cbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:38:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1.0,\n",
       "              enable_categorical=False, gamma=0.3, gpu_id=-1,\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.01, max_delta_step=0, max_depth=10,\n",
       "              min_child_weight=7, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=250, n_jobs=12, num_parallel_tree=1,\n",
       "              objective='multi:softprob', predictor='auto', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=3, scale_pos_weight=None, subsample=0.7,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_final = XGBClassifier(random_state=0, \n",
    "                          n_estimators=xgb_rscv.best_params_['n_estimators'], \n",
    "                          max_depth=xgb_rscv.best_params_['max_depth'],\n",
    "                          learning_rate=xgb_rscv.best_params_['learning_rate'],\n",
    "                          gamma=xgb_rscv.best_params_['gamma'],\n",
    "                          colsample_bytree=xgb_rscv.best_params_['colsample_bytree'],\n",
    "                          subsample=xgb_rscv.best_params_['subsample'],\n",
    "                          reg_alpha=xgb_rscv.best_params_['reg_alpha'],\n",
    "                          reg_lambda=xgb_rscv.best_params_['reg_lambda'],\n",
    "                          min_child_weight=xgb_rscv.best_params_['min_child_weight'])\n",
    "xgb_final.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07011f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgb_final.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acf44109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         2.0      0.000     0.000     0.000         0\n",
      "         4.0      1.000     0.286     0.444         7\n",
      "         5.0      0.000     0.000     0.000         0\n",
      "         6.0      0.750     0.484     0.588        31\n",
      "         7.0      0.083     0.286     0.129         7\n",
      "         8.0      0.771     0.703     0.735       158\n",
      "         9.0      0.382     0.382     0.382        55\n",
      "        10.0      0.538     0.532     0.535        79\n",
      "        11.0      0.200     0.200     0.200        20\n",
      "        12.0      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.552       357\n",
      "   macro avg      0.372     0.287     0.301       357\n",
      "weighted avg      0.617     0.552     0.576       357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_pred,y_test,digits=3)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5e798a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm1 = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm1, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0ffb871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5518207282913166"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016bd82f",
   "metadata": {},
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "### What is a Support Vector MAchine (SVM)?\n",
    "A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they’re able to categorize new text.\n",
    "\n",
    "Compared to newer algorithms like neural networks, they have two main advantages: higher speed and better performance with a limited number of samples (in the thousands). This makes the algorithm very suitable for text classification problems, where it’s common to have access to a dataset of at most a couple of thousands of tagged samples.\n",
    "\n",
    "### How do SVMs work?\n",
    "A support vector machine takes data points and outputs the hyperplane (which in two dimensions it’s simply a line) that best separates the tags. This line is the decision boundary: anything that falls to one side of it we will classify as blue, and anything that falls to the other as red.\n",
    "\n",
    "<img src=\"hyperplane.png\" />\n",
    "\n",
    "### Advantages\n",
    "1. SVMs can give results at a higher speed and better accuracy with lesser number of samples.\n",
    "2. The SVM provides a very useful technique within it known as kernel and by the application of associated kernel function we can solve any complex problem.\n",
    "3. SVM generally do not suffer condition of overfitting and performs well when there is a clear indication of separation between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c28a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X = StandardScaler()\n",
    "X_Train = sc_X.fit_transform(X_train)\n",
    "X_Test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3de852fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(random_state=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_Train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d192ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Pred = classifier.predict(X_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a51a888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD4CAYAAADSIzzWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAtrElEQVR4nO3deXxU5fX48c+ZJGxhkRBZBNcWK6gVFRXRqgi44ILWIlh3bXHhq6K1VuuK1hbXKj+qENxwIy6IILIvKlZWIVY0QRYVgmBAkF0gmfP7YwYaMMlMkrvMPD1vXveVyc3MPfeZuZzcPPe5zxFVxRhjTPAiYe+AMcb8r7IEbIwxIbEEbIwxIbEEbIwxIbEEbIwxIcn0PUCd1jbMohbqZmYFFmt76c7AYkVEAokTtVE+aaV0x8paHxg71y5L+kPPyj0kmAOxEnYGbIwxIfH9DNgYYwIVLQt7D5JmCdgY45ay0rD3IGmWgI0xTlGNhr0LSbMEbIxxS9QSsDHGhCONzoBTbhTEmWecxhcLP6Loy4+548/9LFY11K1blw8/epdZs8Yzd94k7r7nVl/i7BLU+5c39HGKVxSwYP4U32KU59pxEWScoGNVKFqW/BIy8Xs2tOqMA45EIhR+MYOzelxCcfEqZs0cx2WX30hh4WLP9ytdYlV3HHB2dgO2bNlKZmYmU6a+zZ9vH8DcuQuSem11xgHX9v2rzjjgk08+gc2bt/DiC09x9DHdkn4dVH8ccLocF6kYx4tYXowD3vHNvKQ/9DoHdUztccAicpiI/EVEBonI0/HH7fzYmeOPO5qlS7/h66+Xs3PnTt58czTnn3emH6GcjbVly1YAsrIyycrKRPHnF2yQbfr449msX/+jL9vem4vHhYttqoqWlSa9hK3KBCwifwHyAQHmAHPjj0eIyJ1e78x+rVuyovi73d8Xr1zFfvu19DqM07EikQgzZ43jm28/ZdrUj5k3t8CXOEG2KUguHhcutqlK0WjyS8gSXYS7FjhcVff421REngS+AAZW9CIR6Qv0BZCMJkQi2UntjFTwZ6lfXSSuxopGo5zYqQdNmjRmRP5Q2rc/lC+//MrzOEG2KUguHhcutqlKDl2EiwL7VbC+VfxnFVLVPFXtqKodk02+ACuLV7F/m/+Ga9O6FatWfZ/066vD1Vi7bNiwkRkzZtG9+6m+bD+MNgXBxePCxTZVKY0uwiVKwP2BqSIyXkTy4ssEYCpwi9c7M3deAb/85cEcdND+ZGVlcfHFPXlv7CSvwzgbKzc3hyZNGgNQr15dunQ5iUVfLfU8DgT7/gXJxePCxTZVSaPJLyGrsgtCVSeIyKHA8UBrYv2/xcBcVfX810dZWRm39L+Hce+/TkYkwkvD3/Dlz2dXY7Vs2Zy8YU+QEYkQiUQY+c77TBg/zfM4EOz798rLgznllBPJzc1h2dK5PPjQE7z0Ur4vsVw8LlxsU9U7Ef7FtWSl1DA083M2HWXt2HSU6cWLYWjb/zMx6Q+97q/PDHUYmt0JZ4xxig9/nPvGErAxxi0p0LebLEvAxhi3pMD43mRZAjbGuMXOgI0xJiRlwV1Mri1LwMYYt1gXhPHKzjQa02hMSrAuCGOMCYmdARtjTEgsARtjTDjULsIZY0xIrA/YGGNCYl0QxhgTkjQ6A7aqyI7FCrKCsIttAjePCxfbVCkPSxKJyAsiUiIiC8utyxGRySKyOP61abmf3SUiS0RkkYgkLIaXUgk4Eokw6OmHOfe8yzjyqC707n0B7dq1tVjV8PIrb3HueZf5su3yXGwTuHlcuNimKnk7IftLwFl7rbsTmKqqbYkVp7gTQETaA32Aw+OveUZEMqraeEolYFert7pYQdjFNoGbx4WLbapSaWnySwKq+hGwbq/VPYHh8cfDgQvKrc9X1e2q+jWwhFgxi0rVOAGLyNU1fW1lXK3emhKVYj3mYpvAzePCxTZVqRpnwCLSV0TmlVv6JhGhhaquAoh/bR5f3xpYUe55xfF1larNRbgBwIsV/cCqIocXKygutgncPC5cbFOVqjEKQlXzgDyPIldUXaPKxleZgEXkP1UEalHZ68o3qjoliVyt3poSlWI95mKbwM3jwsU2Vcn/URDfi0grVV0lIq2Akvj6YmD/cs9rA3z3s1eXk6gLogVwBXBeBcsPNdjxKrlavTUlKsV6zMU2gZvHhYttqpKHoyAqMQa4Mv74SmB0ufV9RKSuiBwMtAXmVLWhRF0QY4GGqlqw9w9E5INq7HBSXK3e6mIFYRfbBG4eFy62qUoengGLyAjgNCBXRIqB+4GBwJsici2wHOgFoKpfiMibwJdAKdAvUfV4q4qc4oKqHgzBVhC2qsimIl5URd725oNJf+j1L77PqiIbY4xn0uiXriVgY4xbbC4IY4wJiSVgY4wJSRpNxmMJ2BjjlrIqBx6kFEvAKS4iwU3XEa16xIzHsdLnQolJM9YFYYwxIbEEbIwxIbE+YGOMCYdG06d7yxKwMcYt1gVhjDEhsVEQxhgTEjsDNsaYkKRRAk6pmnDgbvXWoGK1adOKiRPzKSiYyvz5U+jX7xrfYrn4/rkay8U2VUo1+SVkKTUdZSQSofCLGZzV4xKKi1cxa+Y4Lrv8RgoLF3u+X+kSKzNSZVHVn2nZsjktWzanoGAhDRtmM3Pm+/Tq9UeKihLHKo0m33eWLu+fxUqvNnkxHeXWJ/+YdM5pcNuwUKejTHgGLCKHiUhXEWm41/q9SzXXmqvVW4OMtXp1CQUFCwHYvHkLRUVLaN3a+6KIrr5/LsZysU1VimryS8iqTMAicjOxchs3AQtFpGe5H//d651xtXprWJViDzywDR06HM6cOQs837ar75+LsVxsU5XKypJfQpboItwfgWNVdbOIHAS8LSIHqerTVFwBFLCqyGHG2iU7uwEjRgzl9tsHsGnTZs+37+r752IsF9tUFU2ji3CJEnCGqm4GUNVvROQ0Ykn4QKpIwFYVObxYAJmZmeTnDyU/fxSjR0/wJYar75+LsVxsU5VSoGshWYn6gFeLSIdd38ST8blALnCk1zvjavXWoCvFDh36GEVFSxg06DnfYrj6/rkYy8U2VUmjyS8hS3QGfAWx6p67qWopcIWIDPV6Z1yt3hpkrM6dj+PSSy/i888LmT17PAD33fcoEydO9zSOq++fi7FcbFOV0ugMOKWGoZmfq+4wtNqozjA0Y/zgxTC0Lff1STrnZD+Yb1WRjTHGMynQtZAsS8DGGLekUReEJWBjjFNcGoZmjDHpxc6AjTEmJJaAjVdaZTcNLNaKTWsDi7V/o9xA4gTZJpMiUuAW42Sl3HSUxhhTGxrVpJdERORWEflCRBaKyAgRqSciOSIyWUQWx7/W+CzJErAxxi0ezYYmIq2Bm4GOqnoEkAH0Ae4EpqpqW2Bq/PsasQRsjHFLNJr8klgmUF9EMoEGwHdAT2B4/OfDgQtququWgI0xbqnGGbCI9BWReeWWvrs2o6orgceB5cAqYIOqTgJaqOqq+HNWAc1ruqt2Ec4Y45ZqjIIoP3Pj3uJ9uz2Bg4EfgbdE5DIP9nA3S8DGGKdomWc3YnQDvlbVNQAi8g7QGfheRFqp6ioRaQWU1DRAynVBuFo80M9YjwwawNyi6Uz4eOTudU32acwrI4cwbc4YXhk5hMZNGnkaE9xsE7hzXIQRJ+hYFfKuJNFyoJOINJDYTPNdgUJgDHBl/DlXEqsaVCMplYAjkQiDnn6Yc8+7jCOP6kLv3hfQrl1bi5XAyBGjueriG/ZYd8Mt1/Dvj+Zw+vHn8++P5nBD/2s9iwdutgncOi6CjhN0rMp4NQxNVWcDbwPzgc+J5cs8YCDQXUQWA93j39dISiVgV4sH+h1rzsz5/Lh+4x7ruvfowsj8MQCMzB/DGT26eBYP3GwTuHVcBB0n6FiV8rAop6rer6qHqeoRqnq5qm5X1R9Utauqto1/XVfTXU2mKvLxInJc/HF7EblNRHrUNGBVXC0eGEahwtx9c1jzfewusDXfr6VZbo6n23exTeDmceFim6oUrcYSsiovwonI/cDZQKaITAZOAD4A7hSRo1X14UpeZ0U5Q4oVFBfbBG4eFy62qSpamgKZNUmJRkH8DugA1AVWA21UdaOIPAbMBipMwFaUM7xYu6xds459W+Sy5vu17Nsilx/W1vivpAq52CZw87hwsU1VSp/8m7ALolRVy1R1K7BUVTcCqOo2fGimq8UDwyhUOGX8B1zU53wALupzPpPHeVsTzsU2gZvHhYttqoqXc0H4LdEZ8A4RaRBPwMfuWikiTfAhAbtaPNDvWE/nDaTTSR1p2mwfPvl8Ek8NfJZnn36BwS88xsWXXsB3K1fT7+rbPYsHbrYJ3Dougo4TdKxKpdEZcJVFOUWkrqpur2B9LtBKVT9PFMCKctZOUNM2gk1HacLnRVHOdReemnTOyRn1YeoW5awo+cbXrwXsyDbGpJ40OgO2W5GNMU7R0rD3IHmWgI0xTkmjqvSWgI0xjrEEbIwx4bAzYGOMCYklYOOZVVvWh70LvjiswX6Jn+QBG4b2v0fLQh1ZVi2WgI0xTrEzYGOMCYlG7QzYGGNCYWfAxhgTElU7AzbGmFDYGbAxxoQkmkajIFKqJhy4W701qFht2rRi4sR8CgqmMn/+FPr1u8a3WH62KbdVLgPfGMjQaUMZMmUIPa/pCcC1d19L3vQ8npn0DPcOu5fsxslVW6kOF48LF9tUGY1K0kvYqpyO0gvVmY4yEolQ+MUMzupxCcXFq5g1cxyXXX4jhYWLPd+vdImVGcmoVqyWLZvTsmVzCgoW0rBhNjNnvk+vXn+kqChxrNJoWdJxavv+dW/x6yp/3rR5U3Ka57B04VLqZ9dn0LhBPPSHh8htlUvBvwuIlkW55q7YL5cX/vFCpduZ/P1/km4TpM9xkYpxvIjlxXSU33TonnTOOahgcqhZuNpnwCLysh87Au5Wbw0y1urVJRQULARg8+YtFBUtoXVr74si+t2m9SXrWbpwKQDbtmxjxZIVNGvZjPkfzSdaFuvkK1pQRG4rb+cVdvG4cLFNVVFNfglblQlYRMbstbwH/HbX917vjKvVW8OqFHvggW3o0OFw5sxZ4Pm2g2xT8zbN+cXhv2DRgkV7rD/j4jOYO32up7FcPC5cbFNV0qkLItFFuDbAl8BzgAICdASeqOpFVhU5vFi7ZGc3YMSIodx++wA2bdrs+faDalO9BvW4Z+g9DH1gKFs3b929vs9NfSgrK2P6KG/rwrl4XLjYpqqk0zC0RF0QHYFPgbuBDar6AbBNVT9U1Q8re5Gq5qlqR1XtmGzyBXertwZdKTYzM5P8/KHk549i9OgJvsQIok0ZmRnck3cP09+dzicTPtm9vtvvunF81+N59KZHPY0Hbh4XLrapKmVlkvQStioTsKpGVfWfwNXA3SIyGB+HrrlavTXoSrFDhz5GUdESBg16zrcYQbSp/2P9WbF4BaOGjdq97tjTjqXXDb0YcM0Atv9UYcWsWnHxuHCxTVVRlaSXsCWVTFW1GOglIucAG/3aGVertwYZq3Pn47j00ov4/PNCZs8eD8B99z3KxIne/qnud5sOP+5wuv2uG18Xfs3gCYMBGP7IcK5/8Hqy6mTx8OsPA1A0v4jBfx3sWVwXjwsX21SVVOjbTVZKDUMzP1fdYWi1UZ1haLWVaBiaV6o7DM2Ey4thaIVteySdc9otHpdew9CMMSaVeTkKQkT2EZG3RaRIRApF5EQRyRGRySKyOP61aU331RKwMcYpZdFI0ksSngYmqOphwFFAIXAnMFVV2wJT49/XiCVgY4xTvLoRQ0QaA6cAz8e2qztU9UegJzA8/rThwAU13VdLwMYYp0RVkl5EpK+IzCu39C23qUOANcCLIrJARJ4TkWyghaquAoh/bV7TfbXZ0IwxTqnO8DJVzQPyKvlxJnAMcJOqzhaRp6lFd0NF7AzYGOMUD+eCKAaKVXV2/Pu3iSXk70WkFUD8a0lN99XOgFNcRiS435FBxhqz4F+BxDn+iMsDiQNQvC24Cszrt3l/e7kroh7dYKGqq0VkhYj8SlUXAV2JTc3wJXAlMDD+dXRNY1gCNsY4JcnRDcm6CXhNROoAy4jdFRwB3hSRa4HlQK+abtwSsDHGKV7e+aWqBcTmxNlbVy+2bwnYGOMUr7oggmAJ2BjjlFSYZCdZloCNMU5Jo6LIloCNMW5R0ucMOOXGAbtavTWIWHXr1uXDj95l1qzxzJ03ibvvudWXOMnGato0k1b71aFFy6wKtzF24jQuvOIGLrziBi697jaKFi+r9X7t2LGDP937D86++Bou+WN/ohKb4S0aKeWn7B/Z1nA92xqupzTrv3MJ3//Pu5i6cCxvffDK7nXX3X4NExe8S/6Ul8if8hIndz2x1vtWkcZNGvHCy0/zydzx/HvOODoe18GXOK4d61UpVUl6CVtKJeBIJMKgpx/m3PMu48ijutC79wW0a9fWYiVp+/bt9Dj793TqdDYndupB9+6nctxxR3seJ9lYW7aWsXbNzkq30Xq/lrw0+FFGvfws1191CQMeHZR0/JWrvueq/7vjZ+vfGTuJxo0aMv7NF7i89wXsrLcl/hOhzrZG1N/clLpbGrOj3mY0/sfqe2+Mo98lt/1sW6/mvUGfblfRp9tVfDx1ZtL7Vh1/H3g306bMoPNxZ3PaST356qulnsdw8ViviiJJL2GrVgIWkZNF5DYROcOPnXG1emuQsbZsidVNy8rKJCsrE/V0UE71Yu3YrkSjlcc/+sj2NGncCIBfH34Y35f890aG9yZOo88fbuGiK/sx4NFBlJUlN1fxtBkz6dmjGwBnnPYbyjJ3oiiRaAaRaGxu5YhmIBpBI7F9mz/rMzb86FudgUo1bJRNp5OO49WX3wZg586dbNywyfM4rh7rlYlWYwlboqrIc8o9/iMwGGgE3C8int4TDe5Wbw0yViQSYeascXzz7adMm/ox8+YW+BLH61jvjJ3IyZ1iwy2XfrOcCVM/5JUhTzBy+L+IRCKMnZRcRY+SNT/QsnmsVH1mZgaiArLnL4GyjNhZuSQYsN/nmot4Y9pw7v/nXTRq0qi6TUrooIP254e16/h/z/yDaTNG8c//9zcaNKjveRxXj/XKuHQGXL7zri/QXVUHAGcAl1b2ovIzDEWjWyp7WkWv+9k6F6q3BhkrGo1yYqceHNr2RI7teBTt2x/qSxwvY8359DPeGTuJ2268BoDZ8wr4smgJfa6NnQHPnldA8XerAbj5rge56Mp+3HD7vXxRtJiLruzHRVf2Y9T7sbpjid5XlSg76m+mzraGSBX/Ad96aRTnnXAxfbpexdrvf+C2B/6vRm2rSkZmJr8+qj0vPj+C039zIVu3bOPmW/smfmE1uXqsVyadzoATjYKIxGd7jxArX7QGQFW3iEhpZS8qP8NQdUoSuVq9NYxKsRs2bGTGjFl0736q7zW5ahNr0ZKvuW/gUwx54iH2adIYiP2HPf/sbtx6w9U/e/6gf9wHxPqA7374CV4avGdl5BbNc1ldspaWzfeltLQMFYX4xRYlyk/ZG8j6qQEZZRVfGNxl3dr1ux+/89oYBr3yWLXalYxVK1fz3crVzP80VjbpvdETfEnArh/reytLgTPbZCU6A25CrCz9PCBHRFoCiEhD8L6VrlZvDSpWbm4OTeJJrF69unTpchKLfLio41WsVatL6P/Xh/jHfX/moAPa7F7fqWMHJn/wMT+s/xGADRs38d3q5P4Tdzm5E6PHTQFg0gczyCjNQhAUZXv2JjJ31COztG7i9jVvtvvx6WefytKi2o/Q2FtJyVq+W7maX/zyYAB+c+qJLFrk/efl4rFelagkv4StyjNgVT2okh9FgQu93hlXq7cGFatly+bkDXuCjEiESCTCyHfeZ8L4aZ7HSTZWTk4mdetFiESgZas6bNxYusdv7WdffJ0NGzfxt8djM6NlZGTw5guD+MXBB3LTH6+gb/+7iWqUrMxM7r7tRvZr2SLhfv323DO566HHOPvia2jSuBFZP2UDUJa1nWjGTlSilNb5CYC6WxsRiWbyj2cf4NjOR7NPzj5MmD+KIY89z7Gdj+ZXR7RFVVm1YjV/+/OjVYWtsbvueIghzz1OVlYW336zgpv73eV5DBeP9apE0+gM2Koip7i6mVX/qZyuflzuzy+Gvdl0lOnFi6rI77b8fdI554LVr4eare1OOGOMU1Lh4lqyLAEbY5wSrWAkRqqyBGyMcUpyt+ykBkvAxhinpMLohmRZAjbGOCWdRkFYAk5x20srn8wmnZ3R4bpA4hxct1niJ3nkhx3BzSexPvFTPBNJoz5V8LYkkd8sARtjnGJdEMYYExIbhmaMMSEpszNgY4wJh50BG2NMSCwBG2NMSFKg1FvSLAEbY5ySTmfAKVWUE9yt3hpULFfatG+rfXnyzcd4afrzvDh1GBddG5v9tNE+jXjs9YG8MuMlHnt9IA2bNKx1rGatchmQ/zcGTf0XT00ezDlXnwdA7/6XMGz2izwx7imeGPcUx3Q5ttax9nbNdZcy+d/vMOWTUVx7/WWeb3+XII+LvKGPU7yigAXzp/gapzJl1VjCllLTUUYiEQq/mMFZPS6huHgVs2aO47LLb6SwcLHn++VirHRq02+at6/y5znNc2jWPIfFC5dQP7s+Q8c/w73X3s9ZF5/Bxh83MeJfb3BJv940atKIvL8/V+l2mmYkrrHWtHlTmjZvyrKFy6iXXZ/Hxz7JwL5/56RzTuanrdsYnfduUm2at+nrpJ63y6Htfsm/nnuU87r9np07dvLKW0P46+0P8c2y5Qlf+93mdUnHqe1nVd0bMU4++QQ2b97Ciy88xdHHdKvWa3dsL651B8LTB1yWdM65ZfmroXZYJCrKeYKINI4/ri8iA0TkPRF5RESaeL0zrlZvDSqWS21aV7KOxQuXALBtyzaWL15ObstcOp/RmYlvTQZg4luTOenMzrWOtb5kPcsWxipe/LRlG8VLimnWwv876Noeegjz5/2Hn7b9RFlZGbM+mcdZ53T1PE7QlYo//ng26+PVTMLgdU04EckQkQUiMjb+fY6ITBaRxfGvTWu6r4m6IF4AtsYfP02sRNEj8XUv1jRoZVyt3hpULBfbBNCiTQt+ecQvKVxQRE5uU9aVxM7+1pWso2mzfTyNtW+b5hx8+CF8VbAIgLOvOIcnJwyi32M3k90429NYiwoXc8KJx7JP0ybUq1+PLt1/Q6vW6X1cpAIfinLeAhSW+/5OYKqqtgWmxr+vkUQJOKKqu4pvdlTV/qr6cbwy8iGVvciqIocTy8U21WtQjwfz7uNfDzzL1s1bE7+glrHuGHInLzz4HNs2b2PCq+O58ZTr+NPZt7C+ZB1X3Xutp/GWfPU1zw56gdfeyeOVt4ZQuHARZWXe90ymQqXiIGk1lkREpA1wDlC+n6snMDz+eDhwQU33NVECXigiu0rTfiYiHeM7dShQ6Swxqpqnqh1VtWMkkvxZg6vVW4OK5VqbMjIzeDDvfqaMmsaM8R8DsWrFOc1zgFg/8foffvQs1p+H3MlH737I7AkzAdiw9kei0SiqyuQRk2h7VFtPYpX3xqujOKdLb3qdexU/rt/A10u/9TxGKlQqDlJ1inKWP1mML3uXpX4KuIM9T5hbqOoqgPjX5jXd10QJ+A/AqSKyFGgPzBSRZcCw+M885Wr11qBiudamOx7/E98uWc5bw0buXvfJ5Jmc2as7AGf26s4nkz7xJFa/R29i5ZJi3ntu9O51TZv/t2vvhDM7sXyR98mxWW7sl8l+rVty1rndGDNyvOcxUqFScZCqMwqi/MlifMnbtR0RORcoUdVP/drXRFWRNwBXiUgjYl0OmUCxqvry69PV6q1BxXKpTUccdzhn/K47SwuXMWziEACee+QFRgzO5/4h99Kjz9mUrCzhgesfqnWswzq247SLTuebwm94YtxTALz22CucfP4pHNz+YFRhTfH3DPnrM7WOtbehw5+kac4+7NxZyr13PMyGDd5PaRl0peJXXh7MKaecSG5uDsuWzuXBh57gpZfyfYu3t6h3E1KeBJwvIj2AekBjEXkV+F5EWqnqKhFpBZTUNEBKDUMz/zsSDUPzSjLD0LxS3WFotVGdYWi1FeR8wF4MQ3vowEuTzjn3fvtaUvFE5DTgdlU9V0QeA35Q1YEicieQo6p31GRf7U44Y4xTAjjjGwi8KSLXAsuBXjXdkCVgY4xT/LgVWVU/AD6IP/4B8GTAtiVgY4xTSiV9ej0tARtjnJI+6dcSsDHGMek0G5olYBOKpVtXBxLn8Ow2gcQB+Gv2UYHF6r/1o8BiRTWdUpqnw9B8ZwnYGOOU9Em/loCNMY5Jp/N1S8DGGKeUpdE5sCVgY4xT7AzYGGNConYGbIwx4UinM2AryulYLBfbBP4Wr8xtlcvANwYydNpQhkwZQs9regJw7d3Xkjc9j2cmPcO9w+71rCKGRITfjf8bZ7/4pz3WH3VdD65f8Sr1mta+0Gh5bdq0YuLEfAoKpjJ//hT69bvG0+3vLeyinFE06SVsKZWAI5EIg55+mHPPu4wjj+pC794X0K6d95NguxrLxTZBrHjlJVdcxHndfs+Zv/kdXc84lYMOOcCz7ZeVlTHsoWFcd/p13NrzVs698lwOaHsAC2Ys4Ppu13PjGTeyctlKevfr7Um8I689i/VLvttjXXarHNr85gg2Fa/1JEZ5paVl/OUvf6NDh66cckpPrr/+Cg47zJ/PCuDlV97i3PP8q/CciJcVMfyWUgnYpaKSYcRysU3gf/HK9SXrWbpwKRArALpiyQqatWzG/I/mEy2L/UFbtKCI3Fa5tY6V3TKHA07vQOGID/ZY3/n+y5j1cD74MD3s6tUlFBQsBGDz5i0UFS2htQ+153YJuyhnKZr0ErZEVZFvFpH9g9oZV4tKWlHO2gmqeCVA8zbN+cXhv2DRgkV7rD/j4jOYO31urbff+YHLmPX3ERD973/+A7sfw9bV6/mhMHE5+to68MA2dOhwOHPmLPA9Vli0Gv/Clugi3EPAnfGSRCOAt1R1TaKNxusq9QWQjCYkWxfOxaKSQcZysU2wZ/HKrVu2+Va8sl6Detwz9B6GPjB0jwKgfW7qQ1lZGdNHTa/V9g/o2oGfftjI2s+/Yb9O7QDIrFeHY246n/cvfaRW205GdnYDRowYyu23D2DTps2+xwtLOl2ES5SAlwHHAt2A3sAAEfmUWDJ+R1U3VfSieF2lPKheRQzXikoGHcvFNu3yxqujeOPVUQDccc/NrPrO+wKg9+Tdw/R3p/PJhP/Wmev2u24c3/V47upzV61jtOx4KAd2P4YDuhxFRt0sshrV5/Snr6fx/vvSa+LfgVhf8EXj/8Y7593PtjUbah1zl8zMTPLzh5KfP4rRoyd4tt1UlApntslKlIBVVaPAJGCSiGQBZwOXAI8D+3q5M+WLB65cuZqLL+7J5Vf4c3XdxVgutmmXZrk5/LB23e7ilRee6e1Fnv6P9WfF4hWMGjZq97pjTzuWXjf04o5ed7D9p+21jjHnkTeZ88ibAOzXqR1HXdeDSdcN2uM5l37yT0aecy8/rff2DHXo0McoKlrCoEHPJX5ymnPpDHiPvzNVdScwBhgjIp4X23KpqGQYsVxs0y5+Fq88/LjD6fa7bnxd+DWDJwwGYPgjw7n+wevJqpPFw68/DEDR/CIG/3WwZ3GD0rnzcVx66UV8/nkhs2fHqi7fd9+jTJxYuy6VyoRdlLPM5zqXXqqyKKeIHKqqtfpfZUU5TUX2a5gTSJwgp6PsSbPAYvVf4+Z0lF4U5fz9gRcmnXNe/3ZUcBVHK5CoLL1/pzTGGOMDl/qAjTEmrbjUB2yMMWklFW4xTpYlYGOMU6wLwhhjQpJOoyAsARtjnGJdEMYksHrL+kDilGz17m6yRNbmHBRYrNKo97diVyZSwW3nqcwuwhljTEisD9gYY0KSTl0QKTUfsDHG1JaqJr1URUT2F5HpIlIoIl+IyC3x9TkiMllEFse/Nq3pvloCNsY4pQxNekmgFPiTqrYDOgH9RKQ9cCcwVVXbAlPj39eIJWBjjFO8qgmnqqtUdX788SagEGgN9ASGx582HLigpvtqfcDGGKf4VOzgIOBoYDbQQlVXxWOtEpHmNd1uyp0Bu1rV16oi11yQVXb9riB875N/YeJ/RpM/7aU91l98zW95e8arvDF9ODfdc72nMSHY4yKdqiKLSF8RmVdu6bv39kSkITAS6K+q3s2DSoolYFer+lpV5NoJssqu3xWEx74xgZsv/fMe647tfDSnnnkyl3S9mt5druTVZ72dOzfIzwpSoSpyNf6p5qlqx3JLXvltxYtQjAReU9V34qu/F5FW8Z+3Akpquq+JinLWEZErRKRb/Pvfi8hgEekX3zFPuVrV16oi106QVXb9riC8YPZnbFy/50nURVf0ZPjg19i5YycA63/40bN4EOxnBeFXRS5TTXqpisQKHz4PFKrqk+V+NAa4Mv74SmB0Tfc10Rnwi8A5wC0i8grQi1gfyHGA57VNXK3qa1WR01NQFYQP/MX+dDjh17w4dghDRw6i/VGHebr9/4XPqjyvLsIBJwGXA6eLSEF86QEMBLqLyGKge/z7Gkl0Ee5IVf21iGQCK4H9VLVMRF4FPqvsRVYVOZxYLrYpLEFWEM7IyKBRk0Zcfe71tO/Qjr8PHcAFnXp7tn3XP6u9eXUjhqp+zF5l2crp6kWMRGfAERGpAzQCGgBN4uvrApV2QZTvV0k2+YK7VX2tKnJ6CbqCcMmqNUwfFysx9GVBIRqNsk9OkwSvSp7Ln1VFvLoRIwiJEvDzQBFQANwNvCUiw4C5gOdV9spX2s3KyuLii3vy3thJXodxNpaLbQpD0BWEP5gwg+NOPgaAAw5pQ1adLH5c590kQi5/VhXxsAvCd4lqwv1TRN6IP/5ORF4GugHDVHWO1zvjalVfq4pcO0FW2fW7gvDfnrmPY088mn1ymjB23tvkPfEiY/LHcd+Td5I/7SV27izlgVv+7kmsXYKuYB12VeR0moynyqrIXrCqyKYiQU1xGJHgRloeGeB0lAvWLg0sVpDTUXpRFfmYVicnnXPmr/o4dasiG2NMukmFvt1kWQI2xjglFfp2k2UJ2BjjlHTqA7YEbIxxStS6IIwxJhx2BmyMMSEp0/Qpy2kJ2IQiqD8Toxpc9eAlG79L/CSP1M30fC6sSm0v3RlYLC9YF4QxxoTEuiCMMSYkdgZsjDEhsTNgY4wJSVmA/f61ZQnYGOMUuxXZGGNCkk63IqdUUU5ws6pvkLFcbJPLsT774gP+Pft9PvpkDNM+GuVLjLp16/LhR+8ya9Z45s6bxN333OpLnF2CfP8qkk4TsqfUdJSRSITCL2ZwVo9LKC5exayZ47js8hspLFzs+X65GMvFNqVTrEZ16lc73mdffECXUy5k3Q/rq/W6HdHSaj0/O7sBW7ZsJTMzkylT3+bPtw9g7tzkat1VZxxwbT+r0h0raz09ZKt92iedc1b9+GWo01EmPAMWkV+IyO0i8rSIPCEi14uId/VSynG1qq9VRbZYYduyZSsAWVmZZGVl+jZSIBXev+qUpQ9borL0NwNDgHrEKiHXB/YHZorIaV7vjKtVfa0qssWqjKryzuiXmD7jXa682rtCnHuLRCLMnDWOb779lGlTP2be3AJf4qRCBeYyjSa9hC3RRbg/Ah3ilZCfBMap6mkiMhQYDRxd0YusKnI4sVxsk8uxAM7q1pvVq0vI3TeHUWOGs/irZXzy77mex4lGo5zYqQdNmjRmRP5Q2rc/1JeyRKlQgTkV+naTlcxFuF1Jui6x6sio6nKsKnLKxXKxTS7HAli9ugSAtWvWMfa9yRxz7K99iwWwYcNGZsyYRffup/qy/VSowBxVTXoJW6IE/BwwV0TygJnAYAAR2RdY5/XOuFrV16oiW6yKNGhQn4YNs3c/Pv30kyn80vsLi7m5OTRp0hiAevXq0qXLSSz6yp+acqlQgTmdRkEkqor8tIhMAdoBT6pqUXz9GuAUr3fG1aq+VhXZYlVk3+a5vDriGQAyMjMZ+eYYpk75yPM4LVs2J2/YE2REIkQiEUa+8z4Txk/zPA4EX4G5Iuk0DjilhqEZk85qMgytpqo7DK02gpyO0othaI2zD0k652zcssyqIhtjjFdSYXRDsiwBG2OckgoX15JlCdgY45RUuLiWrJSbC8IYY2rDyzvhROQsEVkkIktE5E6v99XOgI0xTvHqDFhEMoB/Ad2BYmJDcseo6peeBMASsDHGMR72AR8PLFHVZQAikg/0BNInAdd0WImI9FXVPK/3J6w4Fiu9YrnYJpdjlVednFN+2oS4vHL73BpYUe5nxcAJtd/D/0rlPuC+iZ+SVnEsVnrFcrFNLseqkfLTJsSX8r8wKkrknl7hS+UEbIwxYSomNvvjLm2A7yp5bo1YAjbGmIrNBdqKyMEiUgfoA4zxMkAqX4QLqu8oyD4qi5U+sVxsk8uxPKeqpSLyf8BEIAN4QVW/8DKG73NBGGOMqZh1QRhjTEgsARtjTEhSLgH7fetfuTgviEiJiCz0K0a5WPuLyHQRKRSRL0TkFp/i1BOROSLyWTzOAD/i7BUzQ0QWiMhYn+N8IyKfi0iBiMzzOdY+IvK2iBTFP7MTfYrzq3h7di0bRaS/T7FujR8TC0VkhIjU8yNOPNYt8Thf+NUeZ1Rn9ni/F2Id3UuBQ4A6wGdAe59inQIcAywMoF2tgGPijxsBX/nRLmLjFhvGH2cBs4FOPrftNuB1YKzPcb4Bcv3+rOKxhgN/iD+uA+wTQMwMYDVwoA/bbg18DdSPf/8mcJVP7TgCWAg0IHaRfwrQNojPLR2XVDsD3n3rn6ruAHbd+uc5Vf0IH8oqVRJrlarOjz/eBBQS+0/hdRxV1c3xb7Pii29XWUWkDXAOsdJVThCRxsR+OT8PoKo7VPXHAEJ3BZaq6rc+bT8TqC8imcSSo6fjWctpB8xS1a2qWgp8CFzoU6y0l2oJuKJb/zxPVGESkYOIVZOe7dP2M0SkACgBJquqL3HingLuAIKYAVuBSSLyafz2Ub8cAqwBXox3rTwnIslXlq25PsAIPzasqiuBx4HlwCpgg6r6VahtIXCKiDQTkQZAD/a8mcGUk2oJ2Pdb/8IkIg2BkUB/Vd3oRwxVLVPVDsTu2jleRI7wI46InAuUqOqnfmy/Aiep6jHA2UA/EfG8JmFcJrGuqWdV9WhgC+DbtQiA+CD/84G3fNp+U2J/SR4M7Adki8hlfsRS1ULgEWAyMIFYN2Jw9ZPSTKolYN9v/QuLiGQRS76vqeo7fseL/9n8AXCWTyFOAs4XkW+IdRWdLiKv+hQLVf0u/rUEGEWsu8oPxUBxub8c3iaWkP10NjBfVf2q394N+FpV16jqTuAdoLNPsVDV51X1GFU9hVg3n/elnh2RagnY91v/wiAiQqxPsVBVn/Qxzr4isk/8cX1i//GK/IilqnepahtVPYjY5zRNVX05qxKRbBFptOsxcAaxP3U9p6qrgRUi8qv4qq54OP1gJS7Bp+6HuOVAJxFpED8WuxK7DuELEWke/3oA8Fv8bVtaS6lbkTWAW/92EZERwGlArogUA/er6vN+xCJ2tng58Hm8fxbgr6o6zuM4rYDh8YmkI8Cbqurr8LCAtABGxXIHmcDrqjrBx3g3Aa/FTwKWAVf7FSjeT9oduM6vGKo6W0TeBuYT6w5YgL+3CY8UkWbATqCfqq73MVZas1uRjTEmJKnWBWGMMf8zLAEbY0xILAEbY0xILAEbY0xILAEbY0xILAEbY0xILAEbY0xI/j9Zels+y3XzygAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm2 = confusion_matrix(y_test, Y_Pred)\n",
    "sns.heatmap(cm2, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2da8a219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5490196078431373"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, Y_Pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff62dae5",
   "metadata": {},
   "source": [
    "## Extreme Learning Machine (ELM)\n",
    "\n",
    "### What is ELM?\n",
    "Extreme learning machines are feed-forward neural networks for classification, regression, clustering, sparse approximation, compression and feature learning with a single layer or multiple layers of hidden nodes, where the parameters of hidden nodes (not just the weights connecting inputs to hidden nodes) need not be tuned. These hidden nodes can be randomly assigned and never updated (i.e. they are random projection but with nonlinear transforms), or can be inherited from their ancestors without being changed. In most cases, the output weights of hidden nodes are usually learned in a single step, which essentially amounts to learning a linear model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53818935",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "### What is Linear Regression?\n",
    "Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable. For example, a modeler might want to relate the weights of individuals to their heights using a linear regression model.\n",
    "\n",
    "### How does it work?\n",
    "A linear regression line has an equation of the form Y = a + bX, where X is the explanatory variable and Y is the dependent variable. The slope of the line is b, and a is the intercept (the value of y when x = 0). \n",
    "### Advantages\n",
    "1. Linear Regression is a very simple algorithm that can be implemented very easily to give satisfactory results.\n",
    "2. Linear regression fits linearly seperable datasets almost perfectly and is often used to find the nature of the relationship between variables.\n",
    "3. Overfitting can be reduced by regularization. <strong>Overfitting</strong> is a situation that arises when a machine learning model fits a dataset very closely and hence captures the noisy data as well.This negatively impacts the performance of model and reduces its accuracy on the test set. <strong>Regularization</strong> is a technique that can be easily implemented and is capable of effectively reducing the complexity of a function so as to reduce the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d59e4a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73fd94bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination is:  0.7229117431679706\n",
      "Model intercept:  65.02658297280007\n",
      "Slope:  [ 2.56401747e-03  4.26552571e-03  6.01508031e-01  1.84289328e-02\n",
      "  1.36171431e-01 -1.10263654e+00 -1.91027354e+00 -1.99825456e+00\n",
      " -1.52506725e+00 -6.34244194e+01 -6.30677136e+01 -6.27916193e+01\n",
      "  4.93669562e+00]\n"
     ]
    }
   ],
   "source": [
    "    r_sq = model.score(X_train, y_train)\n",
    "    print('Coefficient of determination is: ', r_sq)\n",
    "    print('Model intercept: ', model.intercept_)\n",
    "    print(\"Slope: \", model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "394484f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values:  [7, 6, 8, 7, 8, 9, 10, 9, 7, 10, 9, 7, 8, 8, 8, 9, 9, 7, 7, 8, 7, 9, 5, 9, 9, 9, 7, 7, 10, 8, 8, 9, 8, 7, 11, 9, 8, 11, 8, 6, 9, 7, 8, 7, 10, 7, 9, 7, 6, 8, 8, 7, 9, 7, 7, 10, 8, 8, 8, 7, 10, 7, 9, 8, 9, 8, 8, 8, 7, 9, 9, 7, 9, 10, 8, 9, 8, 8, 7, 9, 6, 8, 10, 7, 10, 8, 5, 8, 8, 10, 10, 9, 11, 10, 8, 4, 10, 7, 8, 9, 6, 7, 6, 9, 8, 9, 9, 6, 8, 8, 8, 8, 8, 10, 10, 5, 9, 8, 6, 8, 8, 8, 9, 9, 11, 10, 8, 8, 7, 6, 10, 9, 7, 9, 9, 7, 7, 7, 8, 7, 9, 8, 8, 8, 9, 8, 7, 9, 7, 8, 10, 9, 7, 9, 9, 8, 9, 6, 8, 9, 9, 8, 8, 8, 8, 7, 9, 8, 8, 8, 7, 10, 10, 11, 7, 4, 10, 8, 6, 8, 9, 9, 10, 8, 7, 8, 10, 9, 9, 9, 8, 6, 7, 6, 6, 8, 7, 9, 8, 6, 10, 9, 10, 9, 7, 7, 7, 9, 10, 8, 7, 6, 8, 9, 9, 7, 6, 6, 7, 7, 9, 7, 11, 9, 9, 9, 10, 10, 8, 7, 10, 9, 7, 10, 8, 8, 10, 8, 8, 7, 9, 8, 9, 9, 10, 8, 9, 10, 9, 9, 7, 7, 8, 8, 8, 5, 9, 8, 10, 9, 7, 8, 9, 8, 7, 7, 9, 8, 9, 8, 10, 11, 8, 6, 8, 7, 6, 7, 8, 10, 8, 7, 7, 8, 9, 9, 11, 6, 10, 7, 7, 4, 8, 7, 10, 11, 9, 8, 10, 5, 7, 10, 11, 9, 10, 8, 9, 6, 8, 9, 7, 13, 8, 7, 8, 4, 10, 8, 8, 8, 9, 8, 7, 9, 9, 6, 7, 11, 9, 8, 7, 7, 7, 6, 9, 6, 7, 9, 7, 7, 7, 7, 8, 10, 8, 7, 8, 6, 9, 9, 9, 7, 7, 9, 8, 7, 10]\n"
     ]
    }
   ],
   "source": [
    "y_pred = list(map(int, model.predict(X_test)))\n",
    "print(\"Predicted values: \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c44b367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.7008\n",
      "Variance score: 0.7090\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean squared error: %.4f\" % np.mean((model.predict(X_test) - y_test) ** 2))\n",
    "print('Variance score: %.4f' % model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "024f4b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4061624649859944"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ba8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
