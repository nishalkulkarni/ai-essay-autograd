{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1acaeb2c",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "In this notebook we perform data sanitization and extraction, the data is taken from kaggle consisting of more than 12000 essays.\n",
    "\n",
    "As part of extracting features we use ideas from multiple research papers referenced below.\n",
    "\n",
    "### Novelty\n",
    "In addition to already proposed ideas we also use **Latent Semantic Indexing** for extracting concepts and getting similarity between essays from the given text essays.\n",
    "\n",
    "\n",
    "#### REFRENCES:\n",
    "1. Mahana, M., Johns, M., & Apte, A. (2012). Automated essay grading using machine learning. Mach. Learn. Session, Stanford University.\n",
    "\n",
    "2. Suresh, A., & Jha, M. (2018). Automated essay grading using natural language processing and support vector machine. International Journal of Computing and Technology, 5(2), 18-21.\n",
    "\n",
    "3. Rokade, A., Patil, B., Rajani, S., Revandkar, S., & Shedge, R. (2018, April). Automated Grading System Using Natural Language Processing. In 2018 Second International Conference on Inventive Communication and Computational Technologies (ICICCT) (pp. 1123-1127). IEEE.\n",
    "\n",
    "4. Song, S., & Zhao, J. (2013). Automated essay scoring using machine learning. Stanford University.\n",
    "\n",
    "5. Kakkonen, T., Myller, N., & Sutinen, E. (2006). Applying Part-of-Seech Enhanced LSA to Automatic Essay Grading. arXiv preprint cs/0610118."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ede996d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import enchant\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d401a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/nishal/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/nishal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/nishal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nishal/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb93d2",
   "metadata": {},
   "source": [
    "### Loading data set\n",
    "\n",
    "8 essay set, totalling 12977 unique essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85a99784",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('training_set_rel3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5948d9da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10686</th>\n",
       "      <td>17834</td>\n",
       "      <td>7</td>\n",
       "      <td>Patience is when your waiting .I was patience ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10687</th>\n",
       "      <td>17836</td>\n",
       "      <td>7</td>\n",
       "      <td>I am not a patience person, like I can’t sit i...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10688</th>\n",
       "      <td>17837</td>\n",
       "      <td>7</td>\n",
       "      <td>One day I was at basketball practice and I was...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10689</th>\n",
       "      <td>17838</td>\n",
       "      <td>7</td>\n",
       "      <td>I going to write about a time when I went to t...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10690</th>\n",
       "      <td>17839</td>\n",
       "      <td>7</td>\n",
       "      <td>It can be very hard for somebody to be patient...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12250</th>\n",
       "      <td>19558</td>\n",
       "      <td>7</td>\n",
       "      <td>One time I was getting a cool @CAPS1 game it w...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12251</th>\n",
       "      <td>19559</td>\n",
       "      <td>7</td>\n",
       "      <td>A patent person in my life is my mom. Aicason ...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12252</th>\n",
       "      <td>19561</td>\n",
       "      <td>7</td>\n",
       "      <td>A time when someone else I know was patient wa...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12253</th>\n",
       "      <td>19562</td>\n",
       "      <td>7</td>\n",
       "      <td>I hate weddings. I love when people get marrie...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12254</th>\n",
       "      <td>19563</td>\n",
       "      <td>7</td>\n",
       "      <td>A few weeks ago, we had a garage sale and a mo...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1569 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set                                              essay  \\\n",
       "10686     17834          7  Patience is when your waiting .I was patience ...   \n",
       "10687     17836          7  I am not a patience person, like I can’t sit i...   \n",
       "10688     17837          7  One day I was at basketball practice and I was...   \n",
       "10689     17838          7  I going to write about a time when I went to t...   \n",
       "10690     17839          7  It can be very hard for somebody to be patient...   \n",
       "...         ...        ...                                                ...   \n",
       "12250     19558          7  One time I was getting a cool @CAPS1 game it w...   \n",
       "12251     19559          7  A patent person in my life is my mom. Aicason ...   \n",
       "12252     19561          7  A time when someone else I know was patient wa...   \n",
       "12253     19562          7  I hate weddings. I love when people get marrie...   \n",
       "12254     19563          7  A few weeks ago, we had a garage sale and a mo...   \n",
       "\n",
       "       rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "10686             8.0             7.0             NaN           15.0   \n",
       "10687             6.0             7.0             NaN           13.0   \n",
       "10688             7.0             8.0             NaN           15.0   \n",
       "10689             8.0             9.0             NaN           17.0   \n",
       "10690             7.0             6.0             NaN           13.0   \n",
       "...               ...             ...             ...            ...   \n",
       "12250             6.0             6.0             NaN           12.0   \n",
       "12251             9.0             7.0             NaN           16.0   \n",
       "12252            11.0             8.0             NaN           19.0   \n",
       "12253            12.0            10.0             NaN           22.0   \n",
       "12254             7.0             8.0             NaN           15.0   \n",
       "\n",
       "       rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n",
       "10686             NaN             NaN            NaN  ...            2.0   \n",
       "10687             NaN             NaN            NaN  ...            2.0   \n",
       "10688             NaN             NaN            NaN  ...            2.0   \n",
       "10689             NaN             NaN            NaN  ...            2.0   \n",
       "10690             NaN             NaN            NaN  ...            1.0   \n",
       "...               ...             ...            ...  ...            ...   \n",
       "12250             NaN             NaN            NaN  ...            2.0   \n",
       "12251             NaN             NaN            NaN  ...            2.0   \n",
       "12252             NaN             NaN            NaN  ...            2.0   \n",
       "12253             NaN             NaN            NaN  ...            2.0   \n",
       "12254             NaN             NaN            NaN  ...            2.0   \n",
       "\n",
       "       rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  \\\n",
       "10686            2.0            NaN            NaN            NaN   \n",
       "10687            1.0            NaN            NaN            NaN   \n",
       "10688            2.0            NaN            NaN            NaN   \n",
       "10689            3.0            NaN            NaN            NaN   \n",
       "10690            2.0            NaN            NaN            NaN   \n",
       "...              ...            ...            ...            ...   \n",
       "12250            1.0            NaN            NaN            NaN   \n",
       "12251            3.0            NaN            NaN            NaN   \n",
       "12252            2.0            NaN            NaN            NaN   \n",
       "12253            3.0            NaN            NaN            NaN   \n",
       "12254            2.0            NaN            NaN            NaN   \n",
       "\n",
       "       rater3_trait2  rater3_trait3  rater3_trait4  rater3_trait5  \\\n",
       "10686            NaN            NaN            NaN            NaN   \n",
       "10687            NaN            NaN            NaN            NaN   \n",
       "10688            NaN            NaN            NaN            NaN   \n",
       "10689            NaN            NaN            NaN            NaN   \n",
       "10690            NaN            NaN            NaN            NaN   \n",
       "...              ...            ...            ...            ...   \n",
       "12250            NaN            NaN            NaN            NaN   \n",
       "12251            NaN            NaN            NaN            NaN   \n",
       "12252            NaN            NaN            NaN            NaN   \n",
       "12253            NaN            NaN            NaN            NaN   \n",
       "12254            NaN            NaN            NaN            NaN   \n",
       "\n",
       "       rater3_trait6  \n",
       "10686            NaN  \n",
       "10687            NaN  \n",
       "10688            NaN  \n",
       "10689            NaN  \n",
       "10690            NaN  \n",
       "...              ...  \n",
       "12250            NaN  \n",
       "12251            NaN  \n",
       "12252            NaN  \n",
       "12253            NaN  \n",
       "12254            NaN  \n",
       "\n",
       "[1569 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay_set_num = 7\n",
    "data = data.loc[data['essay_set'] == essay_set_num]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867fda30",
   "metadata": {},
   "source": [
    "### Filtering data set\n",
    "\n",
    "We only use the actual essay along with the domain score for training, all of the other columns are discarded.\n",
    "\n",
    "We do this for feature extraction of every set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "debdf4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.iloc[:, 1:2], inplace=True, axis=1)\n",
    "data.drop(data.iloc[:, 2:5], inplace=True, axis=1)\n",
    "data.drop(data.iloc[:, 3:], inplace=True, axis=1)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b9a0907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17834</td>\n",
       "      <td>Patience is when your waiting .I was patience ...</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17836</td>\n",
       "      <td>I am not a patience person, like I can’t sit i...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17837</td>\n",
       "      <td>One day I was at basketball practice and I was...</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17838</td>\n",
       "      <td>I going to write about a time when I went to t...</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17839</td>\n",
       "      <td>It can be very hard for somebody to be patient...</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>19558</td>\n",
       "      <td>One time I was getting a cool @CAPS1 game it w...</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>19559</td>\n",
       "      <td>A patent person in my life is my mom. Aicason ...</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>19561</td>\n",
       "      <td>A time when someone else I know was patient wa...</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>19562</td>\n",
       "      <td>I hate weddings. I love when people get marrie...</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>19563</td>\n",
       "      <td>A few weeks ago, we had a garage sale and a mo...</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1569 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id                                              essay  \\\n",
       "0        17834  Patience is when your waiting .I was patience ...   \n",
       "1        17836  I am not a patience person, like I can’t sit i...   \n",
       "2        17837  One day I was at basketball practice and I was...   \n",
       "3        17838  I going to write about a time when I went to t...   \n",
       "4        17839  It can be very hard for somebody to be patient...   \n",
       "...        ...                                                ...   \n",
       "1564     19558  One time I was getting a cool @CAPS1 game it w...   \n",
       "1565     19559  A patent person in my life is my mom. Aicason ...   \n",
       "1566     19561  A time when someone else I know was patient wa...   \n",
       "1567     19562  I hate weddings. I love when people get marrie...   \n",
       "1568     19563  A few weeks ago, we had a garage sale and a mo...   \n",
       "\n",
       "      domain1_score  \n",
       "0              15.0  \n",
       "1              13.0  \n",
       "2              15.0  \n",
       "3              17.0  \n",
       "4              13.0  \n",
       "...             ...  \n",
       "1564           12.0  \n",
       "1565           16.0  \n",
       "1566           19.0  \n",
       "1567           22.0  \n",
       "1568           15.0  \n",
       "\n",
       "[1569 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_essays = data.shape[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "146f02d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay  domain1_score\n",
       "0         5      6              7"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([[5, 6, 7]], columns=[\"essay_id\",\"essay\",\"domain1_score\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fa6f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordlist(sentence):\n",
    "    # Remove non-alphanumeric characters\n",
    "    sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", sentence)\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b2cd50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentences(essay):\n",
    "    sentences = nltk.sent_tokenize(essay.strip())\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > 0:\n",
    "            tokenized_sentences.append(get_wordlist(sentence))\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290b804",
   "metadata": {},
   "source": [
    "### Numerical features \n",
    "Features like the average length of words, the word count and the sentence count give us an idea about the fluency in language and dextirity of the writer.\n",
    "\n",
    "Reference - \n",
    "\n",
    "1. Mahana, M., Johns, M., & Apte, A. (2012). Automated essay grading using machine learning. Mach. Learn. Session, Stanford University."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e7a2701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_length_average(essay):\n",
    "    # Sanitize essay\n",
    "    essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(essay)\n",
    "    avg = sum(len(word) for word in words) / len(words)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02ab1218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count(essay):\n",
    "    essay = re.sub(r'\\W', ' ', essay)\n",
    "    count = len(nltk.word_tokenize(essay))\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10c8510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_count(essay):\n",
    "    sentences = nltk.sent_tokenize(essay)\n",
    "    count = len(sentences)\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d71861",
   "metadata": {},
   "source": [
    "### Lemmatization and Part of Speech tagging\n",
    "\n",
    "Lemmatization involves use of a vocabulary to perform a morphological analysis of words. Getting lemma count along with different part of speech count like that of nouns, adjectives, verbs, adverbs allows us to understand the lexical density and overall semantic difficulty of the essay.\n",
    "\n",
    "Reference - \n",
    "\n",
    "2. Suresh, A., & Jha, M. (2018). Automated essay grading using natural language processing and support vector machine. International Journal of Computing and Technology, 5(2), 18-21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "872ad2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_count(essay):\n",
    "    tokenized_sentences = get_tokenized_sentences(essay)      \n",
    "    \n",
    "    lemmas = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        pos_tagged_tokens = nltk.pos_tag(sentence) \n",
    "        for token_tuple in pos_tagged_tokens:\n",
    "            word = token_tuple[0]\n",
    "            pos_tag = token_tuple[1]\n",
    "            # assume default part of speech to be noun\n",
    "            pos = wordnet.NOUN\n",
    "            if pos_tag.startswith('J'):\n",
    "                pos = wordnet.ADJ\n",
    "            elif pos_tag.startswith('V'):\n",
    "                pos = wordnet.VERB\n",
    "            elif pos_tag.startswith('R'):\n",
    "                pos = wordnet.ADV\n",
    "                \n",
    "            lemmas.append(WordNetLemmatizer().lemmatize(word, pos))\n",
    "    \n",
    "    lemma_count = len(set(lemmas))\n",
    "    \n",
    "    return lemma_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aa494c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_counts(essay):\n",
    "    tokenized_sentences = get_tokenized_sentences(essay)\n",
    "    \n",
    "    nouns, adjectives, verbs, adverbs = 0, 0, 0, 0\n",
    "    \n",
    "    essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(essay)\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        pos_tagged_tokens = nltk.pos_tag(sentence)\n",
    "        for token_tuple in pos_tagged_tokens:\n",
    "            pos_tag = token_tuple[1]\n",
    "            if pos_tag.startswith('N'): \n",
    "                nouns += 1\n",
    "            elif pos_tag.startswith('J'):\n",
    "                adjectives += 1\n",
    "            elif pos_tag.startswith('V'):\n",
    "                verbs += 1\n",
    "            elif pos_tag.startswith('R'):\n",
    "                adverbs += 1\n",
    "    \n",
    "    return nouns/len(words), adjectives/len(words), verbs/len(words), adverbs/len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940da472",
   "metadata": {},
   "source": [
    "### Spell Check (Orthography)\n",
    "Correct word spelling indicates command over language and facility of use. To test for these characteristics we extracted the count of spelling errors per essay. We used PyEnchant spell checker along with the hunspell dictionary to obtain the count of misspelt words per essay.\n",
    "\n",
    "Reference - \n",
    "\n",
    "1. Mahana, M., Johns, M., & Apte, A. (2012). Automated essay grading using machine learning. Mach. Learn. Session, Stanford University."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6853e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spell_error_count(essay):   \n",
    "    essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(essay)\n",
    "    \n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    misspelt_count = 0\n",
    "    for word in words:\n",
    "        if(d.check(word) == False):\n",
    "            misspelt_count += 1\n",
    "    \n",
    "    total_words = get_word_count(essay)\n",
    "    error_prob = misspelt_count/total_words\n",
    "    \n",
    "    return error_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21efe81f",
   "metadata": {},
   "source": [
    "### Sentiment Analysis (Opinion mining)\n",
    "It allows us to gauge the emotive effectiveness of the essay. We use the VADER which is a  lexicon and rule-based sentiment analysis tool provided by nltk package. It does also provide the degree of positiveness or negativess although we don't use it in this case.\n",
    "\n",
    "Reference - \n",
    "\n",
    "4. Song, S., & Zhao, J. (2013). Automated essay scoring using machine learning. Stanford University."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af685a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_tags(essay):\n",
    "    negative, positive, neutral = 0, 0, 0\n",
    "    \n",
    "    ss = SentimentIntensityAnalyzer().polarity_scores(essay)\n",
    "    for k in sorted(ss):\n",
    "        if k == 'compound':\n",
    "            pass\n",
    "        elif k == 'neg':\n",
    "            negative += ss[k]\n",
    "        elif k == 'pos':\n",
    "            positive += ss[k]\n",
    "        elif k == 'neu':\n",
    "            neutral += ss[k]\n",
    "            \n",
    "    return negative, positive, neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941dd39b",
   "metadata": {},
   "source": [
    "### Term-Frequence & Inverse Document Frequency\n",
    "\n",
    "Term frequency is the number of times a word appears in the document. Document frequency is number of times a word occurs in all the documents. TF multiplied by the inverse of DF gives TF-IDF scores. The vectors obtained help us get the corresspondence between essays. \n",
    "\n",
    "Reference -\n",
    "\n",
    "2. Suresh, A., & Jha, M. (2018). Automated essay grading using natural language processing and support vector machine. International Journal of Computing and Technology, 5(2), 18-21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0c4d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_vectors(essays):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    words = []\n",
    "    for essay in essays:\n",
    "        essay = re.sub(r'\\W', ' ', essay)\n",
    "        words.append(nltk.word_tokenize(essay))\n",
    "        \n",
    "    docs_lemmatized = [[WordNetLemmatizer().lemmatize(j) for j in i]for i in words]\n",
    "    \n",
    "    corpus = [' '.join(i) for i in docs_lemmatized]\n",
    "    vectors = vectorizer.fit_transform(corpus)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    return feature_names, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42c4a478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of essays X number of features (1569, 9138)\n"
     ]
    }
   ],
   "source": [
    "feature_names,vectors_all = get_tfidf_vectors(data['essay'])\n",
    "print(\"num of essays X number of features\",vectors_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b69558",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dcdab5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>15</th>\n",
       "      <th>190</th>\n",
       "      <th>21</th>\n",
       "      <th>27</th>\n",
       "      <th>30</th>\n",
       "      <th>30am</th>\n",
       "      <th>30pm</th>\n",
       "      <th>35</th>\n",
       "      <th>...</th>\n",
       "      <th>œsix</th>\n",
       "      <th>œthe</th>\n",
       "      <th>œthen</th>\n",
       "      <th>œwas</th>\n",
       "      <th>œwellâ</th>\n",
       "      <th>œwhat</th>\n",
       "      <th>œwhatâ</th>\n",
       "      <th>œwhy</th>\n",
       "      <th>œyou</th>\n",
       "      <th>œyouâ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001041</td>\n",
       "      <td>-0.000316</td>\n",
       "      <td>-0.000378</td>\n",
       "      <td>-0.000644</td>\n",
       "      <td>-0.000627</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.000619</td>\n",
       "      <td>-0.000190</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000365</td>\n",
       "      <td>-0.000365</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>-0.000095</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>-0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002137</td>\n",
       "      <td>-0.000618</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>-0.000488</td>\n",
       "      <td>-0.000634</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.000892</td>\n",
       "      <td>-0.000798</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.000787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.001304</td>\n",
       "      <td>-0.000886</td>\n",
       "      <td>-0.000349</td>\n",
       "      <td>-0.001009</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>-0.000210</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>-0.000550</td>\n",
       "      <td>-0.000144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.000222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.002155</td>\n",
       "      <td>-0.000991</td>\n",
       "      <td>-0.000763</td>\n",
       "      <td>-0.000685</td>\n",
       "      <td>-0.000519</td>\n",
       "      <td>-0.000841</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>-0.000221</td>\n",
       "      <td>-0.000459</td>\n",
       "      <td>-0.000334</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000456</td>\n",
       "      <td>-0.000456</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000133</td>\n",
       "      <td>0.000094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>-0.002234</td>\n",
       "      <td>0.018293</td>\n",
       "      <td>0.005340</td>\n",
       "      <td>-0.003878</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.012321</td>\n",
       "      <td>-0.005822</td>\n",
       "      <td>-0.007661</td>\n",
       "      <td>-0.016320</td>\n",
       "      <td>0.005765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000808</td>\n",
       "      <td>-0.000495</td>\n",
       "      <td>-0.000495</td>\n",
       "      <td>-0.000495</td>\n",
       "      <td>-0.000495</td>\n",
       "      <td>-0.000495</td>\n",
       "      <td>-0.000808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>0.004450</td>\n",
       "      <td>-0.010192</td>\n",
       "      <td>-0.002901</td>\n",
       "      <td>-0.009475</td>\n",
       "      <td>-0.009178</td>\n",
       "      <td>0.007652</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>-0.004794</td>\n",
       "      <td>-0.006527</td>\n",
       "      <td>-0.001624</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>0.002626</td>\n",
       "      <td>-0.001118</td>\n",
       "      <td>-0.001118</td>\n",
       "      <td>-0.001118</td>\n",
       "      <td>-0.001118</td>\n",
       "      <td>-0.001118</td>\n",
       "      <td>0.002626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>-0.001575</td>\n",
       "      <td>-0.034740</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>-0.000368</td>\n",
       "      <td>-0.000731</td>\n",
       "      <td>-0.005036</td>\n",
       "      <td>0.002357</td>\n",
       "      <td>-0.005813</td>\n",
       "      <td>0.012520</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>-0.003287</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>-0.006879</td>\n",
       "      <td>-0.006879</td>\n",
       "      <td>-0.006879</td>\n",
       "      <td>-0.006879</td>\n",
       "      <td>-0.006879</td>\n",
       "      <td>0.000641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>0.054281</td>\n",
       "      <td>-0.097997</td>\n",
       "      <td>-0.006105</td>\n",
       "      <td>-0.005157</td>\n",
       "      <td>-0.000963</td>\n",
       "      <td>0.004370</td>\n",
       "      <td>-0.001572</td>\n",
       "      <td>-0.002846</td>\n",
       "      <td>-0.001660</td>\n",
       "      <td>-0.006483</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008133</td>\n",
       "      <td>-0.008133</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>-0.001723</td>\n",
       "      <td>-0.001723</td>\n",
       "      <td>-0.001723</td>\n",
       "      <td>-0.001723</td>\n",
       "      <td>-0.001723</td>\n",
       "      <td>0.000604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>-0.005133</td>\n",
       "      <td>-0.001552</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>-0.000977</td>\n",
       "      <td>-0.006539</td>\n",
       "      <td>-0.011341</td>\n",
       "      <td>0.006198</td>\n",
       "      <td>-0.002337</td>\n",
       "      <td>0.008996</td>\n",
       "      <td>0.003927</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005385</td>\n",
       "      <td>-0.005385</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>-0.001608</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>-0.001608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1569 rows × 9138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            00       000        15       190        21        27        30  \\\n",
       "0     0.001022  0.001117  0.000238  0.000398  0.000538  0.000458  0.000364   \n",
       "1    -0.001041 -0.000316 -0.000378 -0.000644 -0.000627  0.000821 -0.000094   \n",
       "2     0.002137 -0.000618 -0.000045 -0.000488 -0.000634 -0.000058 -0.000892   \n",
       "3    -0.001304 -0.000886 -0.000349 -0.001009  0.001308 -0.000210  0.002147   \n",
       "4    -0.002155 -0.000991 -0.000763 -0.000685 -0.000519 -0.000841  0.000893   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1564 -0.002234  0.018293  0.005340 -0.003878  0.001401  0.012321 -0.005822   \n",
       "1565  0.004450 -0.010192 -0.002901 -0.009475 -0.009178  0.007652  0.003318   \n",
       "1566 -0.001575 -0.034740  0.000245 -0.000368 -0.000731 -0.005036  0.002357   \n",
       "1567  0.054281 -0.097997 -0.006105 -0.005157 -0.000963  0.004370 -0.001572   \n",
       "1568 -0.005133 -0.001552  0.000922 -0.000977 -0.006539 -0.011341  0.006198   \n",
       "\n",
       "          30am      30pm        35  ...      œsix      œthe     œthen  \\\n",
       "0     0.000550  0.000605  0.000268  ...  0.000272  0.000272  0.000091   \n",
       "1    -0.000719 -0.000619 -0.000190  ... -0.000365 -0.000365  0.000189   \n",
       "2    -0.000798  0.001315  0.001227  ...  0.000696  0.000696  0.000969   \n",
       "3     0.000143 -0.000550 -0.000144  ...  0.000627  0.000627  0.000299   \n",
       "4    -0.000221 -0.000459 -0.000334  ... -0.000456 -0.000456  0.000643   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1564 -0.007661 -0.016320  0.005765  ...  0.002224  0.002224 -0.000048   \n",
       "1565 -0.004794 -0.006527 -0.001624  ...  0.003654  0.003654  0.001742   \n",
       "1566 -0.005813  0.012520  0.001081  ...  0.001796  0.001796 -0.003287   \n",
       "1567 -0.002846 -0.001660 -0.006483  ... -0.008133 -0.008133  0.000648   \n",
       "1568 -0.002337  0.008996  0.003927  ... -0.005385 -0.005385  0.005041   \n",
       "\n",
       "          œwas    œwellâ     œwhat    œwhatâ      œwhy      œyou     œyouâ  \n",
       "0     0.000081  0.000231  0.000231  0.000231  0.000231  0.000231  0.000081  \n",
       "1    -0.000095 -0.000219 -0.000219 -0.000219 -0.000219 -0.000219 -0.000095  \n",
       "2     0.000787  0.001675  0.001675  0.001675  0.001675  0.001675  0.000787  \n",
       "3     0.000222  0.000104  0.000104  0.000104  0.000104  0.000104  0.000222  \n",
       "4     0.000094  0.000133  0.000133  0.000133  0.000133  0.000133  0.000094  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1564 -0.000808 -0.000495 -0.000495 -0.000495 -0.000495 -0.000495 -0.000808  \n",
       "1565  0.002626 -0.001118 -0.001118 -0.001118 -0.001118 -0.001118  0.002626  \n",
       "1566  0.000641 -0.006879 -0.006879 -0.006879 -0.006879 -0.006879  0.000641  \n",
       "1567  0.000604 -0.001723 -0.001723 -0.001723 -0.001723 -0.001723  0.000604  \n",
       "1568 -0.001608  0.000398  0.000398  0.000398  0.000398  0.000398 -0.001608  \n",
       "\n",
       "[1569 rows x 9138 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVD represent documents and terms in vectors \n",
    "reduced_dim = vectors_all.shape[0]\n",
    "svd_model = TruncatedSVD(n_components=reduced_dim, algorithm='randomized', random_state=122)\n",
    "lsa = svd_model.fit_transform(vectors_all)\n",
    "\n",
    "pd.DataFrame(svd_model.components_,index=range(reduced_dim), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3762f8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1559</th>\n",
       "      <th>1560</th>\n",
       "      <th>1561</th>\n",
       "      <th>1562</th>\n",
       "      <th>1563</th>\n",
       "      <th>1564</th>\n",
       "      <th>1565</th>\n",
       "      <th>1566</th>\n",
       "      <th>1567</th>\n",
       "      <th>1568</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.071469</td>\n",
       "      <td>0.015369</td>\n",
       "      <td>0.193639</td>\n",
       "      <td>0.051428</td>\n",
       "      <td>0.029865</td>\n",
       "      <td>0.077420</td>\n",
       "      <td>0.013215</td>\n",
       "      <td>0.388682</td>\n",
       "      <td>0.213989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077083</td>\n",
       "      <td>0.004276</td>\n",
       "      <td>0.072728</td>\n",
       "      <td>0.076294</td>\n",
       "      <td>0.131251</td>\n",
       "      <td>0.021802</td>\n",
       "      <td>0.053630</td>\n",
       "      <td>0.037008</td>\n",
       "      <td>0.072476</td>\n",
       "      <td>0.046536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.071469</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026020</td>\n",
       "      <td>0.045808</td>\n",
       "      <td>0.128025</td>\n",
       "      <td>0.015626</td>\n",
       "      <td>0.047426</td>\n",
       "      <td>0.019096</td>\n",
       "      <td>0.094359</td>\n",
       "      <td>0.028794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023090</td>\n",
       "      <td>0.037797</td>\n",
       "      <td>0.039388</td>\n",
       "      <td>0.060182</td>\n",
       "      <td>0.080888</td>\n",
       "      <td>0.034003</td>\n",
       "      <td>0.012451</td>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.033281</td>\n",
       "      <td>0.007367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015369</td>\n",
       "      <td>0.026020</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.063186</td>\n",
       "      <td>0.039994</td>\n",
       "      <td>0.040058</td>\n",
       "      <td>0.014183</td>\n",
       "      <td>0.028007</td>\n",
       "      <td>0.048132</td>\n",
       "      <td>0.020856</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024036</td>\n",
       "      <td>0.031381</td>\n",
       "      <td>0.061835</td>\n",
       "      <td>0.071120</td>\n",
       "      <td>0.070754</td>\n",
       "      <td>0.067503</td>\n",
       "      <td>0.039460</td>\n",
       "      <td>0.025963</td>\n",
       "      <td>0.027108</td>\n",
       "      <td>0.009317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.193639</td>\n",
       "      <td>0.045808</td>\n",
       "      <td>0.063186</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061093</td>\n",
       "      <td>0.076080</td>\n",
       "      <td>0.086670</td>\n",
       "      <td>0.043097</td>\n",
       "      <td>0.069822</td>\n",
       "      <td>0.117171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056199</td>\n",
       "      <td>0.053734</td>\n",
       "      <td>0.106519</td>\n",
       "      <td>0.107885</td>\n",
       "      <td>0.085123</td>\n",
       "      <td>0.077114</td>\n",
       "      <td>0.036716</td>\n",
       "      <td>0.044122</td>\n",
       "      <td>0.079889</td>\n",
       "      <td>0.032604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051428</td>\n",
       "      <td>0.128025</td>\n",
       "      <td>0.039994</td>\n",
       "      <td>0.061093</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018170</td>\n",
       "      <td>0.018797</td>\n",
       "      <td>0.018461</td>\n",
       "      <td>0.027674</td>\n",
       "      <td>0.061834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010182</td>\n",
       "      <td>0.069484</td>\n",
       "      <td>0.032971</td>\n",
       "      <td>0.056238</td>\n",
       "      <td>0.055449</td>\n",
       "      <td>0.033205</td>\n",
       "      <td>0.041778</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.038159</td>\n",
       "      <td>0.011424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>0.021802</td>\n",
       "      <td>0.034003</td>\n",
       "      <td>0.067503</td>\n",
       "      <td>0.077114</td>\n",
       "      <td>0.033205</td>\n",
       "      <td>0.087350</td>\n",
       "      <td>0.017830</td>\n",
       "      <td>0.130354</td>\n",
       "      <td>0.106623</td>\n",
       "      <td>0.046599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061299</td>\n",
       "      <td>0.029110</td>\n",
       "      <td>0.048785</td>\n",
       "      <td>0.160802</td>\n",
       "      <td>0.108553</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025467</td>\n",
       "      <td>0.056428</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.034714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>0.053630</td>\n",
       "      <td>0.012451</td>\n",
       "      <td>0.039460</td>\n",
       "      <td>0.036716</td>\n",
       "      <td>0.041778</td>\n",
       "      <td>0.071719</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.022927</td>\n",
       "      <td>0.044425</td>\n",
       "      <td>0.038887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070641</td>\n",
       "      <td>0.059585</td>\n",
       "      <td>0.031530</td>\n",
       "      <td>0.060079</td>\n",
       "      <td>0.057630</td>\n",
       "      <td>0.025467</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.037601</td>\n",
       "      <td>0.054673</td>\n",
       "      <td>0.080925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>0.037008</td>\n",
       "      <td>0.029164</td>\n",
       "      <td>0.025963</td>\n",
       "      <td>0.044122</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.084145</td>\n",
       "      <td>0.020094</td>\n",
       "      <td>0.028937</td>\n",
       "      <td>0.062734</td>\n",
       "      <td>0.040779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045872</td>\n",
       "      <td>0.069468</td>\n",
       "      <td>0.032464</td>\n",
       "      <td>0.090366</td>\n",
       "      <td>0.139172</td>\n",
       "      <td>0.056428</td>\n",
       "      <td>0.037601</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041949</td>\n",
       "      <td>0.042769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>0.072476</td>\n",
       "      <td>0.033281</td>\n",
       "      <td>0.027108</td>\n",
       "      <td>0.079889</td>\n",
       "      <td>0.038159</td>\n",
       "      <td>0.092889</td>\n",
       "      <td>0.049453</td>\n",
       "      <td>0.030616</td>\n",
       "      <td>0.076587</td>\n",
       "      <td>0.043874</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065519</td>\n",
       "      <td>0.046898</td>\n",
       "      <td>0.041646</td>\n",
       "      <td>0.129504</td>\n",
       "      <td>0.101350</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.054673</td>\n",
       "      <td>0.041949</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.028050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>0.046536</td>\n",
       "      <td>0.007367</td>\n",
       "      <td>0.009317</td>\n",
       "      <td>0.032604</td>\n",
       "      <td>0.011424</td>\n",
       "      <td>0.098011</td>\n",
       "      <td>0.008875</td>\n",
       "      <td>0.022959</td>\n",
       "      <td>0.025178</td>\n",
       "      <td>0.051681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.015282</td>\n",
       "      <td>0.061942</td>\n",
       "      <td>0.013088</td>\n",
       "      <td>0.034714</td>\n",
       "      <td>0.080925</td>\n",
       "      <td>0.042769</td>\n",
       "      <td>0.028050</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1569 rows × 1569 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     1.000000  0.071469  0.015369  0.193639  0.051428  0.029865  0.077420   \n",
       "1     0.071469  1.000000  0.026020  0.045808  0.128025  0.015626  0.047426   \n",
       "2     0.015369  0.026020  1.000000  0.063186  0.039994  0.040058  0.014183   \n",
       "3     0.193639  0.045808  0.063186  1.000000  0.061093  0.076080  0.086670   \n",
       "4     0.051428  0.128025  0.039994  0.061093  1.000000  0.018170  0.018797   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1564  0.021802  0.034003  0.067503  0.077114  0.033205  0.087350  0.017830   \n",
       "1565  0.053630  0.012451  0.039460  0.036716  0.041778  0.071719  0.024900   \n",
       "1566  0.037008  0.029164  0.025963  0.044122  0.021484  0.084145  0.020094   \n",
       "1567  0.072476  0.033281  0.027108  0.079889  0.038159  0.092889  0.049453   \n",
       "1568  0.046536  0.007367  0.009317  0.032604  0.011424  0.098011  0.008875   \n",
       "\n",
       "          7         8         9     ...      1559      1560      1561  \\\n",
       "0     0.013215  0.388682  0.213989  ...  0.077083  0.004276  0.072728   \n",
       "1     0.019096  0.094359  0.028794  ...  0.023090  0.037797  0.039388   \n",
       "2     0.028007  0.048132  0.020856  ...  0.024036  0.031381  0.061835   \n",
       "3     0.043097  0.069822  0.117171  ...  0.056199  0.053734  0.106519   \n",
       "4     0.018461  0.027674  0.061834  ...  0.010182  0.069484  0.032971   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1564  0.130354  0.106623  0.046599  ...  0.061299  0.029110  0.048785   \n",
       "1565  0.022927  0.044425  0.038887  ...  0.070641  0.059585  0.031530   \n",
       "1566  0.028937  0.062734  0.040779  ...  0.045872  0.069468  0.032464   \n",
       "1567  0.030616  0.076587  0.043874  ...  0.065519  0.046898  0.041646   \n",
       "1568  0.022959  0.025178  0.051681  ...  0.011500  0.000963  0.015282   \n",
       "\n",
       "          1562      1563      1564      1565      1566      1567      1568  \n",
       "0     0.076294  0.131251  0.021802  0.053630  0.037008  0.072476  0.046536  \n",
       "1     0.060182  0.080888  0.034003  0.012451  0.029164  0.033281  0.007367  \n",
       "2     0.071120  0.070754  0.067503  0.039460  0.025963  0.027108  0.009317  \n",
       "3     0.107885  0.085123  0.077114  0.036716  0.044122  0.079889  0.032604  \n",
       "4     0.056238  0.055449  0.033205  0.041778  0.021484  0.038159  0.011424  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1564  0.160802  0.108553  1.000000  0.025467  0.056428  0.050847  0.034714  \n",
       "1565  0.060079  0.057630  0.025467  1.000000  0.037601  0.054673  0.080925  \n",
       "1566  0.090366  0.139172  0.056428  0.037601  1.000000  0.041949  0.042769  \n",
       "1567  0.129504  0.101350  0.050847  0.054673  0.041949  1.000000  0.028050  \n",
       "1568  0.061942  0.013088  0.034714  0.080925  0.042769  0.028050  1.000000  \n",
       "\n",
       "[1569 rows x 1569 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute document similarity using LSA components\n",
    "lsa_similarity = np.asarray(np.asmatrix(lsa) * np.asmatrix(lsa).T)\n",
    "pd.DataFrame(lsa_similarity,index=range(num_essays), columns=range(num_essays))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a6833",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "Using the vectors we got we measure how similar they are using cosine distance between vector pairs. \n",
    "\n",
    "This feature has not been taken from any of the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e572490",
   "metadata": {},
   "outputs": [],
   "source": [
    "highest = max(data['domain1_score'].tolist())\n",
    "\n",
    "def get_cosine_similarity(essay_id):\n",
    "    index_high = data.index[data['domain1_score'] == highest].tolist()\n",
    "    n = len(index_high)\n",
    "\n",
    "    j = data.index[data['essay_id'] == essay_id]\n",
    "    similarity = 0\n",
    "    for i in index_high:\n",
    "        similarity += cosine_similarity(vectors_all[i,:],vectors_all[j,:])\n",
    "    similarity /= n\n",
    "    \n",
    "    return np.asscalar(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3cc8f1",
   "metadata": {},
   "source": [
    "### Extraction\n",
    "\n",
    "Finally we apply all the above functions to our data set and store the results in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a1cc978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "    \n",
    "    features = data.copy()\n",
    "    \n",
    "    features['word_count'] = features['essay'].apply(get_word_count)\n",
    "    print(\"Added 'word_count' feature successfully.\")\n",
    "    \n",
    "    features['sent_count'] = features['essay'].apply(get_sentence_count)\n",
    "    print(\"Added 'sent_count' feature successfully.\")\n",
    "    \n",
    "    features['avg_word_len'] = features['essay'].apply(get_word_length_average)\n",
    "    print(\"Added 'avg_word_len' feature successfully.\")\n",
    "    \n",
    "    features['lemma_count'] = features['essay'].apply(get_lemma_count)\n",
    "    print(\"Added 'lemma_count' feature successfully.\")\n",
    "    \n",
    "    features['spell_err_count'] = features['essay'].apply(get_spell_error_count)\n",
    "    print(\"Added 'spell_err_count' feature successfully.\")\n",
    "    \n",
    "    features['noun_count'], features['adj_count'], features['verb_count'], features['adv_count'] = zip(*features['essay'].map(get_pos_counts))\n",
    "    print(\"Added 'noun_count', 'adj_count', 'verb_count' and 'adv_count' features successfully.\")\n",
    "    \n",
    "    features['neg_score'], features['pos_score'], features['neu_score'] = zip(*features['essay'].map(get_sentiment_tags))\n",
    "    print(\"Added 'neg_score', 'pos_score' and 'neu_score' features successfully.\")\n",
    "    \n",
    "    features['cosine_similarity'] = features['essay_id'].apply(get_cosine_similarity)\n",
    "    print(\"Added 'similarity' feature successfully.\")\n",
    "        \n",
    "    # TODO: LSA \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e458b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'word_count' feature successfully.\n",
      "Added 'sent_count' feature successfully.\n",
      "Added 'avg_word_len' feature successfully.\n",
      "Added 'lemma_count' feature successfully.\n",
      "Added 'spell_err_count' feature successfully.\n",
      "Added 'noun_count', 'adj_count', 'verb_count' and 'adv_count' features successfully.\n",
      "Added 'neg_score', 'pos_score' and 'neu_score' features successfully.\n",
      "Added 'similarity' feature successfully.\n"
     ]
    }
   ],
   "source": [
    "features_set1 = extract_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5a2542f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>lemma_count</th>\n",
       "      <th>spell_err_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neu_score</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17834</td>\n",
       "      <td>Patience is when your waiting .I was patience ...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>94</td>\n",
       "      <td>3</td>\n",
       "      <td>4.063830</td>\n",
       "      <td>60</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.053191</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.055627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17836</td>\n",
       "      <td>I am not a patience person, like I can’t sit i...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>96</td>\n",
       "      <td>3</td>\n",
       "      <td>3.708333</td>\n",
       "      <td>59</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.072917</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.873</td>\n",
       "      <td>0.045038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17837</td>\n",
       "      <td>One day I was at basketball practice and I was...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "      <td>3.814103</td>\n",
       "      <td>83</td>\n",
       "      <td>0.057692</td>\n",
       "      <td>0.217949</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.049515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17838</td>\n",
       "      <td>I going to write about a time when I went to t...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>224</td>\n",
       "      <td>13</td>\n",
       "      <td>3.830357</td>\n",
       "      <td>118</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.205357</td>\n",
       "      <td>0.040179</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.081108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17839</td>\n",
       "      <td>It can be very hard for somebody to be patient...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>155</td>\n",
       "      <td>12</td>\n",
       "      <td>4.083871</td>\n",
       "      <td>73</td>\n",
       "      <td>0.012903</td>\n",
       "      <td>0.148387</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.238710</td>\n",
       "      <td>0.109677</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.048745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>19558</td>\n",
       "      <td>One time I was getting a cool @CAPS1 game it w...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>73</td>\n",
       "      <td>6</td>\n",
       "      <td>3.273973</td>\n",
       "      <td>51</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.260274</td>\n",
       "      <td>0.082192</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.068362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>19559</td>\n",
       "      <td>A patent person in my life is my mom. Aicason ...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>223</td>\n",
       "      <td>9</td>\n",
       "      <td>4.188341</td>\n",
       "      <td>121</td>\n",
       "      <td>0.035874</td>\n",
       "      <td>0.237668</td>\n",
       "      <td>0.058296</td>\n",
       "      <td>0.183857</td>\n",
       "      <td>0.085202</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.039163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>19561</td>\n",
       "      <td>A time when someone else I know was patient wa...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>172</td>\n",
       "      <td>13</td>\n",
       "      <td>3.924419</td>\n",
       "      <td>94</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.215116</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.087209</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.053159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>19562</td>\n",
       "      <td>I hate weddings. I love when people get marrie...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>3.896667</td>\n",
       "      <td>152</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.256667</td>\n",
       "      <td>0.063333</td>\n",
       "      <td>0.216667</td>\n",
       "      <td>0.096667</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.081143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1568</th>\n",
       "      <td>19563</td>\n",
       "      <td>A few weeks ago, we had a garage sale and a mo...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>167</td>\n",
       "      <td>9</td>\n",
       "      <td>3.341317</td>\n",
       "      <td>71</td>\n",
       "      <td>0.005988</td>\n",
       "      <td>0.203593</td>\n",
       "      <td>0.029940</td>\n",
       "      <td>0.221557</td>\n",
       "      <td>0.041916</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.023739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1569 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id                                              essay  \\\n",
       "0        17834  Patience is when your waiting .I was patience ...   \n",
       "1        17836  I am not a patience person, like I can’t sit i...   \n",
       "2        17837  One day I was at basketball practice and I was...   \n",
       "3        17838  I going to write about a time when I went to t...   \n",
       "4        17839  It can be very hard for somebody to be patient...   \n",
       "...        ...                                                ...   \n",
       "1564     19558  One time I was getting a cool @CAPS1 game it w...   \n",
       "1565     19559  A patent person in my life is my mom. Aicason ...   \n",
       "1566     19561  A time when someone else I know was patient wa...   \n",
       "1567     19562  I hate weddings. I love when people get marrie...   \n",
       "1568     19563  A few weeks ago, we had a garage sale and a mo...   \n",
       "\n",
       "      domain1_score  word_count  sent_count  avg_word_len  lemma_count  \\\n",
       "0              15.0          94           3      4.063830           60   \n",
       "1              13.0          96           3      3.708333           59   \n",
       "2              15.0         156           1      3.814103           83   \n",
       "3              17.0         224          13      3.830357          118   \n",
       "4              13.0         155          12      4.083871           73   \n",
       "...             ...         ...         ...           ...          ...   \n",
       "1564           12.0          73           6      3.273973           51   \n",
       "1565           16.0         223           9      4.188341          121   \n",
       "1566           19.0         172          13      3.924419           94   \n",
       "1567           22.0         300          30      3.896667          152   \n",
       "1568           15.0         167           9      3.341317           71   \n",
       "\n",
       "      spell_err_count  noun_count  adj_count  verb_count  adv_count  \\\n",
       "0            0.021277    0.255319   0.021277    0.276596   0.053191   \n",
       "1            0.093750    0.250000   0.041667    0.187500   0.072917   \n",
       "2            0.057692    0.217949   0.051282    0.250000   0.038462   \n",
       "3            0.017857    0.205357   0.040179    0.250000   0.062500   \n",
       "4            0.012903    0.148387   0.064516    0.238710   0.109677   \n",
       "...               ...         ...        ...         ...        ...   \n",
       "1564         0.095890    0.260274   0.082192    0.232877   0.000000   \n",
       "1565         0.035874    0.237668   0.058296    0.183857   0.085202   \n",
       "1566         0.034884    0.215116   0.034884    0.209302   0.087209   \n",
       "1567         0.110000    0.256667   0.063333    0.216667   0.096667   \n",
       "1568         0.005988    0.203593   0.029940    0.221557   0.041916   \n",
       "\n",
       "      neg_score  pos_score  neu_score  cosine_similarity  \n",
       "0         0.061      0.000      0.939           0.055627  \n",
       "1         0.127      0.000      0.873           0.045038  \n",
       "2         0.015      0.031      0.955           0.049515  \n",
       "3         0.047      0.055      0.898           0.081108  \n",
       "4         0.056      0.045      0.899           0.048745  \n",
       "...         ...        ...        ...                ...  \n",
       "1564      0.042      0.063      0.895           0.068362  \n",
       "1565      0.046      0.108      0.846           0.039163  \n",
       "1566      0.010      0.080      0.910           0.053159  \n",
       "1567      0.093      0.085      0.822           0.081143  \n",
       "1568      0.000      0.149      0.851           0.023739  \n",
       "\n",
       "[1569 rows x 16 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_set1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "195bf786",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'features_set_' + str(essay_set_num) + '.csv'\n",
    "features_set1.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918fc518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
