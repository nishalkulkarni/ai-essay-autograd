{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1acaeb2c",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "In this notebook we perform data sanitization and extraction, the data is taken from kaggle consisting of more than 12000 essays.\n",
    "\n",
    "As part of extracting features we use ideas from multiple research papers referenced below.\n",
    "\n",
    "### Novelty\n",
    "In addition to already proposed ideas we also use **Latent Semantic Indexing** for extracting concepts and getting similarity between essays from the given text essays.\n",
    "\n",
    "\n",
    "#### REFRENCES:\n",
    "1. Mahana, M., Johns, M., & Apte, A. (2012). Automated essay grading using machine learning. Mach. Learn. Session, Stanford University.\n",
    "\n",
    "2. Suresh, A., & Jha, M. (2018). Automated essay grading using natural language processing and support vector machine. International Journal of Computing and Technology, 5(2), 18-21.\n",
    "\n",
    "3. Rokade, A., Patil, B., Rajani, S., Revandkar, S., & Shedge, R. (2018, April). Automated Grading System Using Natural Language Processing. In 2018 Second International Conference on Inventive Communication and Computational Technologies (ICICCT) (pp. 1123-1127). IEEE.\n",
    "\n",
    "4. Song, S., & Zhao, J. (2013). Automated essay scoring using machine learning. Stanford University.\n",
    "\n",
    "5. Kakkonen, T., Myller, N., & Sutinen, E. (2006). Applying Part-of-Seech Enhanced LSA to Automatic Essay Grading. arXiv preprint cs/0610118."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ede996d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import enchant\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d401a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/nishal/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/nishal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/nishal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nishal/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb93d2",
   "metadata": {},
   "source": [
    "### Loading data set\n",
    "\n",
    "8 essay set, totalling 12977 unique essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85a99784",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('training_set_rel3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5948d9da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12973</th>\n",
       "      <td>21626</td>\n",
       "      <td>8</td>\n",
       "      <td>In most stories mothers and daughters are eit...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12974</th>\n",
       "      <td>21628</td>\n",
       "      <td>8</td>\n",
       "      <td>I never understood the meaning laughter is th...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12975</th>\n",
       "      <td>21629</td>\n",
       "      <td>8</td>\n",
       "      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12976</th>\n",
       "      <td>21630</td>\n",
       "      <td>8</td>\n",
       "      <td>Trippin' on fen...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12977</th>\n",
       "      <td>21633</td>\n",
       "      <td>8</td>\n",
       "      <td>Many people believe that laughter can improve...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12978 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set                                              essay  \\\n",
       "0             1          1  Dear local newspaper, I think effects computer...   \n",
       "1             2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2             3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3             4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4             5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "...         ...        ...                                                ...   \n",
       "12973     21626          8   In most stories mothers and daughters are eit...   \n",
       "12974     21628          8   I never understood the meaning laughter is th...   \n",
       "12975     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n",
       "12976     21630          8                                 Trippin' on fen...   \n",
       "12977     21633          8   Many people believe that laughter can improve...   \n",
       "\n",
       "       rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0                 4.0             4.0             NaN            8.0   \n",
       "1                 5.0             4.0             NaN            9.0   \n",
       "2                 4.0             3.0             NaN            7.0   \n",
       "3                 5.0             5.0             NaN           10.0   \n",
       "4                 4.0             4.0             NaN            8.0   \n",
       "...               ...             ...             ...            ...   \n",
       "12973            17.0            18.0             NaN           35.0   \n",
       "12974            15.0            17.0             NaN           32.0   \n",
       "12975            20.0            26.0            40.0           40.0   \n",
       "12976            20.0            20.0             NaN           40.0   \n",
       "12977            20.0            20.0             NaN           40.0   \n",
       "\n",
       "       rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n",
       "0                 NaN             NaN            NaN  ...            NaN   \n",
       "1                 NaN             NaN            NaN  ...            NaN   \n",
       "2                 NaN             NaN            NaN  ...            NaN   \n",
       "3                 NaN             NaN            NaN  ...            NaN   \n",
       "4                 NaN             NaN            NaN  ...            NaN   \n",
       "...               ...             ...            ...  ...            ...   \n",
       "12973             NaN             NaN            NaN  ...            4.0   \n",
       "12974             NaN             NaN            NaN  ...            4.0   \n",
       "12975             NaN             NaN            NaN  ...            5.0   \n",
       "12976             NaN             NaN            NaN  ...            4.0   \n",
       "12977             NaN             NaN            NaN  ...            4.0   \n",
       "\n",
       "       rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  \\\n",
       "0                NaN            NaN            NaN            NaN   \n",
       "1                NaN            NaN            NaN            NaN   \n",
       "2                NaN            NaN            NaN            NaN   \n",
       "3                NaN            NaN            NaN            NaN   \n",
       "4                NaN            NaN            NaN            NaN   \n",
       "...              ...            ...            ...            ...   \n",
       "12973            4.0            4.0            3.0            NaN   \n",
       "12974            4.0            4.0            3.0            NaN   \n",
       "12975            5.0            5.0            5.0            4.0   \n",
       "12976            4.0            4.0            4.0            NaN   \n",
       "12977            4.0            4.0            4.0            NaN   \n",
       "\n",
       "       rater3_trait2  rater3_trait3  rater3_trait4  rater3_trait5  \\\n",
       "0                NaN            NaN            NaN            NaN   \n",
       "1                NaN            NaN            NaN            NaN   \n",
       "2                NaN            NaN            NaN            NaN   \n",
       "3                NaN            NaN            NaN            NaN   \n",
       "4                NaN            NaN            NaN            NaN   \n",
       "...              ...            ...            ...            ...   \n",
       "12973            NaN            NaN            NaN            NaN   \n",
       "12974            NaN            NaN            NaN            NaN   \n",
       "12975            4.0            4.0            4.0            4.0   \n",
       "12976            NaN            NaN            NaN            NaN   \n",
       "12977            NaN            NaN            NaN            NaN   \n",
       "\n",
       "       rater3_trait6  \n",
       "0                NaN  \n",
       "1                NaN  \n",
       "2                NaN  \n",
       "3                NaN  \n",
       "4                NaN  \n",
       "...              ...  \n",
       "12973            NaN  \n",
       "12974            NaN  \n",
       "12975            4.0  \n",
       "12976            NaN  \n",
       "12977            NaN  \n",
       "\n",
       "[12978 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867fda30",
   "metadata": {},
   "source": [
    "### Filtering data set\n",
    "\n",
    "We only use the actual essay along with the domain score for training, all of the other columns are discarded.\n",
    "\n",
    "We use the first 2000 essays for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "debdf4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.iloc[:, 1:2], inplace=True, axis=1)\n",
    "data.drop(data.iloc[:, 2:5], inplace=True, axis=1)\n",
    "data.drop(data.iloc[:, 3:], inplace=True, axis=1)\n",
    "\n",
    "num_essays = 2000\n",
    "data.drop(range(num_essays,12978), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b9a0907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>3190</td>\n",
       "      <td>I believe that they should not be pulled off o...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>3191</td>\n",
       "      <td>When have you ever went into a library and fou...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>3192</td>\n",
       "      <td>When I go to a library I @MONTH1 find some stu...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>3193</td>\n",
       "      <td>Certain people beleive that offensive books, m...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>3194</td>\n",
       "      <td>Katherine Paterson said ' If I have the right ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id                                              essay  \\\n",
       "0            1  Dear local newspaper, I think effects computer...   \n",
       "1            2  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2            3  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3            4  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4            5  Dear @LOCATION1, I know having computers has a...   \n",
       "...        ...                                                ...   \n",
       "1995      3190  I believe that they should not be pulled off o...   \n",
       "1996      3191  When have you ever went into a library and fou...   \n",
       "1997      3192  When I go to a library I @MONTH1 find some stu...   \n",
       "1998      3193  Certain people beleive that offensive books, m...   \n",
       "1999      3194  Katherine Paterson said ' If I have the right ...   \n",
       "\n",
       "      domain1_score  \n",
       "0               8.0  \n",
       "1               9.0  \n",
       "2               7.0  \n",
       "3              10.0  \n",
       "4               8.0  \n",
       "...             ...  \n",
       "1995            4.0  \n",
       "1996            4.0  \n",
       "1997            3.0  \n",
       "1998            3.0  \n",
       "1999            3.0  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fa6f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordlist(sentence):\n",
    "    # Remove non-alphanumeric characters\n",
    "    sentence = re.sub(\"[^a-zA-Z0-9]\",\" \", sentence)\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b2cd50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_sentences(essay):\n",
    "    sentences = nltk.sent_tokenize(essay.strip())\n",
    "    \n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > 0:\n",
    "            tokenized_sentences.append(get_wordlist(sentence))\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290b804",
   "metadata": {},
   "source": [
    "### Numerical features \n",
    "Features like the average length of words, the word count and the sentence count give us an idea about the fluency in language and dextirity of the writer.\n",
    "\n",
    "Reference - \n",
    "\n",
    "1. Mahana, M., Johns, M., & Apte, A. (2012). Automated essay grading using machine learning. Mach. Learn. Session, Stanford University."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e7a2701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_length_average(essay):\n",
    "    # Sanitize essay\n",
    "    essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(essay)\n",
    "    avg = sum(len(word) for word in words) / len(words)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02ab1218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count(essay):\n",
    "    essay = re.sub(r'\\W', ' ', essay)\n",
    "    count = len(nltk.word_tokenize(essay))\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10c8510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_count(essay):\n",
    "    sentences = nltk.sent_tokenize(essay)\n",
    "    count = len(sentences)\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d71861",
   "metadata": {},
   "source": [
    "### Lemmatization and Part of Speech tagging\n",
    "\n",
    "Lemmatization involves use of a vocabulary to perform a morphological analysis of words. Getting lemma count along with different part of speech count like that of nouns, adjectives, verbs, adverbs allows us to understand the lexical density and overall semantic difficulty of the essay.\n",
    "\n",
    "Reference - \n",
    "\n",
    "2. Suresh, A., & Jha, M. (2018). Automated essay grading using natural language processing and support vector machine. International Journal of Computing and Technology, 5(2), 18-21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "872ad2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_count(essay):\n",
    "    tokenized_sentences = get_tokenized_sentences(essay)      \n",
    "    \n",
    "    lemmas = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        pos_tagged_tokens = nltk.pos_tag(sentence) \n",
    "        for token_tuple in pos_tagged_tokens:\n",
    "            word = token_tuple[0]\n",
    "            pos_tag = token_tuple[1]\n",
    "            # assume default part of speech to be noun\n",
    "            pos = wordnet.NOUN\n",
    "            if pos_tag.startswith('J'):\n",
    "                pos = wordnet.ADJ\n",
    "            elif pos_tag.startswith('V'):\n",
    "                pos = wordnet.VERB\n",
    "            elif pos_tag.startswith('R'):\n",
    "                pos = wordnet.ADV\n",
    "                \n",
    "            lemmas.append(WordNetLemmatizer().lemmatize(word, pos))\n",
    "    \n",
    "    lemma_count = len(set(lemmas))\n",
    "    \n",
    "    return lemma_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aa494c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_counts(essay):\n",
    "    tokenized_sentences = get_tokenized_sentences(essay)\n",
    "    \n",
    "    nouns, adjectives, verbs, adverbs = 0, 0, 0, 0\n",
    "    \n",
    "    essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(essay)\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        pos_tagged_tokens = nltk.pos_tag(sentence)\n",
    "        for token_tuple in pos_tagged_tokens:\n",
    "            pos_tag = token_tuple[1]\n",
    "            if pos_tag.startswith('N'): \n",
    "                nouns += 1\n",
    "            elif pos_tag.startswith('J'):\n",
    "                adjectives += 1\n",
    "            elif pos_tag.startswith('V'):\n",
    "                verbs += 1\n",
    "            elif pos_tag.startswith('R'):\n",
    "                adverbs += 1\n",
    "    \n",
    "    return nouns/len(words), adjectives/len(words), verbs/len(words), adverbs/len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940da472",
   "metadata": {},
   "source": [
    "### Spell Check (Orthography)\n",
    "Correct word spelling indicates command over language and facility of use. To test for these characteristics we extracted the count of spelling errors per essay. We used PyEnchant spell checker along with the hunspell dictionary to obtain the count of misspelt words per essay.\n",
    "\n",
    "Reference - \n",
    "\n",
    "1. Mahana, M., Johns, M., & Apte, A. (2012). Automated essay grading using machine learning. Mach. Learn. Session, Stanford University."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6853e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spell_error_count(essay):   \n",
    "    essay = re.sub(r'\\W', ' ', essay)\n",
    "    words = nltk.word_tokenize(essay)\n",
    "    \n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    misspelt_count = 0\n",
    "    for word in words:\n",
    "        if(d.check(word) == False):\n",
    "            misspelt_count += 1\n",
    "    \n",
    "    total_words = get_word_count(essay)\n",
    "    error_prob = misspelt_count/total_words\n",
    "    \n",
    "    return error_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21efe81f",
   "metadata": {},
   "source": [
    "### Sentiment Analysis (Opinion mining)\n",
    "It allows us to gauge the emotive effectiveness of the essay. We use the VADER which is a  lexicon and rule-based sentiment analysis tool provided by nltk package. It does also provide the degree of positiveness or negativess although we don't use it in this case.\n",
    "\n",
    "Reference - \n",
    "\n",
    "4. Song, S., & Zhao, J. (2013). Automated essay scoring using machine learning. Stanford University."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af685a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_tags(essay):\n",
    "    negative, positive, neutral = 0, 0, 0\n",
    "    \n",
    "    ss = SentimentIntensityAnalyzer().polarity_scores(essay)\n",
    "    for k in sorted(ss):\n",
    "        if k == 'compound':\n",
    "            pass\n",
    "        elif k == 'neg':\n",
    "            negative += ss[k]\n",
    "        elif k == 'pos':\n",
    "            positive += ss[k]\n",
    "        elif k == 'neu':\n",
    "            neutral += ss[k]\n",
    "            \n",
    "    return negative, positive, neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941dd39b",
   "metadata": {},
   "source": [
    "### Term-Frequence & Inverse Document Frequency\n",
    "\n",
    "Term frequency is the number of times a word appears in the document. Document frequency is number of times a word occurs in all the documents. TF multiplied by the inverse of DF gives TF-IDF scores. The vectors obtained help us get the corresspondence between essays. \n",
    "\n",
    "Reference -\n",
    "\n",
    "2. Suresh, A., & Jha, M. (2018). Automated essay grading using natural language processing and support vector machine. International Journal of Computing and Technology, 5(2), 18-21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0c4d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_vectors(essays):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    \n",
    "    words = []\n",
    "    for essay in essays:\n",
    "        essay = re.sub(r'\\W', ' ', essay)\n",
    "        words.append(nltk.word_tokenize(essay))\n",
    "        \n",
    "    docs_lemmatized = [[WordNetLemmatizer().lemmatize(j) for j in i]for i in words]\n",
    "    \n",
    "    corpus = [' '.join(i) for i in docs_lemmatized]\n",
    "    vectors = vectorizer.fit_transform(corpus)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    return feature_names, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42c4a478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of essays X number of features (2000, 15378)\n"
     ]
    }
   ],
   "source": [
    "feature_names,vectors_all = get_tfidf_vectors(data['essay'])\n",
    "print(\"num of essays X number of features\",vectors_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b69558",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dcdab5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00pm</th>\n",
       "      <th>aa</th>\n",
       "      <th>aamous</th>\n",
       "      <th>aand</th>\n",
       "      <th>aare</th>\n",
       "      <th>abad</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>...</th>\n",
       "      <th>yup</th>\n",
       "      <th>zap</th>\n",
       "      <th>zero</th>\n",
       "      <th>zingbobway</th>\n",
       "      <th>zip</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>1.036299e-04</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.000350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.000017</td>\n",
       "      <td>-0.000173</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>-0.000218</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>-0.000092</td>\n",
       "      <td>-4.487398e-07</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>-0.000355</td>\n",
       "      <td>-0.000185</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>-0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000401</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000363</td>\n",
       "      <td>-0.000706</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000639</td>\n",
       "      <td>-1.363574e-04</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.001566</td>\n",
       "      <td>-0.000198</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>-0.000770</td>\n",
       "      <td>-0.000386</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.000181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.000220</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>2.614697e-04</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000688</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>-0.002359</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>-0.000734</td>\n",
       "      <td>-0.001660</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>-0.000514</td>\n",
       "      <td>-0.001230</td>\n",
       "      <td>-0.000176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000483</td>\n",
       "      <td>-0.001170</td>\n",
       "      <td>-0.000850</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>-2.421830e-04</td>\n",
       "      <td>-0.000740</td>\n",
       "      <td>-0.001198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000708</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>-0.000440</td>\n",
       "      <td>-0.001458</td>\n",
       "      <td>-0.000609</td>\n",
       "      <td>-0.003020</td>\n",
       "      <td>-0.000834</td>\n",
       "      <td>-0.000549</td>\n",
       "      <td>-0.003298</td>\n",
       "      <td>-0.001048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.003748</td>\n",
       "      <td>0.004954</td>\n",
       "      <td>0.007427</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>-0.009587</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>-0.003213</td>\n",
       "      <td>1.464198e-04</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.004019</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>0.002392</td>\n",
       "      <td>-0.001746</td>\n",
       "      <td>-0.002693</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>-0.003040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>-0.005518</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>-0.004113</td>\n",
       "      <td>-0.014792</td>\n",
       "      <td>-0.000835</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>-0.004457</td>\n",
       "      <td>-8.130669e-04</td>\n",
       "      <td>-0.004209</td>\n",
       "      <td>-0.004715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>-0.004622</td>\n",
       "      <td>0.003274</td>\n",
       "      <td>-0.004002</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>-0.000175</td>\n",
       "      <td>-0.001691</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>-0.006926</td>\n",
       "      <td>0.009475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>-0.007540</td>\n",
       "      <td>-0.000406</td>\n",
       "      <td>-0.000246</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>-0.000886</td>\n",
       "      <td>-7.077058e-04</td>\n",
       "      <td>0.002643</td>\n",
       "      <td>-0.003071</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002596</td>\n",
       "      <td>-0.000471</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>-0.001592</td>\n",
       "      <td>-0.003722</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>-0.002417</td>\n",
       "      <td>0.012219</td>\n",
       "      <td>-0.004841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>-0.004616</td>\n",
       "      <td>-0.012992</td>\n",
       "      <td>0.005516</td>\n",
       "      <td>-0.006076</td>\n",
       "      <td>-0.011491</td>\n",
       "      <td>-0.000809</td>\n",
       "      <td>0.004747</td>\n",
       "      <td>-1.410273e-03</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>-0.002323</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004501</td>\n",
       "      <td>-0.010876</td>\n",
       "      <td>-0.005693</td>\n",
       "      <td>-0.006089</td>\n",
       "      <td>0.005860</td>\n",
       "      <td>0.008448</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.005299</td>\n",
       "      <td>0.003088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>-0.002467</td>\n",
       "      <td>-0.006535</td>\n",
       "      <td>0.001849</td>\n",
       "      <td>0.006731</td>\n",
       "      <td>-0.001921</td>\n",
       "      <td>-0.003012</td>\n",
       "      <td>-0.004729</td>\n",
       "      <td>1.371697e-03</td>\n",
       "      <td>-0.001773</td>\n",
       "      <td>-0.003626</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004189</td>\n",
       "      <td>0.004750</td>\n",
       "      <td>-0.002009</td>\n",
       "      <td>-0.003450</td>\n",
       "      <td>-0.003731</td>\n",
       "      <td>-0.012943</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>-0.009482</td>\n",
       "      <td>-0.003324</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 15378 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            00       000      00pm        aa    aamous      aand      aare  \\\n",
       "0     0.000136  0.000324  0.000300  0.000202  0.000221  0.000053  0.000335   \n",
       "1    -0.000017 -0.000173  0.000004  0.000015 -0.000218  0.001072 -0.000092   \n",
       "2    -0.000401 -0.000022 -0.000363 -0.000706  0.001078 -0.000022 -0.000639   \n",
       "3    -0.000053 -0.000220 -0.000755 -0.000142  0.001396  0.000333 -0.000016   \n",
       "4    -0.000483 -0.001170 -0.000850  0.000362 -0.000025  0.000244  0.000113   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1995  0.003748  0.004954  0.007427  0.001501 -0.009587  0.004324 -0.003213   \n",
       "1996 -0.005518  0.000429 -0.004113 -0.014792 -0.000835  0.002408 -0.004457   \n",
       "1997 -0.007540 -0.000406 -0.000246  0.000815  0.002653  0.001489 -0.000886   \n",
       "1998 -0.004616 -0.012992  0.005516 -0.006076 -0.011491 -0.000809  0.004747   \n",
       "1999 -0.002467 -0.006535  0.001849  0.006731 -0.001921 -0.003012 -0.004729   \n",
       "\n",
       "              abad   abandon  abandoned  ...       yup       zap      zero  \\\n",
       "0     1.036299e-04  0.000142   0.000152  ...  0.000167  0.000406  0.001298   \n",
       "1    -4.487398e-07 -0.000067  -0.000032  ... -0.000018 -0.000030 -0.000355   \n",
       "2    -1.363574e-04 -0.000600   0.000704  ...  0.000089  0.001566 -0.000198   \n",
       "3     2.614697e-04  0.000320  -0.000077  ... -0.000688  0.001051 -0.002359   \n",
       "4    -2.421830e-04 -0.000740  -0.001198  ... -0.000708  0.000610 -0.000440   \n",
       "...            ...       ...        ...  ...       ...       ...       ...   \n",
       "1995  1.464198e-04  0.001533   0.004019  ... -0.000129  0.001986  0.005908   \n",
       "1996 -8.130669e-04 -0.004209  -0.004715  ...  0.002359 -0.004622  0.003274   \n",
       "1997 -7.077058e-04  0.002643  -0.003071  ... -0.002596 -0.000471  0.000359   \n",
       "1998 -1.410273e-03  0.001028  -0.002323  ...  0.004501 -0.010876 -0.005693   \n",
       "1999  1.371697e-03 -0.001773  -0.003626  ... -0.004189  0.004750 -0.002009   \n",
       "\n",
       "      zingbobway       zip    zombie      zone    zoning       zoo      zoom  \n",
       "0       0.000226  0.000130  0.000913  0.000091  0.000163  0.001020  0.000350  \n",
       "1      -0.000185 -0.000028  0.000020 -0.000081 -0.000049  0.000363 -0.000019  \n",
       "2       0.000663  0.000263 -0.000770 -0.000386 -0.000026  0.000954  0.000181  \n",
       "3       0.000774 -0.000734 -0.001660  0.000245 -0.000514 -0.001230 -0.000176  \n",
       "4      -0.001458 -0.000609 -0.003020 -0.000834 -0.000549 -0.003298 -0.001048  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "1995    0.002301  0.002396  0.002392 -0.001746 -0.002693  0.001786 -0.003040  \n",
       "1996   -0.004002  0.004485 -0.000175 -0.001691  0.000440 -0.006926  0.009475  \n",
       "1997    0.000770 -0.001592 -0.003722  0.001559 -0.002417  0.012219 -0.004841  \n",
       "1998   -0.006089  0.005860  0.008448  0.000202 -0.000009  0.005299  0.003088  \n",
       "1999   -0.003450 -0.003731 -0.012943  0.000665  0.000460 -0.009482 -0.003324  \n",
       "\n",
       "[2000 rows x 15378 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVD represent documents and terms in vectors \n",
    "reduced_dim = 2000\n",
    "svd_model = TruncatedSVD(n_components=reduced_dim, algorithm='randomized', random_state=122)\n",
    "lsa = svd_model.fit_transform(vectors_all)\n",
    "\n",
    "pd.DataFrame(svd_model.components_,index=range(reduced_dim), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3762f8cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.140902</td>\n",
       "      <td>0.171522</td>\n",
       "      <td>0.095338</td>\n",
       "      <td>0.138435</td>\n",
       "      <td>0.096193</td>\n",
       "      <td>0.145993</td>\n",
       "      <td>0.159541</td>\n",
       "      <td>0.161309</td>\n",
       "      <td>0.120384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144524</td>\n",
       "      <td>0.082640</td>\n",
       "      <td>0.082743</td>\n",
       "      <td>0.049397</td>\n",
       "      <td>0.127117</td>\n",
       "      <td>0.058964</td>\n",
       "      <td>0.093549</td>\n",
       "      <td>0.036924</td>\n",
       "      <td>0.079958</td>\n",
       "      <td>0.075325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.140902</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.193965</td>\n",
       "      <td>0.139094</td>\n",
       "      <td>0.156889</td>\n",
       "      <td>0.116773</td>\n",
       "      <td>0.172582</td>\n",
       "      <td>0.155856</td>\n",
       "      <td>0.134259</td>\n",
       "      <td>0.105740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069009</td>\n",
       "      <td>0.042250</td>\n",
       "      <td>0.017035</td>\n",
       "      <td>0.049591</td>\n",
       "      <td>0.021204</td>\n",
       "      <td>0.034626</td>\n",
       "      <td>0.055880</td>\n",
       "      <td>0.051850</td>\n",
       "      <td>0.025982</td>\n",
       "      <td>0.023517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.171522</td>\n",
       "      <td>0.193965</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.130630</td>\n",
       "      <td>0.145819</td>\n",
       "      <td>0.095854</td>\n",
       "      <td>0.174459</td>\n",
       "      <td>0.160876</td>\n",
       "      <td>0.119314</td>\n",
       "      <td>0.141472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042253</td>\n",
       "      <td>0.047994</td>\n",
       "      <td>0.064691</td>\n",
       "      <td>0.066445</td>\n",
       "      <td>0.043972</td>\n",
       "      <td>0.043620</td>\n",
       "      <td>0.082474</td>\n",
       "      <td>0.045107</td>\n",
       "      <td>0.099334</td>\n",
       "      <td>0.071228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.095338</td>\n",
       "      <td>0.139094</td>\n",
       "      <td>0.130630</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.144374</td>\n",
       "      <td>0.094235</td>\n",
       "      <td>0.111451</td>\n",
       "      <td>0.150861</td>\n",
       "      <td>0.095013</td>\n",
       "      <td>0.079430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028193</td>\n",
       "      <td>0.039631</td>\n",
       "      <td>0.045939</td>\n",
       "      <td>0.034298</td>\n",
       "      <td>0.055913</td>\n",
       "      <td>0.040684</td>\n",
       "      <td>0.048377</td>\n",
       "      <td>0.023817</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.044435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.138435</td>\n",
       "      <td>0.156889</td>\n",
       "      <td>0.145819</td>\n",
       "      <td>0.144374</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.107828</td>\n",
       "      <td>0.167058</td>\n",
       "      <td>0.187601</td>\n",
       "      <td>0.190546</td>\n",
       "      <td>0.144120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>0.069090</td>\n",
       "      <td>0.070003</td>\n",
       "      <td>0.035328</td>\n",
       "      <td>0.066820</td>\n",
       "      <td>0.042474</td>\n",
       "      <td>0.062947</td>\n",
       "      <td>0.032841</td>\n",
       "      <td>0.047874</td>\n",
       "      <td>0.060753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.058964</td>\n",
       "      <td>0.034626</td>\n",
       "      <td>0.043620</td>\n",
       "      <td>0.040684</td>\n",
       "      <td>0.042474</td>\n",
       "      <td>0.040073</td>\n",
       "      <td>0.053939</td>\n",
       "      <td>0.049708</td>\n",
       "      <td>0.077236</td>\n",
       "      <td>0.039848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.215721</td>\n",
       "      <td>0.264417</td>\n",
       "      <td>0.115308</td>\n",
       "      <td>0.134424</td>\n",
       "      <td>0.240917</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.278135</td>\n",
       "      <td>0.292377</td>\n",
       "      <td>0.376166</td>\n",
       "      <td>0.285387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.093549</td>\n",
       "      <td>0.055880</td>\n",
       "      <td>0.082474</td>\n",
       "      <td>0.048377</td>\n",
       "      <td>0.062947</td>\n",
       "      <td>0.050669</td>\n",
       "      <td>0.051481</td>\n",
       "      <td>0.113600</td>\n",
       "      <td>0.065854</td>\n",
       "      <td>0.057337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258010</td>\n",
       "      <td>0.304455</td>\n",
       "      <td>0.120156</td>\n",
       "      <td>0.102158</td>\n",
       "      <td>0.380408</td>\n",
       "      <td>0.278135</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.161604</td>\n",
       "      <td>0.258801</td>\n",
       "      <td>0.300367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.036924</td>\n",
       "      <td>0.051850</td>\n",
       "      <td>0.045107</td>\n",
       "      <td>0.023817</td>\n",
       "      <td>0.032841</td>\n",
       "      <td>0.031405</td>\n",
       "      <td>0.051775</td>\n",
       "      <td>0.086812</td>\n",
       "      <td>0.044223</td>\n",
       "      <td>0.035252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113711</td>\n",
       "      <td>0.201387</td>\n",
       "      <td>0.045754</td>\n",
       "      <td>0.054098</td>\n",
       "      <td>0.151760</td>\n",
       "      <td>0.292377</td>\n",
       "      <td>0.161604</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.417356</td>\n",
       "      <td>0.256883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.079958</td>\n",
       "      <td>0.025982</td>\n",
       "      <td>0.099334</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.047874</td>\n",
       "      <td>0.051362</td>\n",
       "      <td>0.063268</td>\n",
       "      <td>0.084928</td>\n",
       "      <td>0.047775</td>\n",
       "      <td>0.045450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189957</td>\n",
       "      <td>0.282968</td>\n",
       "      <td>0.163649</td>\n",
       "      <td>0.050309</td>\n",
       "      <td>0.249007</td>\n",
       "      <td>0.376166</td>\n",
       "      <td>0.258801</td>\n",
       "      <td>0.417356</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.310659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.075325</td>\n",
       "      <td>0.023517</td>\n",
       "      <td>0.071228</td>\n",
       "      <td>0.044435</td>\n",
       "      <td>0.060753</td>\n",
       "      <td>0.043752</td>\n",
       "      <td>0.068758</td>\n",
       "      <td>0.103060</td>\n",
       "      <td>0.055511</td>\n",
       "      <td>0.051385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269585</td>\n",
       "      <td>0.441403</td>\n",
       "      <td>0.127978</td>\n",
       "      <td>0.082310</td>\n",
       "      <td>0.418322</td>\n",
       "      <td>0.285387</td>\n",
       "      <td>0.300367</td>\n",
       "      <td>0.256883</td>\n",
       "      <td>0.310659</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6     \\\n",
       "0     1.000000  0.140902  0.171522  0.095338  0.138435  0.096193  0.145993   \n",
       "1     0.140902  1.000000  0.193965  0.139094  0.156889  0.116773  0.172582   \n",
       "2     0.171522  0.193965  1.000000  0.130630  0.145819  0.095854  0.174459   \n",
       "3     0.095338  0.139094  0.130630  1.000000  0.144374  0.094235  0.111451   \n",
       "4     0.138435  0.156889  0.145819  0.144374  1.000000  0.107828  0.167058   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1995  0.058964  0.034626  0.043620  0.040684  0.042474  0.040073  0.053939   \n",
       "1996  0.093549  0.055880  0.082474  0.048377  0.062947  0.050669  0.051481   \n",
       "1997  0.036924  0.051850  0.045107  0.023817  0.032841  0.031405  0.051775   \n",
       "1998  0.079958  0.025982  0.099334  0.064920  0.047874  0.051362  0.063268   \n",
       "1999  0.075325  0.023517  0.071228  0.044435  0.060753  0.043752  0.068758   \n",
       "\n",
       "          7         8         9     ...      1990      1991      1992  \\\n",
       "0     0.159541  0.161309  0.120384  ...  0.144524  0.082640  0.082743   \n",
       "1     0.155856  0.134259  0.105740  ...  0.069009  0.042250  0.017035   \n",
       "2     0.160876  0.119314  0.141472  ...  0.042253  0.047994  0.064691   \n",
       "3     0.150861  0.095013  0.079430  ...  0.028193  0.039631  0.045939   \n",
       "4     0.187601  0.190546  0.144120  ...  0.046729  0.069090  0.070003   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1995  0.049708  0.077236  0.039848  ...  0.215721  0.264417  0.115308   \n",
       "1996  0.113600  0.065854  0.057337  ...  0.258010  0.304455  0.120156   \n",
       "1997  0.086812  0.044223  0.035252  ...  0.113711  0.201387  0.045754   \n",
       "1998  0.084928  0.047775  0.045450  ...  0.189957  0.282968  0.163649   \n",
       "1999  0.103060  0.055511  0.051385  ...  0.269585  0.441403  0.127978   \n",
       "\n",
       "          1993      1994      1995      1996      1997      1998      1999  \n",
       "0     0.049397  0.127117  0.058964  0.093549  0.036924  0.079958  0.075325  \n",
       "1     0.049591  0.021204  0.034626  0.055880  0.051850  0.025982  0.023517  \n",
       "2     0.066445  0.043972  0.043620  0.082474  0.045107  0.099334  0.071228  \n",
       "3     0.034298  0.055913  0.040684  0.048377  0.023817  0.064920  0.044435  \n",
       "4     0.035328  0.066820  0.042474  0.062947  0.032841  0.047874  0.060753  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1995  0.134424  0.240917  1.000000  0.278135  0.292377  0.376166  0.285387  \n",
       "1996  0.102158  0.380408  0.278135  1.000000  0.161604  0.258801  0.300367  \n",
       "1997  0.054098  0.151760  0.292377  0.161604  1.000000  0.417356  0.256883  \n",
       "1998  0.050309  0.249007  0.376166  0.258801  0.417356  1.000000  0.310659  \n",
       "1999  0.082310  0.418322  0.285387  0.300367  0.256883  0.310659  1.000000  \n",
       "\n",
       "[2000 rows x 2000 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute document similarity using LSA components\n",
    "lsa_similarity = np.asarray(np.asmatrix(lsa) * np.asmatrix(lsa).T)\n",
    "pd.DataFrame(lsa_similarity,index=range(num_essays), columns=range(num_essays))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a6833",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "Using the vectors we got we measure how similar they are using cosine distance between vector pairs. \n",
    "\n",
    "This feature has not been taken from any of the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e572490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(essay_id):\n",
    "    index_high = data.index[data['domain1_score'] == 4].tolist()\n",
    "    n = len(index_high)\n",
    "\n",
    "    j = data.index[data['essay_id'] == essay_id]\n",
    "    similarity = 0\n",
    "    for i in index_high:\n",
    "        similarity += cosine_similarity(vectors_all[i,:],vectors_all[j,:])\n",
    "    similarity /= n\n",
    "    \n",
    "    return np.asscalar(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3cc8f1",
   "metadata": {},
   "source": [
    "### Extraction\n",
    "\n",
    "Finally we apply all the above functions to our data set and store the results in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a1cc978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "    \n",
    "    features = data.copy()\n",
    "    \n",
    "    features['word_count'] = features['essay'].apply(get_word_count)\n",
    "    print(\"Added 'word_count' feature successfully.\")\n",
    "    \n",
    "    features['sent_count'] = features['essay'].apply(get_sentence_count)\n",
    "    print(\"Added 'sent_count' feature successfully.\")\n",
    "    \n",
    "    features['avg_word_len'] = features['essay'].apply(get_word_length_average)\n",
    "    print(\"Added 'avg_word_len' feature successfully.\")\n",
    "    \n",
    "    features['lemma_count'] = features['essay'].apply(get_lemma_count)\n",
    "    print(\"Added 'lemma_count' feature successfully.\")\n",
    "    \n",
    "    features['spell_err_count'] = features['essay'].apply(get_spell_error_count)\n",
    "    print(\"Added 'spell_err_count' feature successfully.\")\n",
    "    \n",
    "    features['noun_count'], features['adj_count'], features['verb_count'], features['adv_count'] = zip(*features['essay'].map(get_pos_counts))\n",
    "    print(\"Added 'noun_count', 'adj_count', 'verb_count' and 'adv_count' features successfully.\")\n",
    "    \n",
    "    features['neg_score'], features['pos_score'], features['neu_score'] = zip(*features['essay'].map(get_sentiment_tags))\n",
    "    print(\"Added 'neg_score', 'pos_score' and 'neu_score' features successfully.\")\n",
    "    \n",
    "    features['cosine_similarity'] = features['essay_id'].apply(get_cosine_similarity)\n",
    "    print(\"Added 'similarity' feature successfully.\")\n",
    "        \n",
    "    # TODO: LSA \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e458b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'word_count' feature successfully.\n",
      "Added 'sent_count' feature successfully.\n",
      "Added 'avg_word_len' feature successfully.\n",
      "Added 'lemma_count' feature successfully.\n",
      "Added 'spell_err_count' feature successfully.\n",
      "Added 'noun_count', 'adj_count', 'verb_count' and 'adv_count' features successfully.\n",
      "Added 'neg_score', 'pos_score' and 'neu_score' features successfully.\n",
      "Added 'similarity' feature successfully.\n"
     ]
    }
   ],
   "source": [
    "features_set1 = extract_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5a2542f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>lemma_count</th>\n",
       "      <th>spell_err_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neu_score</th>\n",
       "      <th>cosine_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>350</td>\n",
       "      <td>16</td>\n",
       "      <td>4.237143</td>\n",
       "      <td>162</td>\n",
       "      <td>0.045714</td>\n",
       "      <td>0.237143</td>\n",
       "      <td>0.051429</td>\n",
       "      <td>0.211429</td>\n",
       "      <td>0.068571</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.090943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>423</td>\n",
       "      <td>20</td>\n",
       "      <td>4.312057</td>\n",
       "      <td>185</td>\n",
       "      <td>0.061466</td>\n",
       "      <td>0.252955</td>\n",
       "      <td>0.044917</td>\n",
       "      <td>0.200946</td>\n",
       "      <td>0.044917</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.049000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>283</td>\n",
       "      <td>14</td>\n",
       "      <td>4.342756</td>\n",
       "      <td>145</td>\n",
       "      <td>0.031802</td>\n",
       "      <td>0.289753</td>\n",
       "      <td>0.070671</td>\n",
       "      <td>0.183746</td>\n",
       "      <td>0.056537</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.069262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>530</td>\n",
       "      <td>27</td>\n",
       "      <td>4.813208</td>\n",
       "      <td>236</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>0.335849</td>\n",
       "      <td>0.079245</td>\n",
       "      <td>0.183019</td>\n",
       "      <td>0.054717</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.056878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>473</td>\n",
       "      <td>30</td>\n",
       "      <td>4.334038</td>\n",
       "      <td>190</td>\n",
       "      <td>0.035941</td>\n",
       "      <td>0.241015</td>\n",
       "      <td>0.067653</td>\n",
       "      <td>0.190275</td>\n",
       "      <td>0.076110</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.071470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>3190</td>\n",
       "      <td>I believe that they should not be pulled off o...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>368</td>\n",
       "      <td>28</td>\n",
       "      <td>4.105978</td>\n",
       "      <td>152</td>\n",
       "      <td>0.024457</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.190217</td>\n",
       "      <td>0.081522</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.221566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>3191</td>\n",
       "      <td>When have you ever went into a library and fou...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>610</td>\n",
       "      <td>36</td>\n",
       "      <td>4.013115</td>\n",
       "      <td>199</td>\n",
       "      <td>0.031148</td>\n",
       "      <td>0.178689</td>\n",
       "      <td>0.054098</td>\n",
       "      <td>0.254098</td>\n",
       "      <td>0.068852</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>3192</td>\n",
       "      <td>When I go to a library I @MONTH1 find some stu...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>196</td>\n",
       "      <td>11</td>\n",
       "      <td>4.076531</td>\n",
       "      <td>102</td>\n",
       "      <td>0.045918</td>\n",
       "      <td>0.183673</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.051020</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.134513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>3193</td>\n",
       "      <td>Certain people beleive that offensive books, m...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>382</td>\n",
       "      <td>19</td>\n",
       "      <td>4.526178</td>\n",
       "      <td>141</td>\n",
       "      <td>0.031414</td>\n",
       "      <td>0.225131</td>\n",
       "      <td>0.083770</td>\n",
       "      <td>0.206806</td>\n",
       "      <td>0.078534</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.206831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>3194</td>\n",
       "      <td>Katherine Paterson said ' If I have the right ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>433</td>\n",
       "      <td>13</td>\n",
       "      <td>4.094688</td>\n",
       "      <td>145</td>\n",
       "      <td>0.034642</td>\n",
       "      <td>0.175520</td>\n",
       "      <td>0.090069</td>\n",
       "      <td>0.182448</td>\n",
       "      <td>0.096998</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.211123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essay_id                                              essay  \\\n",
       "0            1  Dear local newspaper, I think effects computer...   \n",
       "1            2  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2            3  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3            4  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4            5  Dear @LOCATION1, I know having computers has a...   \n",
       "...        ...                                                ...   \n",
       "1995      3190  I believe that they should not be pulled off o...   \n",
       "1996      3191  When have you ever went into a library and fou...   \n",
       "1997      3192  When I go to a library I @MONTH1 find some stu...   \n",
       "1998      3193  Certain people beleive that offensive books, m...   \n",
       "1999      3194  Katherine Paterson said ' If I have the right ...   \n",
       "\n",
       "      domain1_score  word_count  sent_count  avg_word_len  lemma_count  \\\n",
       "0               8.0         350          16      4.237143          162   \n",
       "1               9.0         423          20      4.312057          185   \n",
       "2               7.0         283          14      4.342756          145   \n",
       "3              10.0         530          27      4.813208          236   \n",
       "4               8.0         473          30      4.334038          190   \n",
       "...             ...         ...         ...           ...          ...   \n",
       "1995            4.0         368          28      4.105978          152   \n",
       "1996            4.0         610          36      4.013115          199   \n",
       "1997            3.0         196          11      4.076531          102   \n",
       "1998            3.0         382          19      4.526178          141   \n",
       "1999            3.0         433          13      4.094688          145   \n",
       "\n",
       "      spell_err_count  noun_count  adj_count  verb_count  adv_count  \\\n",
       "0            0.045714    0.237143   0.051429    0.211429   0.068571   \n",
       "1            0.061466    0.252955   0.044917    0.200946   0.044917   \n",
       "2            0.031802    0.289753   0.070671    0.183746   0.056537   \n",
       "3            0.122642    0.335849   0.079245    0.183019   0.054717   \n",
       "4            0.035941    0.241015   0.067653    0.190275   0.076110   \n",
       "...               ...         ...        ...         ...        ...   \n",
       "1995         0.024457    0.195652   0.097826    0.190217   0.081522   \n",
       "1996         0.031148    0.178689   0.054098    0.254098   0.068852   \n",
       "1997         0.045918    0.183673   0.102041    0.250000   0.051020   \n",
       "1998         0.031414    0.225131   0.083770    0.206806   0.078534   \n",
       "1999         0.034642    0.175520   0.090069    0.182448   0.096998   \n",
       "\n",
       "      neg_score  pos_score  neu_score  cosine_similarity  \n",
       "0         0.000      0.170      0.830           0.090943  \n",
       "1         0.014      0.219      0.766           0.049000  \n",
       "2         0.045      0.197      0.759           0.069262  \n",
       "3         0.008      0.152      0.840           0.056878  \n",
       "4         0.026      0.096      0.879           0.071470  \n",
       "...         ...        ...        ...                ...  \n",
       "1995      0.121      0.092      0.786           0.221566  \n",
       "1996      0.144      0.051      0.805           0.212200  \n",
       "1997      0.156      0.107      0.737           0.134513  \n",
       "1998      0.128      0.074      0.798           0.206831  \n",
       "1999      0.077      0.059      0.864           0.211123  \n",
       "\n",
       "[2000 rows x 16 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_set1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "195bf786",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_set1.to_csv('features.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
